{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\virtual_environment\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Giai đoạn 1: Nền tảng cơ bản**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Tensor là gì và tại sao cần học về tensor: ###\n",
    "- Tensor là cấu trúc dữ liệu cơ bản trong PyTorch, tương tự như NumPy array nhưng được tối ưu cho deep learning\n",
    "- Tensor có thể chạy trên GPU để tăng tốc độ tính toán\n",
    "- Tensor tự động tính được đạo hàm (gradient) - rất quan trọng trong quá trình huấn luyện neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Các cách tạo tensor cơ bản: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4687, 0.5336, 0.5818, 0.4117],\n",
      "        [0.7924, 0.8372, 0.3551, 0.9943],\n",
      "        [0.8983, 0.5106, 0.8574, 0.9261]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "tensor([0, 2, 4, 6, 8])\n",
      "tensor([0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000, 0.9000,\n",
      "        1.0000])\n",
      "tensor([1, 1, 1, 1])\n",
      "tensor([0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Tạo tensor từ list\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "\n",
    "# Tạo tensor với giá trị ngẫu nhiên\n",
    "random_tensor = torch.rand(size=(3, 4))\n",
    "print(random_tensor)\n",
    "\n",
    "# Tạo tensor toàn số 0 hoặc 1\n",
    "zeros = torch.zeros(size=(3, 4))\n",
    "ones = torch.ones(size=(3, 4))\n",
    "eye = torch.eye(3)\n",
    "print(zeros)\n",
    "print(ones)\n",
    "print(eye)\n",
    "\n",
    "# Tạo tensor với giá trị tuần tự\n",
    "range_tensor = torch.arange(start=0, end=10, step=2)\n",
    "linspace_tensor = torch.linspace(start=0.1, end=1, steps=10)\n",
    "print(range_tensor)\n",
    "print(linspace_tensor)\n",
    "\n",
    "# Tạo tensor với shape giống tensor khác\n",
    "x_ones = torch.ones_like(x)\n",
    "x_zeros = torch.zeros_like(x)\n",
    "print(x_ones)\n",
    "print(x_zeros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Các thuộc tính quan trọng của tensor: ###\n",
    "- shape: kích thước của tensor\n",
    "- dtype: kiểu dữ liệu (float32, int64,...)\n",
    "- device: tensor được lưu ở đâu (CPU hay GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.int64\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Ví dụ kiểm tra thuộc tính\n",
    "print(x.shape)      # Kích thước\n",
    "print(x.dtype)      # Kiểu dữ liệu\n",
    "print(x.device)     # Thiết bị lưu tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Các phép toán tensor cơ bản: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "\n",
    "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "new_shape = (2, 5)\n",
    "\n",
    "# Phép cộng\n",
    "result = tensor1 + tensor2\n",
    "# hoặc\n",
    "result = torch.add(tensor1, tensor2)\n",
    "\n",
    "# Phép nhân ma trận\n",
    "result = torch.matmul(tensor1, tensor2)\n",
    "# hoặc\n",
    "result = tensor1 @ tensor2\n",
    "\n",
    "# Reshape tensor\n",
    "reshaped = tensor.reshape(new_shape)\n",
    "# hoặc\n",
    "reshaped = tensor.view(new_shape)\n",
    "\n",
    "print(reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Các thao tác tensor nâng cao: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n",
      "tensor([[3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor([ 9, 10])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "# Indexing\n",
    "print(tensor[0])          # Phần tử đầu tiên\n",
    "print(tensor[1:4])       # Slice tensor\n",
    "print(tensor[4])        # Phần tử thứ 5\n",
    "\n",
    "# Concatenate tensors\n",
    "concat = torch.cat([tensor1, tensor2], dim=0)\n",
    "print(concat)\n",
    "\n",
    "# Stack tensors\n",
    "stacked = torch.stack([tensor1, tensor2], dim=0)\n",
    "print(stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Chuyển đổi giữa các kiểu dữ liệu: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển từ NumPy sang PyTorch\n",
    "numpy_array = np.array([1, 2, 3])\n",
    "torch_tensor = torch.from_numpy(numpy_array)\n",
    "\n",
    "# Chuyển từ PyTorch sang NumPy\n",
    "numpy_array = torch_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Chuyển đổi CPU và GPU: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Các lớp Layer trong Neural Network - Phân tích chi tiết\n",
    "\n",
    "## 1. Fully Connected Layer (Dense Layer)\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: Tensor $X$ có kích thước $(batch\\_size, input\\_features)$\n",
    "- **Output**: $Y = \\sigma(W \\cdot X + b)$ có kích thước $(batch\\_size, output\\_features)$\n",
    "\n",
    "**Tham số**:\n",
    "- Ma trận trọng số $W$ có kích thước $(input\\_features, output\\_features)$\n",
    "- Bias $b$ có kích thước $(output\\_features)$\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Mỗi neuron nhận input từ tất cả các neuron ở layer trước đó\n",
    "- Tính tổng có trọng số của tất cả các inputs, cộng với bias\n",
    "- Áp dụng hàm kích hoạt $\\sigma$ lên kết quả\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Các bài toán cần tính toán trên dữ liệu vector hóa\n",
    "- Layer cuối cùng trong hầu hết mạng neural để đưa ra dự đoán\n",
    "\n",
    "**Hạn chế**:\n",
    "- Số lượng tham số lớn: $input\\_features \\times output\\_features + output\\_features$\n",
    "- Không giữ được thông tin không gian/vị trí trong dữ liệu\n",
    "- Dễ bị overfitting khi dữ liệu ít\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- Phân loại đơn giản trên dữ liệu vector\n",
    "- Layer cuối trong bài toán hồi quy/phân loại\n",
    "- MLP (Multi-Layer Perceptron) cho dữ liệu có cấu trúc bảng\n",
    "\n",
    "## 2. Convolutional Layer (Conv2D)\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: Tensor $X$ có kích thước $(batch\\_size, height, width, channels\\_in)$\n",
    "- **Output**: $Y$ có kích thước $(batch\\_size, height', width', channels\\_out)$\n",
    "\n",
    "**Tham số**:\n",
    "- Bộ lọc (filters/kernels): $K$ có kích thước $(kernel\\_size, kernel\\_size, channels\\_in, channels\\_out)$\n",
    "- Bias: $b$ có kích thước $(channels\\_out)$\n",
    "- Stride: Bước nhảy của cửa sổ tích chập\n",
    "- Padding: Đệm zero xung quanh input\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Mỗi filter trượt trên input, tính tích chập (dot product)\n",
    "- Mỗi vị trí trượt tạo ra một giá trị đầu ra\n",
    "- Kết quả đầu ra mỗi filter tạo thành một feature map\n",
    "- Nhiều filter tạo ra nhiều feature map (channels)\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Dữ liệu có cấu trúc không gian (ảnh, time series, audio)\n",
    "- Khi cần phát hiện đặc trưng cục bộ, giữ thông tin vị trí\n",
    "\n",
    "**Hạn chế**:\n",
    "- Chỉ bắt được đặc trưng cục bộ, không nắm bắt mối quan hệ toàn cục\n",
    "- Khó khăn với input có kích thước không đều\n",
    "- Thiếu tính bất biến theo phép quay (rotation invariance)\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- Xử lý ảnh, nhận dạng đối tượng\n",
    "- Phân tích time series/1D convolutional\n",
    "- Phát hiện đặc trưng trong xử lý ngôn ngữ tự nhiên\n",
    "\n",
    "## 3. Recurrent Layer (RNN/LSTM/GRU)\n",
    "\n",
    "### 3.1 Simple RNN\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: Chuỗi $X = [x_1, x_2, ..., x_t]$ với mỗi $x_i$ có kích thước $(batch\\_size, features)$\n",
    "- **Output**: $h_t = \\sigma(W_{hx} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)$\n",
    "\n",
    "**Tham số**:\n",
    "- $W_{hx}$: Ma trận trọng số từ input đến hidden state\n",
    "- $W_{hh}$: Ma trận trọng số từ hidden state hiện tại đến hidden state tiếp theo\n",
    "- $b_h$: Bias\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Duy trì một hidden state $h$ qua các bước thời gian\n",
    "- Tại mỗi bước t, kết hợp input hiện tại $x_t$ với hidden state trước đó $h_{t-1}$\n",
    "- Áp dụng hàm kích hoạt (thường là tanh) lên kết quả\n",
    "\n",
    "### 3.2 LSTM (Long Short-Term Memory)\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: Chuỗi $X = [x_1, x_2, ..., x_t]$\n",
    "- **Output**: Hidden states $h_t$ và cell states $c_t$\n",
    "- Các cổng: forget gate $(f_t)$, input gate $(i_t)$, output gate $(o_t)$, và cell candidate $(g_t)$\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "$$g_t = \\tanh(W_g \\cdot [h_{t-1}, x_t] + b_g)$$\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot g_t$$\n",
    "$$h_t = o_t \\odot \\tanh(c_t)$$\n",
    "\n",
    "**Tham số**:\n",
    "- $W_f, W_i, W_g, W_o$: Ma trận trọng số cho mỗi cổng\n",
    "- $b_f, b_i, b_g, b_o$: Bias cho mỗi cổng\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Sử dụng cơ chế cổng (gates) để kiểm soát thông tin\n",
    "- Forget gate quyết định bỏ đi thông tin nào từ cell state\n",
    "- Input gate quyết định cập nhật thông tin nào vào cell state\n",
    "- Output gate kiểm soát thông tin nào từ cell state đưa ra ngoài\n",
    "\n",
    "**Khi nào nên dùng (RNN/LSTM/GRU)**:\n",
    "- Dữ liệu dạng chuỗi (văn bản, time series, audio)\n",
    "- Khi cần nắm bắt phụ thuộc dài hạn trong dữ liệu\n",
    "- Các tác vụ có tính tuần tự và phụ thuộc thời gian\n",
    "\n",
    "**Hạn chế**:\n",
    "- Tính toán tuần tự, khó song song hóa\n",
    "- Vấn đề gradient biến mất/bùng nổ trong RNN đơn giản\n",
    "- Tốn kém tính toán cho chuỗi dài (LSTM/GRU đỡ hơn)\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- Xử lý ngôn ngữ tự nhiên, dịch máy\n",
    "- Dự báo chuỗi thời gian, phân tích tài chính\n",
    "- Phân tích cảm xúc, sinh văn bản\n",
    "\n",
    "## 4. Pooling Layer\n",
    "\n",
    "### 4.1 Max Pooling\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: Tensor $X$ có kích thước $(batch\\_size, height, width, channels)$\n",
    "- **Output**: $Y$ có kích thước $(batch\\_size, height/pool\\_size, width/pool\\_size, channels)$\n",
    "- $Y_{i,j,c} = \\max_{m,n \\in window} X_{stride \\cdot i + m, stride \\cdot j + n, c}$\n",
    "\n",
    "**Tham số**:\n",
    "- Pool size: Kích thước cửa sổ gộp (thường là 2x2)\n",
    "- Stride: Bước nhảy (thường bằng pool size)\n",
    "- Padding: Đệm zero (thường không dùng)\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Chia input thành các cửa sổ không chồng chéo\n",
    "- Lấy giá trị lớn nhất trong mỗi cửa sổ\n",
    "- Giảm kích thước không gian nhưng giữ nguyên số channels\n",
    "\n",
    "### 4.2 Average Pooling\n",
    "\n",
    "**Công thức**:\n",
    "- Tương tự Max Pooling, nhưng lấy giá trị trung bình thay vì giá trị lớn nhất\n",
    "- $Y_{i,j,c} = \\frac{1}{|window|} \\sum_{m,n \\in window} X_{stride \\cdot i + m, stride \\cdot j + n, c}$\n",
    "\n",
    "**Khi nào nên dùng (Pooling)**:\n",
    "- Sau các lớp Convolutional để giảm kích thước không gian\n",
    "- Khi muốn tính bất biến với dịch chuyển nhỏ (translation invariance)\n",
    "- Giảm tham số và tính toán trong mạng\n",
    "\n",
    "**Hạn chế**:\n",
    "- Mất thông tin vị trí chi tiết\n",
    "- Không có tham số để học (fixed operation)\n",
    "- Có thể bỏ qua đặc trưng quan trọng (với Max Pooling)\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- CNN cho phân loại ảnh\n",
    "- Giảm chiều không gian và tránh overfitting\n",
    "- Max Pooling tốt cho việc phát hiện đặc trưng; Average Pooling tốt cho tổng hợp tổng thể\n",
    "\n",
    "## 5. Normalization Layers\n",
    "\n",
    "### 5.1 Batch Normalization\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: $X$ với kích thước $(batch\\_size, features)$ hoặc $(batch\\_size, height, width, channels)$\n",
    "- **Output**: $Y = \\gamma \\cdot \\frac{X - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\beta$\n",
    "\n",
    "**Tham số**:\n",
    "- $\\gamma$: Tham số scale (có thể học)\n",
    "- $\\beta$: Tham số shift (có thể học)\n",
    "- $\\epsilon$: Hằng số nhỏ để ổn định phép chia\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Tính trung bình $\\mu_B$ và phương sai $\\sigma_B^2$ trên mỗi batch\n",
    "- Chuẩn hóa input để có trung bình 0, phương sai 1\n",
    "- Scale và shift bằng các tham số có thể học $\\gamma$ và $\\beta$\n",
    "\n",
    "### 5.2 Layer Normalization\n",
    "\n",
    "**Công thức**:\n",
    "- Tương tự Batch Normalization, nhưng chuẩn hóa theo các features thay vì theo batch\n",
    "- Tính $\\mu$ và $\\sigma$ trên mỗi sample riêng biệt\n",
    "\n",
    "**Khi nào nên dùng (Normalization)**:\n",
    "- Batch Norm: Mạng CNN, mạng feed-forward sâu\n",
    "- Layer Norm: Mạng RNN, Transformer\n",
    "- Khi cần tăng tốc độ hội tụ và ổn định quá trình huấn luyện\n",
    "\n",
    "**Hạn chế**:\n",
    "- Batch Norm phụ thuộc vào kích thước batch\n",
    "- Tăng độ phức tạp tính toán\n",
    "- Có thể làm giảm khả năng biểu diễn của mô hình nếu sử dụng không đúng cách\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- Batch Norm: ResNet, GoogleNet và hầu hết CNN hiện đại\n",
    "- Layer Norm: BERT, GPT, Transformer và mạng NLP tiên tiến\n",
    "- Instance/Group Norm: StyleGAN và mạng sinh ảnh\n",
    "\n",
    "## 6. Dropout Layer\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: $X$ bất kỳ\n",
    "- **Output (khi training)**: $Y = X \\odot M / (1-p)$ với $M$ là mask nhị phân với xác suất $p$ để có giá trị 0\n",
    "- **Output (khi inference)**: $Y = X$ (không áp dụng dropout khi dự đoán)\n",
    "\n",
    "**Tham số**:\n",
    "- Dropout rate $p$: Xác suất một neuron bị \"tắt\"\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Trong quá trình huấn luyện, ngẫu nhiên \"tắt\" một tỷ lệ neuron\n",
    "- Scale lại các neuron còn lại để tổng đầu ra không đổi\n",
    "- Khi dự đoán, sử dụng tất cả các neuron\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Khi có nguy cơ overfitting (mô hình lớn, dữ liệu ít)\n",
    "- Giữa các lớp fully connected\n",
    "- Trong các mạng RNN (nhưng cần thận trọng)\n",
    "\n",
    "**Hạn chế**:\n",
    "- Làm chậm quá trình hội tụ, cần nhiều epochs hơn\n",
    "- Không hiệu quả với các mô hình nhỏ hoặc underfitting\n",
    "- Cần điều chỉnh learning rate đi kèm\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- Các mô hình phân loại lớn\n",
    "- Transfer learning khi fine-tuning\n",
    "- Ensemble learning (mỗi forward pass với dropout là một mô hình khác nhau)\n",
    "\n",
    "## 7. Attention Layer\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: Queries $Q$, Keys $K$, Values $V$\n",
    "- **Output**: $\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "**Tham số**:\n",
    "- Chiều của query/key/value\n",
    "- Số lượng heads (với multi-head attention)\n",
    "- Projection matrices $W_Q, W_K, W_V, W_O$\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Tính độ tương đồng giữa query và mỗi key\n",
    "- Áp dụng softmax để chuẩn hóa trọng số\n",
    "- Tính tổng có trọng số của các value dựa trên trọng số\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Mô hình cần nắm bắt mối quan hệ dài hạn trong dữ liệu\n",
    "- Khi cần tập trung vào các phần khác nhau của input\n",
    "- Thay thế cơ chế tuần tự của RNN\n",
    "\n",
    "**Hạn chế**:\n",
    "- Độ phức tạp O(n²) theo độ dài chuỗi\n",
    "- Tốn kém bộ nhớ cho chuỗi dài\n",
    "- Cần kỹ thuật để áp dụng hiệu quả với input lớn\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- Kiến trúc Transformer (BERT, GPT, T5)\n",
    "- Dịch máy, tóm tắt văn bản\n",
    "- Vision Transformer (ViT) trong xử lý ảnh\n",
    "\n",
    "## 8. Embedding Layer\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: Chỉ số nguyên $i$ (hoặc tensor chỉ số)\n",
    "- **Output**: Vector $v_i$ từ ma trận embedding\n",
    "\n",
    "**Tham số**:\n",
    "- Ma trận embedding $E$ có kích thước $(vocab\\_size, embedding\\_dim)$\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Mỗi chỉ số đầu vào được ánh xạ tới một hàng trong ma trận embedding\n",
    "- Thực chất là một lookup table có thể cập nhật\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Biểu diễn dữ liệu rời rạc (từ, token, category) bằng vector liên tục\n",
    "- Đầu vào cho các mô hình NLP\n",
    "- Encoding categorical features\n",
    "\n",
    "**Hạn chế**:\n",
    "- Số lượng tham số lớn với vocab lớn\n",
    "- Cần đủ dữ liệu để học biểu diễn tốt\n",
    "- Thách thức với từ hiếm/không có trong vocabulary\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- Word embedding trong NLP (Word2Vec, GloVe)\n",
    "- Mô hình ngôn ngữ, dịch máy\n",
    "- Recommender systems (user/item embeddings)\n",
    "\n",
    "## 9. Residual Connections (ResNet Block)\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: $X$\n",
    "- **Output**: $Y = X + F(X)$ với $F$ là một chuỗi các layer (thường là Conv + BN + ReLU)\n",
    "\n",
    "**Tham số**:\n",
    "- Tham số của các layer trong hàm $F$\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Tạo đường tắt (shortcut connection) bỏ qua một hoặc nhiều layer\n",
    "- Cộng đầu ra của các layer này với input ban đầu\n",
    "- Giúp gradient flow dễ dàng hơn qua mạng\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Mạng neural sâu (>10 layers)\n",
    "- Khi gặp vấn đề vanishing gradient\n",
    "- Cải thiện hiệu suất mô hình sâu\n",
    "\n",
    "**Hạn chế**:\n",
    "- Tăng nhẹ độ phức tạp trong triển khai\n",
    "- Cần điều chỉnh chiều nếu input và output có kích thước khác nhau\n",
    "- Có thể không cần thiết cho mạng nông\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- ResNet và các biến thể (ResNeXt, DenseNet)\n",
    "- Vision Transformer với residual connections\n",
    "- Kiến trúc Transformer trong NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Các Activation Function trong Neural Network\n",
    "\n",
    "## 1. Sigmoid (Logistic)\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: $x \\in \\mathbb{R}$\n",
    "- **Output**: $\\sigma(x) = \\frac{1}{1 + e^{-x}} \\in (0,1)$\n",
    "- **Đạo hàm**: $\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$\n",
    "\n",
    "**Tham số**: Không có tham số có thể điều chỉnh\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Biến đổi giá trị đầu vào thành giá trị trong khoảng (0,1)\n",
    "- Đồ thị hình chữ S (S-shaped)\n",
    "- Bão hòa và tiệm cận đến 0 hoặc 1 với đầu vào âm hoặc dương lớn\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Layer cuối cùng trong bài toán phân loại nhị phân\n",
    "- Mô hình hóa xác suất\n",
    "- Các cổng trong LSTM (kết hợp với tanh)\n",
    "\n",
    "**Hạn chế**:\n",
    "- Vấn đề gradient biến mất (vanishing gradient) với đầu vào có giá trị tuyệt đối lớn\n",
    "- Output không zero-centered, làm chậm quá trình học\n",
    "- Đạo hàm tối đa là 0.25, làm chậm quá trình hội tụ\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- Mô hình hồi quy logistic\n",
    "- Cơ chế cổng trong LSTM và GRU\n",
    "- Bài toán phân loại nhị phân\n",
    "\n",
    "## 2. Tanh (Hyperbolic Tangent)\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: $x \\in \\mathbb{R}$\n",
    "- **Output**: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = \\frac{2}{1 + e^{-2x}} - 1 \\in (-1,1)$\n",
    "- **Đạo hàm**: $\\tanh'(x) = 1 - \\tanh^2(x)$\n",
    "\n",
    "**Tham số**: Không có tham số có thể điều chỉnh\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Biến đổi giá trị đầu vào thành giá trị trong khoảng (-1,1)\n",
    "- Tương tự sigmoid nhưng zero-centered\n",
    "- Bão hòa và tiệm cận đến -1 hoặc 1 với đầu vào âm hoặc dương lớn\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Khi cần outputs zero-centered\n",
    "- Trong mạng RNN, LSTM\n",
    "- Khi đầu vào đã được chuẩn hóa\n",
    "\n",
    "**Hạn chế**:\n",
    "- Vẫn gặp vấn đề gradient biến mất với đầu vào có giá trị tuyệt đối lớn\n",
    "- Tính toán phức tạp hơn so với ReLU\n",
    "- Chậm hơn ReLU trong quá trình huấn luyện\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- Mạng RNN cổ điển\n",
    "- Các mô hình xử lý chuỗi thời gian\n",
    "- Mô hình hóa giá trị tiếp theo trong dãy với khoảng giá trị cố định\n",
    "\n",
    "## 3. ReLU (Rectified Linear Unit)\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: $x \\in \\mathbb{R}$\n",
    "- **Output**: $\\text{ReLU}(x) = \\max(0, x) \\in [0, \\infty)$\n",
    "- **Đạo hàm**: $\\text{ReLU}'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}$\n",
    "\n",
    "**Tham số**: Không có tham số có thể điều chỉnh\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Đầu ra bằng 0 với đầu vào âm\n",
    "- Đầu ra bằng đầu vào với đầu vào dương\n",
    "- Đơn giản, hiệu quả tính toán cao\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Layer ẩn trong hầu hết các mạng CNN, MLP hiện đại\n",
    "- Khi cần tốc độ tính toán cao\n",
    "- Mạng sâu với nhiều layer\n",
    "\n",
    "**Hạn chế**:\n",
    "- \"Dying ReLU\" - neuron có thể \"chết\" khi gradient = 0 với x ≤ 0\n",
    "- Không zero-centered, có thể gây ra hiện tượng zigzag khi cập nhật\n",
    "- Không giới hạn trên, có thể gây mất ổn định trong quá trình học\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- CNN cho nhận dạng ảnh\n",
    "- Mạng deep feed-forward\n",
    "- Hầu hết các kiến trúc deep learning hiện đại\n",
    "\n",
    "## 4. Leaky ReLU\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: $x \\in \\mathbb{R}$\n",
    "- **Output**: $\\text{LeakyReLU}(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases} \\in \\mathbb{R}$\n",
    "- **Đạo hàm**: $\\text{LeakyReLU}'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ \\alpha & \\text{if } x \\leq 0 \\end{cases}$\n",
    "\n",
    "**Tham số**: \n",
    "- $\\alpha$: hệ số độ dốc cho x < 0 (thường là 0.01)\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Tương tự ReLU nhưng cho phép gradient nhỏ (alpha) cho đầu vào âm\n",
    "- Giải quyết vấn đề \"dying ReLU\"\n",
    "- Vẫn hiệu quả về mặt tính toán\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Thay thế ReLU khi gặp vấn đề dying neurons\n",
    "- Khi muốn tận dụng thông tin từ giá trị âm\n",
    "- Trong các mạng GAN\n",
    "\n",
    "**Hạn chế**:\n",
    "- Thêm một hyperparameter (alpha) cần tinh chỉnh\n",
    "- Vẫn không zero-centered\n",
    "- Cải thiện không đáng kể trong một số trường hợp\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- Mạng GAN (Generative Adversarial Networks)\n",
    "- Thay thế ReLU trong CNN\n",
    "- Các mô hình học sâu với negative features quan trọng\n",
    "\n",
    "## 5. Parametric ReLU (PReLU)\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: $x \\in \\mathbb{R}$\n",
    "- **Output**: $\\text{PReLU}(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha_i x & \\text{if } x \\leq 0 \\end{cases}$\n",
    "- **Đạo hàm**: $\\text{PReLU}'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ \\alpha_i & \\text{if } x \\leq 0 \\end{cases}$\n",
    "\n",
    "**Tham số**: \n",
    "- $\\alpha_i$: tham số có thể học cho mỗi kênh hoặc mỗi neuron\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Tương tự Leaky ReLU nhưng $\\alpha$ là tham số có thể học\n",
    "- Model tự tìm giá trị tối ưu cho $\\alpha$\n",
    "- Có thể có $\\alpha$ khác nhau cho mỗi kênh/neuron\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Khi muốn mô hình tự tìm slope tối ưu cho giá trị âm\n",
    "- Trong các mạng CNN sâu\n",
    "- Khi có đủ dữ liệu để học tham số thêm\n",
    "\n",
    "**Hạn chế**:\n",
    "- Tăng số lượng tham số, có thể gây overfitting với dữ liệu ít\n",
    "- Cần kỹ thuật regularization tốt\n",
    "- Tính toán phức tạp hơn ReLU thông thường\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- Mạng CNN cho nhận dạng ảnh (ResNet)\n",
    "- Mô hình với dữ liệu lớn\n",
    "- Transfer learning\n",
    "\n",
    "## 6. ELU (Exponential Linear Unit)\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: $x \\in \\mathbb{R}$\n",
    "- **Output**: $\\text{ELU}(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha (e^x - 1) & \\text{if } x \\leq 0 \\end{cases}$\n",
    "- **Đạo hàm**: $\\text{ELU}'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ \\alpha e^x & \\text{if } x \\leq 0 \\end{cases}$\n",
    "\n",
    "**Tham số**: \n",
    "- $\\alpha$: kiểm soát giá trị tiệm cận cho x âm (thường là 1.0)\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Tương tự ReLU cho x > 0\n",
    "- Có giá trị âm bão hòa (-α) cho x âm lớn\n",
    "- Đạo hàm mượt ở mọi điểm (không như ReLU)\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Khi cần tốc độ hội tụ nhanh hơn\n",
    "- Khi muốn giảm thiểu bias shift\n",
    "- Khi muốn đạo hàm liên tục tại x = 0\n",
    "\n",
    "**Hạn chế**:\n",
    "- Tính toán phức tạp hơn ReLU (hàm mũ)\n",
    "- Giá trị alpha cần được chọn trước\n",
    "- Có thể chậm hơn trên GPU\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- Mạng CNN sâu\n",
    "- Mạng RNN cải tiến\n",
    "- Bài toán có nhiễu hoặc cần regularization tốt\n",
    "\n",
    "## 7. SELU (Scaled Exponential Linear Unit)\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: $x \\in \\mathbb{R}$\n",
    "- **Output**: $\\text{SELU}(x) = \\lambda \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha (e^x - 1) & \\text{if } x \\leq 0 \\end{cases}$\n",
    "- **Đạo hàm**: $\\text{SELU}'(x) = \\lambda \\begin{cases} 1 & \\text{if } x > 0 \\\\ \\alpha e^x & \\text{if } x \\leq 0 \\end{cases}$\n",
    "\n",
    "**Tham số**: \n",
    "- $\\lambda \\approx 1.0507$ và $\\alpha \\approx 1.6733$ (được tính toán trước)\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Tương tự ELU nhưng với hệ số scale $\\lambda$\n",
    "- Được thiết kế để tự chuẩn hóa (self-normalizing)\n",
    "- Giữ mean=0 và variance=1 qua các layer\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Mạng feed-forward sâu không sử dụng batch normalization\n",
    "- Khi cần self-normalization\n",
    "- Đầu vào đã được chuẩn hóa\n",
    "\n",
    "**Hạn chế**:\n",
    "- Yêu cầu khởi tạo trọng số đặc biệt (LeCun initialization)\n",
    "- Không phù hợp với mọi kiến trúc (đặc biệt là CNN)\n",
    "- Cần đầu vào được chuẩn hóa tốt\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- Mạng feed-forward sâu\n",
    "- Mô hình không sử dụng batch normalization\n",
    "- Bài toán với dữ liệu đã chuẩn hóa tốt\n",
    "\n",
    "## 8. Swish\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: $x \\in \\mathbb{R}$\n",
    "- **Output**: $\\text{Swish}(x) = x \\cdot \\sigma(\\beta x) = \\frac{x}{1 + e^{-\\beta x}} \\in \\mathbb{R}$\n",
    "- **Đạo hàm**: $\\text{Swish}'(x) = \\beta \\cdot \\text{Swish}(x) + \\sigma(\\beta x)(1 - \\beta \\cdot \\text{Swish}(x))$\n",
    "\n",
    "**Tham số**: \n",
    "- $\\beta$: tham số điều chỉnh (có thể cố định = 1 hoặc là tham số có thể học)\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Kết hợp đặc tính của ReLU và sigmoid\n",
    "- Không bão hòa với đầu vào dương lớn như ReLU\n",
    "- Cho phép gradient nhỏ với đầu vào âm\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Mạng neural rất sâu\n",
    "- Thay thế ReLU trong mạng CNN hiện đại\n",
    "- Khi muốn hiệu suất tốt nhất mà không quan tâm đến tính đơn giản\n",
    "\n",
    "**Hạn chế**:\n",
    "- Tính toán phức tạp hơn ReLU\n",
    "- Đạo hàm phức tạp\n",
    "- Cần thử nghiệm để xác định lợi ích (không phải lúc nào cũng tốt hơn ReLU)\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- CNN hiện đại (EfficientNet)\n",
    "- Mạng neural sâu\n",
    "- Các bài toán cạnh tranh về accuracy\n",
    "\n",
    "## 9. Softmax\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: vector $\\mathbf{z} = (z_1, z_2, ..., z_n) \\in \\mathbb{R}^n$\n",
    "- **Output**: vector $\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}} \\in (0,1)$ với $\\sum_{i=1}^{n} \\sigma(\\mathbf{z})_i = 1$\n",
    "- **Đạo hàm**: $\\frac{\\partial \\sigma(\\mathbf{z})_i}{\\partial z_j} = \\sigma(\\mathbf{z})_i(\\delta_{ij} - \\sigma(\\mathbf{z})_j)$\n",
    "\n",
    "**Tham số**: Không có tham số có thể điều chỉnh\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Chuyển đổi vector thành phân phối xác suất\n",
    "- Tổng tất cả các phần tử output bằng 1\n",
    "- Nhấn mạnh giá trị lớn nhất (winner-take-most)\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Layer cuối cùng trong bài toán phân loại đa lớp\n",
    "- Khi cần biểu diễn dưới dạng xác suất\n",
    "- Attention mechanisms\n",
    "\n",
    "**Hạn chế**:\n",
    "- Không phù hợp cho layer ẩn\n",
    "- Tính toán có thể không ổn định với đầu vào lớn (cần kỹ thuật log-sum-exp)\n",
    "- Toàn bộ outputs phụ thuộc vào mọi input (thay đổi một input ảnh hưởng tất cả outputs)\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- Phân loại đa lớp\n",
    "- Mô hình ngôn ngữ\n",
    "- Cơ chế attention trong Transformer\n",
    "\n",
    "## 10. GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "**Công thức**:\n",
    "- **Input**: $x \\in \\mathbb{R}$\n",
    "- **Output**: $\\text{GELU}(x) = x \\cdot \\Phi(x) = x \\cdot \\frac{1}{2}(1 + \\text{erf}(\\frac{x}{\\sqrt{2}})) \\approx 0.5x(1 + \\tanh(\\sqrt{\\frac{2}{\\pi}}(x + 0.044715x^3)))$\n",
    "- **Đạo hàm**: Phức tạp, thường được tính bằng tự động vi phân\n",
    "\n",
    "**Tham số**: Không có tham số có thể điều chỉnh\n",
    "\n",
    "**Cơ chế hoạt động**:\n",
    "- Kết hợp tính chất của ReLU và dropout\n",
    "- Nhân đầu vào với xác suất cumulative từ phân phối Gaussian\n",
    "- Mượt hơn ReLU và nhẹ nhàng hơn khi x < 0\n",
    "\n",
    "**Khi nào nên dùng**:\n",
    "- Mô hình ngôn ngữ lớn\n",
    "- Transformer và các kiến trúc tương tự\n",
    "- Thay thế ReLU/LeakyReLU trong các mạng sâu\n",
    "\n",
    "**Hạn chế**:\n",
    "- Tính toán phức tạp hơn ReLU\n",
    "- Độ cải thiện có thể không đáng kể trong một số trường hợp\n",
    "- Xấp xỉ được sử dụng trong thực tế có thể không chính xác hoàn toàn\n",
    "\n",
    "**Ứng dụng mạnh mẽ**:\n",
    "- BERT, GPT và các mô hình Transformer\n",
    "- Mạng neural lớn và sâu\n",
    "- Các bài toán xử lý ngôn ngữ tự nhiên hiện đại\n",
    "\n",
    "## So sánh và lựa chọn Activation Function\n",
    "\n",
    "### Bảng tổng hợp\n",
    "\n",
    "| Activation Function | Range | Đạo hàm tại x=0 | Zero-centered | Tính toán | Vanishing Gradient | Dying Neuron |\n",
    "|---------------------|-------|----------------|---------------|-----------|-------------------|--------------|\n",
    "| Sigmoid | (0, 1) | 0.25 | Không | Trung bình | Có | Không |\n",
    "| Tanh | (-1, 1) | 1 | Có | Trung bình | Có | Không |\n",
    "| ReLU | [0, ∞) | Không định nghĩa | Không | Nhanh | Không (x>0) | Có |\n",
    "| Leaky ReLU | ℝ | Không định nghĩa | Không | Nhanh | Hiếm | Hiếm |\n",
    "| PReLU | ℝ | Không định nghĩa | Không | Nhanh | Hiếm | Hiếm |\n",
    "| ELU | (-α, ∞) | α | Gần | Trung bình | Hiếm | Không |\n",
    "| SELU | (-λα, ∞) | λα | Đúng* | Trung bình | Không* | Không |\n",
    "| Swish | ℝ | 0.5 | Gần | Chậm | Hiếm | Không |\n",
    "| GELU | ℝ | 0 | Gần | Chậm | Hiếm | Không |\n",
    "| Softmax | (0, 1) | Varies | Không | Chậm | N/A | N/A |\n",
    "\n",
    "*: Với khởi tạo và sử dụng đúng cách\n",
    "\n",
    "### Hướng dẫn lựa chọn\n",
    "\n",
    "1. **Cho layer output**:\n",
    "   - **Phân loại nhị phân**: Sigmoid\n",
    "   - **Phân loại đa lớp**: Softmax\n",
    "   - **Hồi quy với giá trị không giới hạn**: Linear\n",
    "   - **Hồi quy với giá trị dương**: ReLU\n",
    "\n",
    "2. **Cho layer ẩn**:\n",
    "   - **Mặc định/general**: ReLU hoặc Leaky ReLU\n",
    "   - **CNN**: ReLU, Leaky ReLU, Swish\n",
    "   - **RNN**: Tanh, LSTM/GRU gates: Sigmoid\n",
    "   - **Transformer**: GELU \n",
    "   - **FNN sâu không có BN**: SELU\n",
    "   - **Muốn mạng tự tối ưu**: PReLU\n",
    "   \n",
    "3. **Theo tác vụ**:\n",
    "   - **Nhận dạng ảnh**: ReLU, Leaky ReLU, Swish\n",
    "   - **NLP**: GELU, ReLU, ELU\n",
    "   - **Chuỗi thời gian**: Tanh, LSTM/GRU\n",
    "   - **Hồi quy**: ReLU, ELU, Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Các Mô hình Neural Network\n",
    "\n",
    "## 1. Mạng Perceptron Đa lớp (Multi-Layer Perceptron - MLP)\n",
    "\n",
    "### Kiến trúc\n",
    "- **Cấu trúc cơ bản**: Input layer → Hidden layer(s) → Output layer\n",
    "- **Kết nối**: Fully connected giữa các layer liên tiếp\n",
    "- **Activation**: ReLU, Sigmoid, Tanh cho hidden layers; Softmax/Sigmoid cho output layer\n",
    "- **Độ phức tạp**: O(n²) với n là số neuron trung bình mỗi layer\n",
    "\n",
    "### Cơ chế mô hình\n",
    "- Mỗi neuron áp dụng phép biến đổi tuyến tính (wx + b) rồi qua activation function\n",
    "- Học thông qua backpropagation và gradient descent\n",
    "- Khả năng xấp xỉ hàm đa dạng theo Universal Approximation Theorem\n",
    "\n",
    "### Khi nào nên dùng\n",
    "- **Nên dùng khi**:\n",
    "  - Dữ liệu có cấu trúc bảng (tabular data)\n",
    "  - Số lượng features vừa phải\n",
    "  - Bài toán phân loại/hồi quy đơn giản đến trung bình\n",
    "  - Cần mô hình dễ triển khai, nhẹ\n",
    "\n",
    "- **Hạn chế**:\n",
    "  - Không hiệu quả với dữ liệu có cấu trúc không gian (ảnh, âm thanh)\n",
    "  - Dễ overfitting với dữ liệu kích thước lớn\n",
    "  - Khó mở rộng với số lượng features cực lớn\n",
    "  - Không nắm bắt được quan hệ thời gian/không gian\n",
    "\n",
    "### Ứng dụng mạnh mẽ\n",
    "- Phân loại/hồi quy trên dữ liệu có cấu trúc bảng\n",
    "- Dự đoán tín dụng, phát hiện gian lận\n",
    "- Layer cuối trong các mô hình phức tạp hơn\n",
    "- Embedding học máy cổ điển\n",
    "\n",
    "## 2. Mạng Tích chập (Convolutional Neural Network - CNN)\n",
    "\n",
    "### Kiến trúc\n",
    "- **Cấu trúc cơ bản**: Convolutional layers → Pooling layers → Fully connected layers\n",
    "- **Thành phần chính**: \n",
    "  - Convolutional layers: áp dụng bộ lọc trên vùng cục bộ\n",
    "  - Pooling layers: giảm kích thước không gian (max/average pooling)\n",
    "  - Fully connected layers: phân loại dựa trên features đã trích xuất\n",
    "- **Biến thể hiện đại**: ResNet, Inception, EfficientNet, ConvNeXt\n",
    "\n",
    "### Cơ chế mô hình\n",
    "- Sử dụng phép tích chập (convolution) để trích xuất đặc trưng cục bộ\n",
    "- Tận dụng tính chất bất biến với dịch chuyển (translation invariance)\n",
    "- Chia sẻ tham số (parameter sharing) giảm số lượng tham số cần học\n",
    "- Trích xuất đặc trưng theo hệ thống phân cấp (hierarchical feature extraction)\n",
    "\n",
    "### Khi nào nên dùng\n",
    "- **Nên dùng khi**:\n",
    "  - Dữ liệu có cấu trúc không gian (ảnh, video, tín hiệu 1D/2D/3D)\n",
    "  - Các pattern cục bộ quan trọng\n",
    "  - Cần khai thác thông tin không gian\n",
    "  - Khối lượng dữ liệu lớn\n",
    "\n",
    "- **Hạn chế**:\n",
    "  - Đòi hỏi nhiều dữ liệu để tránh overfitting\n",
    "  - Tốn kém tính toán cho hình ảnh kích thước lớn\n",
    "  - Khó nắm bắt mối quan hệ toàn cục trong ảnh\n",
    "  - Thiếu đặc tính bất biến với phép quay, tỷ lệ\n",
    "\n",
    "### Ứng dụng mạnh mẽ\n",
    "- Nhận dạng ảnh và phân loại\n",
    "- Phát hiện đối tượng (object detection)\n",
    "- Phân đoạn ảnh (image segmentation)\n",
    "- Xử lý ngôn ngữ tự nhiên (trước Transformer)\n",
    "- Phân tích chuỗi thời gian, tín hiệu y sinh\n",
    "\n",
    "## 3. Mạng Hồi quy (Recurrent Neural Network - RNN)\n",
    "\n",
    "### Kiến trúc\n",
    "- **Cấu trúc cơ bản**: Cells có kết nối hồi tiếp (feedback connections)\n",
    "- **Biến thể quan trọng**:\n",
    "  - LSTM (Long Short-Term Memory): giải quyết vấn đề vanishing gradient\n",
    "  - GRU (Gated Recurrent Unit): phiên bản nhẹ hơn của LSTM\n",
    "  - Bidirectional RNN: xử lý chuỗi theo cả hai chiều\n",
    "  - Deep RNN: xếp chồng nhiều layer RNN\n",
    "\n",
    "### Cơ chế mô hình\n",
    "- Duy trì trạng thái ẩn (hidden state) qua các bước thời gian\n",
    "- LSTM/GRU sử dụng cơ chế cổng (gates) để kiểm soát luồng thông tin\n",
    "- Cho phép mô hình \"nhớ\" thông tin từ các bước trước đó\n",
    "- Truyền gradient qua thời gian (BPTT - Backpropagation Through Time)\n",
    "\n",
    "### Khi nào nên dùng\n",
    "- **Nên dùng khi**:\n",
    "  - Dữ liệu dạng chuỗi (văn bản, thời gian, âm thanh)\n",
    "  - Cần nắm bắt phụ thuộc thời gian\n",
    "  - Độ dài đầu vào thay đổi\n",
    "  - Cần \"nhớ\" thông tin trước đó\n",
    "\n",
    "- **Hạn chế**:\n",
    "  - Huấn luyện chậm do tính tuần tự\n",
    "  - Vấn đề gradient biến mất/bùng nổ\n",
    "  - Khó nắm bắt phụ thuộc rất dài (LSTM/GRU đỡ hơn)\n",
    "  - Đã bị Transformer vượt qua trong nhiều tác vụ NLP\n",
    "\n",
    "### Ứng dụng mạnh mẽ\n",
    "- Xử lý ngôn ngữ tự nhiên (trước Transformer)\n",
    "- Dự báo chuỗi thời gian\n",
    "- Phân tích cảm xúc\n",
    "- Nhận dạng giọng nói\n",
    "- Sinh nhạc, văn bản\n",
    "\n",
    "## 4. Transformer\n",
    "\n",
    "### Kiến trúc\n",
    "- **Cấu trúc cơ bản**: Encoder-Decoder với Self-Attention\n",
    "- **Thành phần chính**:\n",
    "  - Multi-head Self-Attention: cho phép mô hình tập trung vào các phần khác nhau\n",
    "  - Position Encoding: đưa thông tin vị trí vào mô hình\n",
    "  - Feed-Forward Networks: xử lý từng vị trí độc lập\n",
    "  - Layer Normalization và Residual Connections\n",
    "- **Biến thể quan trọng**: BERT, GPT, T5, ViT (Vision Transformer)\n",
    "\n",
    "### Cơ chế mô hình\n",
    "- Thay thế xử lý tuần tự bằng cơ chế attention song song\n",
    "- Sử dụng self-attention để nắm bắt mối quan hệ giữa các phần tử\n",
    "- Mỗi phần tử có thể tương tác trực tiếp với tất cả phần tử khác\n",
    "- Encoder nắm bắt ngữ cảnh bidirectional, Decoder tạo đầu ra tự hồi quy\n",
    "\n",
    "### Khi nào nên dùng\n",
    "- **Nên dùng khi**:\n",
    "  - Bài toán NLP (dịch máy, tóm tắt, phân loại văn bản)\n",
    "  - Cần nắm bắt mối quan hệ toàn cục giữa các phần từ xa\n",
    "  - Đủ tài nguyên tính toán và dữ liệu lớn\n",
    "  - Cần state-of-the-art performance\n",
    "\n",
    "- **Hạn chế**:\n",
    "  - Độ phức tạp O(n²) theo độ dài chuỗi\n",
    "  - Đòi hỏi nhiều dữ liệu và tài nguyên tính toán\n",
    "  - Cần kỹ thuật đặc biệt cho chuỗi dài\n",
    "  - Mô hình thường rất lớn\n",
    "\n",
    "### Ứng dụng mạnh mẽ\n",
    "- Dịch máy\n",
    "- Mô hình ngôn ngữ lớn (LLM)\n",
    "- Tóm tắt văn bản\n",
    "- Trả lời câu hỏi\n",
    "- Phân tích cảm xúc\n",
    "- Vision Transformer cho xử lý ảnh\n",
    "\n",
    "## 5. Generative Adversarial Networks (GAN)\n",
    "\n",
    "### Kiến trúc\n",
    "- **Cấu trúc cơ bản**: Hai mạng đấu tranh - Generator và Discriminator\n",
    "- **Generator**: Tạo dữ liệu giả từ nhiễu ngẫu nhiên\n",
    "- **Discriminator**: Phân biệt dữ liệu thật/giả\n",
    "- **Biến thể quan trọng**: DCGAN, StyleGAN, CycleGAN, Pix2Pix, BigGAN\n",
    "\n",
    "### Cơ chế mô hình\n",
    "- Đào tạo song song hai mạng với mục tiêu đối lập\n",
    "- Generator cố gắng tạo dữ liệu đánh lừa Discriminator\n",
    "- Discriminator cố gắng phân biệt dữ liệu thật/giả chính xác\n",
    "- Zero-sum game: Generator được cải thiện khi Discriminator bị đánh lừa\n",
    "\n",
    "### Khi nào nên dùng\n",
    "- **Nên dùng khi**:\n",
    "  - Cần tạo dữ liệu mới chất lượng cao\n",
    "  - Có dữ liệu huấn luyện chất lượng tốt\n",
    "  - Cần mô phỏng phân phối dữ liệu phức tạp\n",
    "  - Bài toán chuyển đổi domain (domain translation)\n",
    "\n",
    "- **Hạn chế**:\n",
    "  - Khó huấn luyện, bất ổn định\n",
    "  - Mode collapse (sinh ra ít mẫu đa dạng)\n",
    "  - Đánh giá hiệu suất khó khăn\n",
    "  - Tốn kém tài nguyên tính toán\n",
    "\n",
    "### Ứng dụng mạnh mẽ\n",
    "- Tạo hình ảnh chân thực\n",
    "- Chuyển đổi hình ảnh (image-to-image translation)\n",
    "- Tăng cường dữ liệu (data augmentation)\n",
    "- Super-resolution\n",
    "- Tổng hợp khuôn mặt, giọng nói\n",
    "- Thiết kế thuốc, vật liệu\n",
    "\n",
    "## 6. Autoencoder\n",
    "\n",
    "### Kiến trúc\n",
    "- **Cấu trúc cơ bản**: Encoder → Latent Space → Decoder\n",
    "- **Encoder**: Nén dữ liệu thành biểu diễn thấp chiều (latent representation)\n",
    "- **Decoder**: Tái tạo dữ liệu gốc từ biểu diễn latent\n",
    "- **Biến thể quan trọng**: \n",
    "  - Variational Autoencoder (VAE): thêm ràng buộc xác suất cho latent space\n",
    "  - Denoising Autoencoder: tái tạo dữ liệu sạch từ dữ liệu nhiễu\n",
    "  - Sparse Autoencoder: áp đặt tính thưa thớt lên latent space\n",
    "  - Contractive Autoencoder: tăng tính ổn định của biểu diễn\n",
    "\n",
    "### Cơ chế mô hình\n",
    "- Học biểu diễn dữ liệu không giám sát\n",
    "- Tối thiểu hóa reconstruction error\n",
    "- VAE học một phân phối xác suất cho latent space\n",
    "- Nén thông tin thành các đặc trưng thiết yếu nhất\n",
    "\n",
    "### Khi nào nên dùng\n",
    "- **Nên dùng khi**:\n",
    "  - Cần giảm chiều dữ liệu\n",
    "  - Phát hiện bất thường (anomaly detection)\n",
    "  - Học biểu diễn không giám sát\n",
    "  - Cần tạo dữ liệu với điều kiện nhất định (VAE)\n",
    "\n",
    "- **Hạn chế**:\n",
    "  - Biểu diễn có thể không phân tách tốt các lớp\n",
    "  - VAE thường tạo ra kết quả mờ (blurry)\n",
    "  - Khó đánh giá chất lượng biểu diễn học được\n",
    "  - Cần tinh chỉnh kiến trúc cho từng loại dữ liệu\n",
    "\n",
    "### Ứng dụng mạnh mẽ\n",
    "- Giảm chiều dữ liệu\n",
    "- Phát hiện bất thường\n",
    "- Loại bỏ nhiễu (denoising)\n",
    "- Sinh dữ liệu có điều kiện (VAE)\n",
    "- Hoàn thiện dữ liệu bị thiếu (image inpainting)\n",
    "- Nén dữ liệu\n",
    "\n",
    "## 7. Vision Transformer (ViT)\n",
    "\n",
    "### Kiến trúc\n",
    "- **Cấu trúc cơ bản**: Image patching → Linear embedding → Transformer Encoder\n",
    "- **Thành phần chính**:\n",
    "  - Patch Embedding: chia nhỏ ảnh thành các patch và embedding\n",
    "  - Position Embedding: thêm thông tin vị trí\n",
    "  - Transformer Encoder: xử lý các patch như tokens trong NLP\n",
    "  - MLP Head: layer cuối dùng cho phân loại\n",
    "\n",
    "### Cơ chế mô hình\n",
    "- Chia ảnh thành các patch không chồng lấp\n",
    "- Áp dụng kiến trúc Transformer từ NLP cho các patch\n",
    "- Self-attention giữa tất cả các patch\n",
    "- Không sử dụng convolution, pooling như CNN truyền thống\n",
    "\n",
    "### Khi nào nên dùng\n",
    "- **Nên dùng khi**:\n",
    "  - Có lượng dữ liệu huấn luyện lớn\n",
    "  - Đủ tài nguyên tính toán\n",
    "  - Cần hiệu suất cao nhất\n",
    "  - Cần nắm bắt mối quan hệ toàn cục trong ảnh\n",
    "\n",
    "- **Hạn chế**:\n",
    "  - Tốn kém tính toán với ảnh độ phân giải cao\n",
    "  - Cần nhiều dữ liệu hơn CNN để đạt hiệu quả tương đương\n",
    "  - Thiếu inductive bias phù hợp cho xử lý ảnh\n",
    "  - Khó triển khai trên thiết bị yếu\n",
    "\n",
    "### Ứng dụng mạnh mẽ\n",
    "- Phân loại ảnh\n",
    "- Phát hiện đối tượng\n",
    "- Phân đoạn ảnh\n",
    "- Ước tính tư thế (pose estimation)\n",
    "- Transfer learning cho các tác vụ thị giác máy tính\n",
    "\n",
    "## 8. Graph Neural Networks (GNN)\n",
    "\n",
    "### Kiến trúc\n",
    "- **Cấu trúc cơ bản**: Các layer xử lý thông tin giữa các node trong đồ thị\n",
    "- **Thành phần chính**:\n",
    "  - Node embeddings: biểu diễn vector của các node\n",
    "  - Message passing: cập nhật thông tin giữa các node kề nhau\n",
    "  - Graph pooling: tổng hợp thông tin toàn đồ thị\n",
    "  - Readout function: tạo biểu diễn đồ thị toàn cục\n",
    "\n",
    "- **Biến thể quan trọng**:\n",
    "  - Graph Convolutional Network (GCN)\n",
    "  - Graph Attention Network (GAT)\n",
    "  - GraphSAGE\n",
    "  - Graph Isomorphism Network (GIN)\n",
    "\n",
    "### Cơ chế mô hình\n",
    "- Học biểu diễn cho các node, edge và toàn bộ đồ thị\n",
    "- Lan truyền thông tin giữa các node kề nhau\n",
    "- Kết hợp đặc trưng node và cấu trúc liên kết\n",
    "- Giữ tính bất biến với thứ tự node (permutation invariant)\n",
    "\n",
    "### Khi nào nên dùng\n",
    "- **Nên dùng khi**:\n",
    "  - Dữ liệu có cấu trúc đồ thị/mạng\n",
    "  - Cần nắm bắt mối quan hệ giữa các thực thể\n",
    "  - Làm việc với dữ liệu không thuần nhất (heterogeneous)\n",
    "  - Cần biểu diễn cả node và cấu trúc kết nối\n",
    "\n",
    "- **Hạn chế**:\n",
    "  - Khó xử lý đồ thị rất lớn\n",
    "  - Vấn đề over-smoothing sau nhiều layer\n",
    "  - Tốn kém tính toán cho đồ thị dày đặc\n",
    "  - Khó áp dụng nhiều layer so với CNN/RNN\n",
    "\n",
    "### Ứng dụng mạnh mẽ\n",
    "- Dự đoán liên kết trong mạng xã hội\n",
    "- Phát hiện gian lận/bất thường\n",
    "- Khoa học phân tử, thiết kế thuốc\n",
    "- Hệ thống gợi ý\n",
    "- Dự báo giao thông\n",
    "- Phân tích mạng lưới diện rộng\n",
    "\n",
    "## 9. Diffusion Models\n",
    "\n",
    "### Kiến trúc\n",
    "- **Cấu trúc cơ bản**: U-Net với attention hoặc Transformer-based backbone\n",
    "- **Thành phần chính**:\n",
    "  - Noise predictor/denoiser: dự đoán nhiễu đã thêm vào\n",
    "  - Time/step embedding: cung cấp thông tin về bước nhiễu\n",
    "  - Cross-attention (cho models có điều kiện)\n",
    "  \n",
    "### Cơ chế mô hình\n",
    "- **Forward process**: Thêm nhiễu Gaussian dần dần vào dữ liệu\n",
    "- **Reverse process**: Học cách loại bỏ nhiễu từng bước nhỏ\n",
    "- Tạo dữ liệu bằng cách bắt đầu từ nhiễu hoàn toàn và dần \"khử nhiễu\"\n",
    "- Có thể thêm điều kiện (text, class) thông qua cross-attention\n",
    "\n",
    "### Khi nào nên dùng\n",
    "- **Nên dùng khi**:\n",
    "  - Cần tạo dữ liệu chất lượng cao\n",
    "  - Muốn kiểm soát quá trình tạo\n",
    "  - Sinh dữ liệu có điều kiện (text-to-image)\n",
    "  - Cần độ đa dạng tốt hơn so với GAN\n",
    "  \n",
    "- **Hạn chế**:\n",
    "  - Quá trình sampling chậm (cần nhiều bước)\n",
    "  - Tốn kém tính toán khi huấn luyện và sinh dữ liệu\n",
    "  - Cần tinh chỉnh lịch trình nhiễu (noise schedule)\n",
    "  - Mô hình thường rất lớn\n",
    "\n",
    "### Ứng dụng mạnh mẽ\n",
    "- Tạo hình ảnh chất lượng cao (Stable Diffusion, DALLE)\n",
    "- Text-to-image generation\n",
    "- Chỉnh sửa ảnh (image editing)\n",
    "- Tạo âm thanh, nhạc\n",
    "- Super-resolution\n",
    "- Hoàn thiện dữ liệu bị thiếu (inpainting)\n",
    "\n",
    "## 10. Self-Supervised Models\n",
    "\n",
    "### Kiến trúc\n",
    "- **Cấu trúc cơ bản**: Encoder (+ Decoder tùy thuộc vào task)\n",
    "- **Biến thể quan trọng**:\n",
    "  - Contrastive learning models (SimCLR, MoCo)\n",
    "  - Masked Autoencoder (MAE)\n",
    "  - BERT, GPT (cho NLP)\n",
    "  - CLIP (multi-modal)\n",
    "  \n",
    "### Cơ chế mô hình\n",
    "- Tạo tác vụ giả (pretext tasks) từ dữ liệu không nhãn\n",
    "- Học biểu diễn chung (representations) mà không cần nhãn\n",
    "- Contrastive learning: tối đa hóa sự tương đồng giữa các augmentation của cùng mẫu\n",
    "- Masked prediction: dự đoán phần bị che của đầu vào\n",
    "\n",
    "### Khi nào nên dùng\n",
    "- **Nên dùng khi**:\n",
    "  - Có nhiều dữ liệu không nhãn, ít dữ liệu có nhãn\n",
    "  - Cần biểu diễn tổng quát cho nhiều tác vụ downstream\n",
    "  - Transfer learning\n",
    "  - Cải thiện khả năng mô hình với dữ liệu ít\n",
    "\n",
    "- **Hạn chế**:\n",
    "  - Có thể tốn kém tính toán khi pre-train\n",
    "  - Thiết kế pretext task phù hợp là khó\n",
    "  - Có thể cần fine-tuning cho tác vụ cụ thể\n",
    "  - Chất lượng phụ thuộc vào kỹ thuật data augmentation\n",
    "\n",
    "### Ứng dụng mạnh mẽ\n",
    "- Pre-training cho tác vụ computer vision\n",
    "- Pre-training cho NLP\n",
    "- Representation learning\n",
    "- Few-shot/zero-shot learning\n",
    "- Cross-modal learning (text-image)\n",
    "- Xử lý dữ liệu không nhãn quy mô lớn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input 1x3x3(CxHxW) => Kernel 1x1x2x2 (NxCxHxW) => Conv 1x2x2 => BatchNorm 1x2x2 => ReLU 1x2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: After Conv = tensor([[[[ 7.,  9.],\n",
      "          [13., 15.]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 0: After BatchNorm = tensor([[[[-1.2649, -0.6325],\n",
      "          [ 0.6325,  1.2649]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 0: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.6325, 1.2649]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 0: Loss = 81.83684539794922\n",
      "Epoch 5: After Conv = tensor([[[[ 7.0000,  9.0000],\n",
      "          [13.0000, 15.0000]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 5: After BatchNorm = tensor([[[[-1.3700, -0.4882],\n",
      "          [ 1.2753,  2.1570]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 5: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [1.2753, 2.1570]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 5: Loss = 75.66133117675781\n",
      "Epoch 10: After Conv = tensor([[[[ 7.0000,  9.0000],\n",
      "          [13.0000, 15.0000]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 10: After BatchNorm = tensor([[[[-1.4700, -0.3509],\n",
      "          [ 1.8874,  3.0065]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 10: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [1.8874, 3.0065]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 10: Loss = 70.06181335449219\n",
      "Epoch 15: After Conv = tensor([[[[ 7.0000,  9.0000],\n",
      "          [13.0000, 15.0000]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 15: After BatchNorm = tensor([[[[-1.5653, -0.2201],\n",
      "          [ 2.4702,  3.8154]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 15: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [2.4702, 3.8154]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 15: Loss = 64.98455810546875\n",
      "Epoch 20: After Conv = tensor([[[[ 7.0000,  9.0000],\n",
      "          [13.0000, 15.0000]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 20: After BatchNorm = tensor([[[[-1.6561, -0.0957],\n",
      "          [ 3.0252,  4.5857]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 20: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [3.0252, 4.5857]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 20: Loss = 60.3808708190918\n",
      "Epoch 25: After Conv = tensor([[[[ 7.0000,  9.0000],\n",
      "          [13.0000, 15.0000]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 25: After BatchNorm = tensor([[[[-1.7426,  0.0228],\n",
      "          [ 3.5537,  5.3191]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 25: After ReLU = tensor([[[[0.0000, 0.0228],\n",
      "          [3.5537, 5.3191]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 25: Loss = 56.14951705932617\n",
      "Epoch 30: After Conv = tensor([[[[ 7.0000,  9.0000],\n",
      "          [13.0000, 15.0000]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 30: After BatchNorm = tensor([[[[-1.6031,  0.3080],\n",
      "          [ 4.1302,  6.0412]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 30: After ReLU = tensor([[[[0.0000, 0.3080],\n",
      "          [4.1302, 6.0412]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 30: Loss = 51.39940643310547\n",
      "Epoch 35: After Conv = tensor([[[[ 7.0000,  9.0000],\n",
      "          [13.0000, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 35: After BatchNorm = tensor([[[[-1.4660,  0.5823],\n",
      "          [ 4.6787,  6.7269]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 35: After ReLU = tensor([[[[0.0000, 0.5823],\n",
      "          [4.6787, 6.7269]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 35: Loss = 47.09434509277344\n",
      "Epoch 40: After Conv = tensor([[[[ 7.0000,  9.0000],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 40: After BatchNorm = tensor([[[[-1.3313,  0.8461],\n",
      "          [ 5.2007,  7.3780]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 40: After ReLU = tensor([[[[0.0000, 0.8461],\n",
      "          [5.2007, 7.3780]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 40: Loss = 43.192100524902344\n",
      "Epoch 45: After Conv = tensor([[[[ 7.0000,  9.0000],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 45: After BatchNorm = tensor([[[[-1.1989,  1.0999],\n",
      "          [ 5.6974,  7.9962]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 45: After ReLU = tensor([[[[0.0000, 1.0999],\n",
      "          [5.6974, 7.9962]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 45: Loss = 39.654457092285156\n",
      "Epoch 50: After Conv = tensor([[[[ 7.0000,  9.0000],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 50: After BatchNorm = tensor([[[[-1.0688,  1.3442],\n",
      "          [ 6.1701,  8.5831]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 50: After ReLU = tensor([[[[0.0000, 1.3442],\n",
      "          [6.1701, 8.5831]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 50: Loss = 36.44684600830078\n",
      "Epoch 55: After Conv = tensor([[[[ 7.0000,  9.0000],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 55: After BatchNorm = tensor([[[[-0.9410,  1.5793],\n",
      "          [ 6.6200,  9.1404]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 55: After ReLU = tensor([[[[0.0000, 1.5793],\n",
      "          [6.6200, 9.1404]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 55: Loss = 33.538002014160156\n",
      "Epoch 60: After Conv = tensor([[[[ 7.0000,  9.0000],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 60: After BatchNorm = tensor([[[[-0.8154,  1.8058],\n",
      "          [ 7.0481,  9.6693]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 60: After ReLU = tensor([[[[0.0000, 1.8058],\n",
      "          [7.0481, 9.6693]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 60: Loss = 30.899614334106445\n",
      "Epoch 65: After Conv = tensor([[[[ 7.0000,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 65: After BatchNorm = tensor([[[[-0.6920,  2.0238],\n",
      "          [ 7.4556, 10.1715]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 65: After ReLU = tensor([[[[ 0.0000,  2.0238],\n",
      "          [ 7.4556, 10.1715]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 65: Loss = 28.506092071533203\n",
      "Epoch 70: After Conv = tensor([[[[ 7.0000,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 70: After BatchNorm = tensor([[[[-0.5708,  2.2339],\n",
      "          [ 7.8434, 10.6481]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 70: After ReLU = tensor([[[[ 0.0000,  2.2339],\n",
      "          [ 7.8434, 10.6481]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 70: Loss = 26.33429718017578\n",
      "Epoch 75: After Conv = tensor([[[[ 7.0000,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 75: After BatchNorm = tensor([[[[-0.4516,  2.4364],\n",
      "          [ 8.2124, 11.1005]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 75: After ReLU = tensor([[[[ 0.0000,  2.4364],\n",
      "          [ 8.2124, 11.1005]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 75: Loss = 24.36324691772461\n",
      "Epoch 80: After Conv = tensor([[[[ 7.0000,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 80: After BatchNorm = tensor([[[[-0.3345,  2.6315],\n",
      "          [ 8.5637, 11.5298]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 80: After ReLU = tensor([[[[ 0.0000,  2.6315],\n",
      "          [ 8.5637, 11.5298]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 80: Loss = 22.573993682861328\n",
      "Epoch 85: After Conv = tensor([[[[ 7.0000,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 85: After BatchNorm = tensor([[[[-0.2195,  2.8197],\n",
      "          [ 8.8980, 11.9372]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 85: After ReLU = tensor([[[[ 0.0000,  2.8197],\n",
      "          [ 8.8980, 11.9372]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 85: Loss = 20.949386596679688\n",
      "Epoch 90: After Conv = tensor([[[[ 7.0000,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 90: After BatchNorm = tensor([[[[-0.1064,  3.0011],\n",
      "          [ 9.2163, 12.3239]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 90: After ReLU = tensor([[[[ 0.0000,  3.0011],\n",
      "          [ 9.2163, 12.3239]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 90: Loss = 19.473907470703125\n",
      "Epoch 95: After Conv = tensor([[[[ 7.0000,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 95: After BatchNorm = tensor([[[[4.6349e-03, 3.1762e+00],\n",
      "          [9.5192e+00, 1.2691e+01]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 95: After ReLU = tensor([[[[4.6349e-03, 3.1762e+00],\n",
      "          [9.5192e+00, 1.2691e+01]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 95: Loss = 18.127714157104492\n",
      "Epoch 100: After Conv = tensor([[[[ 7.0000,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 100: After BatchNorm = tensor([[[[ 0.2715,  3.4542],\n",
      "          [ 9.8197, 13.0024]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 100: After ReLU = tensor([[[[ 0.2715,  3.4542],\n",
      "          [ 9.8197, 13.0024]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 100: Loss = 16.455944061279297\n",
      "Epoch 105: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 105: After BatchNorm = tensor([[[[ 0.5253,  3.7187],\n",
      "          [10.1054, 13.2988]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 105: After ReLU = tensor([[[[ 0.5253,  3.7187],\n",
      "          [10.1054, 13.2988]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 105: Loss = 14.944026947021484\n",
      "Epoch 110: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 110: After BatchNorm = tensor([[[[ 0.7667,  3.9702],\n",
      "          [10.3772, 13.5806]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 110: After ReLU = tensor([[[[ 0.7667,  3.9702],\n",
      "          [10.3772, 13.5806]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 110: Loss = 13.576645851135254\n",
      "Epoch 115: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 115: After BatchNorm = tensor([[[[ 0.9962,  4.2093],\n",
      "          [10.6356, 13.8487]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 115: After ReLU = tensor([[[[ 0.9962,  4.2093],\n",
      "          [10.6356, 13.8487]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 115: Loss = 12.340033531188965\n",
      "Epoch 120: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 120: After BatchNorm = tensor([[[[ 1.2145,  4.4368],\n",
      "          [10.8813, 14.1036]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 120: After ReLU = tensor([[[[ 1.2145,  4.4368],\n",
      "          [10.8813, 14.1036]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 120: Loss = 11.221651077270508\n",
      "Epoch 125: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 125: After BatchNorm = tensor([[[[ 1.4221,  4.6531],\n",
      "          [11.1150, 14.3460]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 125: After ReLU = tensor([[[[ 1.4221,  4.6531],\n",
      "          [11.1150, 14.3460]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 125: Loss = 10.21021842956543\n",
      "Epoch 130: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 130: After BatchNorm = tensor([[[[ 1.6195,  4.8587],\n",
      "          [11.3373, 14.5766]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 130: After ReLU = tensor([[[[ 1.6195,  4.8587],\n",
      "          [11.3373, 14.5766]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 130: Loss = 9.295490264892578\n",
      "Epoch 135: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 135: After BatchNorm = tensor([[[[ 1.8072,  5.0544],\n",
      "          [11.5487, 14.7958]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 135: After ReLU = tensor([[[[ 1.8072,  5.0544],\n",
      "          [11.5487, 14.7958]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 135: Loss = 8.468229293823242\n",
      "Epoch 140: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 140: After BatchNorm = tensor([[[[ 1.9857,  5.2404],\n",
      "          [11.7497, 15.0043]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 140: After ReLU = tensor([[[[ 1.9857,  5.2404],\n",
      "          [11.7497, 15.0043]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 140: Loss = 7.720070838928223\n",
      "Epoch 145: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 145: After BatchNorm = tensor([[[[ 2.1555,  5.4173],\n",
      "          [11.9408, 15.2026]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 145: After ReLU = tensor([[[[ 2.1555,  5.4173],\n",
      "          [11.9408, 15.2026]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 145: Loss = 7.043455123901367\n",
      "Epoch 150: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 150: After BatchNorm = tensor([[[[ 2.3170,  5.5855],\n",
      "          [12.1226, 15.3912]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 150: After ReLU = tensor([[[[ 2.3170,  5.5855],\n",
      "          [12.1226, 15.3912]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 150: Loss = 6.431520462036133\n",
      "Epoch 155: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 155: After BatchNorm = tensor([[[[ 2.4705,  5.7455],\n",
      "          [12.2955, 15.5705]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 155: After ReLU = tensor([[[[ 2.4705,  5.7455],\n",
      "          [12.2955, 15.5705]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 155: Loss = 5.878108024597168\n",
      "Epoch 160: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 160: After BatchNorm = tensor([[[[ 2.6166,  5.8977],\n",
      "          [12.4599, 15.7410]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 160: After ReLU = tensor([[[[ 2.6166,  5.8977],\n",
      "          [12.4599, 15.7410]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 160: Loss = 5.377605438232422\n",
      "Epoch 165: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 165: After BatchNorm = tensor([[[[ 2.7554,  6.0424],\n",
      "          [12.6162, 15.9032]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 165: After ReLU = tensor([[[[ 2.7554,  6.0424],\n",
      "          [12.6162, 15.9032]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 165: Loss = 4.9249677658081055\n",
      "Epoch 170: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 170: After BatchNorm = tensor([[[[ 2.8875,  6.1800],\n",
      "          [12.7649, 16.0574]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 170: After ReLU = tensor([[[[ 2.8875,  6.1800],\n",
      "          [12.7649, 16.0574]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 170: Loss = 4.5156097412109375\n",
      "Epoch 175: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 175: After BatchNorm = tensor([[[[ 3.0131,  6.3108],\n",
      "          [12.9063, 16.2041]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 175: After ReLU = tensor([[[[ 3.0131,  6.3108],\n",
      "          [12.9063, 16.2041]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 175: Loss = 4.145390510559082\n",
      "Epoch 180: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 180: After BatchNorm = tensor([[[[ 3.1325,  6.4353],\n",
      "          [13.0408, 16.3436]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 180: After ReLU = tensor([[[[ 3.1325,  6.4353],\n",
      "          [13.0408, 16.3436]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 180: Loss = 3.8105642795562744\n",
      "Epoch 185: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 185: After BatchNorm = tensor([[[[ 3.2461,  6.5536],\n",
      "          [13.1687, 16.4762]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 185: After ReLU = tensor([[[[ 3.2461,  6.5536],\n",
      "          [13.1687, 16.4762]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 185: Loss = 3.5077624320983887\n",
      "Epoch 190: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 190: After BatchNorm = tensor([[[[ 3.3541,  6.6662],\n",
      "          [13.2903, 16.6023]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 190: After ReLU = tensor([[[[ 3.3541,  6.6662],\n",
      "          [13.2903, 16.6023]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 190: Loss = 3.2339067459106445\n",
      "Epoch 195: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 195: After BatchNorm = tensor([[[[ 3.4568,  6.7732],\n",
      "          [13.4059, 16.7223]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 195: After ReLU = tensor([[[[ 3.4568,  6.7732],\n",
      "          [13.4059, 16.7223]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 195: Loss = 2.9862446784973145\n",
      "Epoch 200: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 200: After BatchNorm = tensor([[[[ 3.5545,  6.8750],\n",
      "          [13.5159, 16.8364]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 200: After ReLU = tensor([[[[ 3.5545,  6.8750],\n",
      "          [13.5159, 16.8364]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 200: Loss = 2.762260913848877\n",
      "Epoch 205: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 205: After BatchNorm = tensor([[[[ 3.6474,  6.9718],\n",
      "          [13.6205, 16.9449]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 205: After ReLU = tensor([[[[ 3.6474,  6.9718],\n",
      "          [13.6205, 16.9449]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 205: Loss = 2.559695243835449\n",
      "Epoch 210: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 210: After BatchNorm = tensor([[[[ 3.7358,  7.0638],\n",
      "          [13.7200, 17.0480]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 210: After ReLU = tensor([[[[ 3.7358,  7.0638],\n",
      "          [13.7200, 17.0480]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 210: Loss = 2.376492977142334\n",
      "Epoch 215: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 215: After BatchNorm = tensor([[[[ 3.8198,  7.1514],\n",
      "          [13.8146, 17.1462]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 215: After ReLU = tensor([[[[ 3.8198,  7.1514],\n",
      "          [13.8146, 17.1462]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 215: Loss = 2.2108116149902344\n",
      "Epoch 220: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 220: After BatchNorm = tensor([[[[ 3.8997,  7.2346],\n",
      "          [13.9045, 17.2395]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 220: After ReLU = tensor([[[[ 3.8997,  7.2346],\n",
      "          [13.9045, 17.2395]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 220: Loss = 2.060972213745117\n",
      "Epoch 225: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 225: After BatchNorm = tensor([[[[ 3.9756,  7.3138],\n",
      "          [13.9901, 17.3282]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 225: After ReLU = tensor([[[[ 3.9756,  7.3138],\n",
      "          [13.9901, 17.3282]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 225: Loss = 1.9254615306854248\n",
      "Epoch 230: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 230: After BatchNorm = tensor([[[[ 4.0479,  7.3891],\n",
      "          [14.0714, 17.4126]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 230: After ReLU = tensor([[[[ 4.0479,  7.3891],\n",
      "          [14.0714, 17.4126]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 230: Loss = 1.8029065132141113\n",
      "Epoch 235: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 235: After BatchNorm = tensor([[[[ 4.1166,  7.4607],\n",
      "          [14.1488, 17.4928]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 235: After ReLU = tensor([[[[ 4.1166,  7.4607],\n",
      "          [14.1488, 17.4928]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 235: Loss = 1.6920702457427979\n",
      "Epoch 240: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 240: After BatchNorm = tensor([[[[ 4.1820,  7.5288],\n",
      "          [14.2224, 17.5691]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 240: After ReLU = tensor([[[[ 4.1820,  7.5288],\n",
      "          [14.2224, 17.5691]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 240: Loss = 1.5918318033218384\n",
      "Epoch 245: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 245: After BatchNorm = tensor([[[[ 4.2441,  7.5935],\n",
      "          [14.2923, 17.6417]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 245: After ReLU = tensor([[[[ 4.2441,  7.5935],\n",
      "          [14.2923, 17.6417]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 245: Loss = 1.501180648803711\n",
      "Epoch 250: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 250: After BatchNorm = tensor([[[[ 4.3032,  7.6551],\n",
      "          [14.3589, 17.7107]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 250: After ReLU = tensor([[[[ 4.3032,  7.6551],\n",
      "          [14.3589, 17.7107]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 250: Loss = 1.4191919565200806\n",
      "Epoch 255: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 255: After BatchNorm = tensor([[[[ 4.3594,  7.7137],\n",
      "          [14.4221, 17.7764]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 255: After ReLU = tensor([[[[ 4.3594,  7.7137],\n",
      "          [14.4221, 17.7764]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 255: Loss = 1.3450452089309692\n",
      "Epoch 260: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 260: After BatchNorm = tensor([[[[ 4.4129,  7.7694],\n",
      "          [14.4823, 17.8388]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 260: After ReLU = tensor([[[[ 4.4129,  7.7694],\n",
      "          [14.4823, 17.8388]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 260: Loss = 1.2779868841171265\n",
      "Epoch 265: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 265: After BatchNorm = tensor([[[[ 4.4637,  7.8223],\n",
      "          [14.5395, 17.8982]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 265: After ReLU = tensor([[[[ 4.4637,  7.8223],\n",
      "          [14.5395, 17.8982]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 265: Loss = 1.2173418998718262\n",
      "Epoch 270: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 270: After BatchNorm = tensor([[[[ 4.5120,  7.8727],\n",
      "          [14.5940, 17.9546]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 270: After ReLU = tensor([[[[ 4.5120,  7.8727],\n",
      "          [14.5940, 17.9546]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 270: Loss = 1.1624979972839355\n",
      "Epoch 275: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 275: After BatchNorm = tensor([[[[ 4.5580,  7.9206],\n",
      "          [14.6457, 18.0083]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 275: After ReLU = tensor([[[[ 4.5580,  7.9206],\n",
      "          [14.6457, 18.0083]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 275: Loss = 1.11289644241333\n",
      "Epoch 280: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 280: After BatchNorm = tensor([[[[ 4.6017,  7.9661],\n",
      "          [14.6949, 18.0594]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 280: After ReLU = tensor([[[[ 4.6017,  7.9661],\n",
      "          [14.6949, 18.0594]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 280: Loss = 1.0680373907089233\n",
      "Epoch 285: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 285: After BatchNorm = tensor([[[[ 4.6433,  8.0094],\n",
      "          [14.7418, 18.1079]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 285: After ReLU = tensor([[[[ 4.6433,  8.0094],\n",
      "          [14.7418, 18.1079]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 285: Loss = 1.0274677276611328\n",
      "Epoch 290: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 290: After BatchNorm = tensor([[[[ 4.6828,  8.0506],\n",
      "          [14.7863, 18.1541]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 290: After ReLU = tensor([[[[ 4.6828,  8.0506],\n",
      "          [14.7863, 18.1541]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 290: Loss = 0.9907792806625366\n",
      "Epoch 295: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 295: After BatchNorm = tensor([[[[ 4.7204,  8.0898],\n",
      "          [14.8286, 18.1980]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 295: After ReLU = tensor([[[[ 4.7204,  8.0898],\n",
      "          [14.8286, 18.1980]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 295: Loss = 0.9575979709625244\n",
      "Epoch 300: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 300: After BatchNorm = tensor([[[[ 4.7562,  8.1271],\n",
      "          [14.8689, 18.2397]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 300: After ReLU = tensor([[[[ 4.7562,  8.1271],\n",
      "          [14.8689, 18.2397]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 300: Loss = 0.9275877475738525\n",
      "Epoch 305: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 305: After BatchNorm = tensor([[[[ 4.7902,  8.1625],\n",
      "          [14.9071, 18.2795]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 305: After ReLU = tensor([[[[ 4.7902,  8.1625],\n",
      "          [14.9071, 18.2795]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 305: Loss = 0.9004476070404053\n",
      "Epoch 310: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 310: After BatchNorm = tensor([[[[ 4.8225,  8.1962],\n",
      "          [14.9435, 18.3172]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 310: After ReLU = tensor([[[[ 4.8225,  8.1962],\n",
      "          [14.9435, 18.3172]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 310: Loss = 0.8759024143218994\n",
      "Epoch 315: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 315: After BatchNorm = tensor([[[[ 4.8533,  8.2282],\n",
      "          [14.9782, 18.3531]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 315: After ReLU = tensor([[[[ 4.8533,  8.2282],\n",
      "          [14.9782, 18.3531]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 315: Loss = 0.8537050485610962\n",
      "Epoch 320: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 320: After BatchNorm = tensor([[[[ 4.8825,  8.2587],\n",
      "          [15.0111, 18.3873]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 320: After ReLU = tensor([[[[ 4.8825,  8.2587],\n",
      "          [15.0111, 18.3873]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 320: Loss = 0.8336286544799805\n",
      "Epoch 325: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 325: After BatchNorm = tensor([[[[ 4.9103,  8.2877],\n",
      "          [15.0424, 18.4198]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 325: After ReLU = tensor([[[[ 4.9103,  8.2877],\n",
      "          [15.0424, 18.4198]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 325: Loss = 0.8154735565185547\n",
      "Epoch 330: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 330: After BatchNorm = tensor([[[[ 4.9368,  8.3153],\n",
      "          [15.0722, 18.4507]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 330: After ReLU = tensor([[[[ 4.9368,  8.3153],\n",
      "          [15.0722, 18.4507]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 330: Loss = 0.7990535497665405\n",
      "Epoch 335: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 335: After BatchNorm = tensor([[[[ 4.9619,  8.3415],\n",
      "          [15.1005, 18.4800]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 335: After ReLU = tensor([[[[ 4.9619,  8.3415],\n",
      "          [15.1005, 18.4800]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 335: Loss = 0.7842034101486206\n",
      "Epoch 340: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 340: After BatchNorm = tensor([[[[ 4.9859,  8.3664],\n",
      "          [15.1274, 18.5080]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 340: After ReLU = tensor([[[[ 4.9859,  8.3664],\n",
      "          [15.1274, 18.5080]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 340: Loss = 0.7707740664482117\n",
      "Epoch 345: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 345: After BatchNorm = tensor([[[[ 5.0086,  8.3901],\n",
      "          [15.1530, 18.5345]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 345: After ReLU = tensor([[[[ 5.0086,  8.3901],\n",
      "          [15.1530, 18.5345]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 345: Loss = 0.7586264610290527\n",
      "Epoch 350: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 350: After BatchNorm = tensor([[[[ 5.0302,  8.4126],\n",
      "          [15.1774, 18.5598]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 350: After ReLU = tensor([[[[ 5.0302,  8.4126],\n",
      "          [15.1774, 18.5598]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 350: Loss = 0.747643232345581\n",
      "Epoch 355: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 355: After BatchNorm = tensor([[[[ 5.0508,  8.4341],\n",
      "          [15.2006, 18.5838]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 355: After ReLU = tensor([[[[ 5.0508,  8.4341],\n",
      "          [15.2006, 18.5838]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 355: Loss = 0.7377094626426697\n",
      "Epoch 360: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 360: After BatchNorm = tensor([[[[ 5.0704,  8.4544],\n",
      "          [15.2226, 18.6067]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 360: After ReLU = tensor([[[[ 5.0704,  8.4544],\n",
      "          [15.2226, 18.6067]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 360: Loss = 0.7287240028381348\n",
      "Epoch 365: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 365: After BatchNorm = tensor([[[[ 5.0890,  8.4738],\n",
      "          [15.2435, 18.6284]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 365: After ReLU = tensor([[[[ 5.0890,  8.4738],\n",
      "          [15.2435, 18.6284]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 365: Loss = 0.7205988168716431\n",
      "Epoch 370: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 370: After BatchNorm = tensor([[[[ 5.1067,  8.4923],\n",
      "          [15.2635, 18.6491]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 370: After ReLU = tensor([[[[ 5.1067,  8.4923],\n",
      "          [15.2635, 18.6491]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 370: Loss = 0.7132510542869568\n",
      "Epoch 375: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 375: After BatchNorm = tensor([[[[ 5.1235,  8.5098],\n",
      "          [15.2824, 18.6687]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 375: After ReLU = tensor([[[[ 5.1235,  8.5098],\n",
      "          [15.2824, 18.6687]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 375: Loss = 0.7066064476966858\n",
      "Epoch 380: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 380: After BatchNorm = tensor([[[[ 5.1395,  8.5265],\n",
      "          [15.3004, 18.6874]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 380: After ReLU = tensor([[[[ 5.1395,  8.5265],\n",
      "          [15.3004, 18.6874]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 380: Loss = 0.7005959749221802\n",
      "Epoch 385: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 385: After BatchNorm = tensor([[[[ 5.1547,  8.5423],\n",
      "          [15.3176, 18.7052]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 385: After ReLU = tensor([[[[ 5.1547,  8.5423],\n",
      "          [15.3176, 18.7052]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 385: Loss = 0.6951607465744019\n",
      "Epoch 390: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 390: After BatchNorm = tensor([[[[ 5.1692,  8.5574],\n",
      "          [15.3338, 18.7221]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 390: After ReLU = tensor([[[[ 5.1692,  8.5574],\n",
      "          [15.3338, 18.7221]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 390: Loss = 0.6902446746826172\n",
      "Epoch 395: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 395: After BatchNorm = tensor([[[[ 5.1830,  8.5718],\n",
      "          [15.3493, 18.7381]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 395: After ReLU = tensor([[[[ 5.1830,  8.5718],\n",
      "          [15.3493, 18.7381]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 395: Loss = 0.6857995986938477\n",
      "Epoch 400: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 400: After BatchNorm = tensor([[[[ 5.1960,  8.5854],\n",
      "          [15.3641, 18.7534]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 400: After ReLU = tensor([[[[ 5.1960,  8.5854],\n",
      "          [15.3641, 18.7534]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 400: Loss = 0.6817784905433655\n",
      "Epoch 405: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 405: After BatchNorm = tensor([[[[ 5.2085,  8.5984],\n",
      "          [15.3781, 18.7680]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 405: After ReLU = tensor([[[[ 5.2085,  8.5984],\n",
      "          [15.3781, 18.7680]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 405: Loss = 0.6781411170959473\n",
      "Epoch 410: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 410: After BatchNorm = tensor([[[[ 5.2203,  8.6107],\n",
      "          [15.3914, 18.7818]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 410: After ReLU = tensor([[[[ 5.2203,  8.6107],\n",
      "          [15.3914, 18.7818]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 410: Loss = 0.674853503704071\n",
      "Epoch 415: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 415: After BatchNorm = tensor([[[[ 5.2316,  8.6224],\n",
      "          [15.4041, 18.7949]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 415: After ReLU = tensor([[[[ 5.2316,  8.6224],\n",
      "          [15.4041, 18.7949]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 415: Loss = 0.6718800067901611\n",
      "Epoch 420: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 420: After BatchNorm = tensor([[[[ 5.2423,  8.6336],\n",
      "          [15.4161, 18.8074]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 420: After ReLU = tensor([[[[ 5.2423,  8.6336],\n",
      "          [15.4161, 18.8074]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 420: Loss = 0.6691902875900269\n",
      "Epoch 425: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 425: After BatchNorm = tensor([[[[ 5.2525,  8.6442],\n",
      "          [15.4276, 18.8193]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 425: After ReLU = tensor([[[[ 5.2525,  8.6442],\n",
      "          [15.4276, 18.8193]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 425: Loss = 0.6667577028274536\n",
      "Epoch 430: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 430: After BatchNorm = tensor([[[[ 5.2621,  8.6543],\n",
      "          [15.4385, 18.8306]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 430: After ReLU = tensor([[[[ 5.2621,  8.6543],\n",
      "          [15.4385, 18.8306]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 430: Loss = 0.664557695388794\n",
      "Epoch 435: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 435: After BatchNorm = tensor([[[[ 5.2714,  8.6639],\n",
      "          [15.4489, 18.8414]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 435: After ReLU = tensor([[[[ 5.2714,  8.6639],\n",
      "          [15.4489, 18.8414]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 435: Loss = 0.6625674962997437\n",
      "Epoch 440: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 440: After BatchNorm = tensor([[[[ 5.2801,  8.6730],\n",
      "          [15.4587, 18.8516]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 440: After ReLU = tensor([[[[ 5.2801,  8.6730],\n",
      "          [15.4587, 18.8516]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 440: Loss = 0.6607683300971985\n",
      "Epoch 445: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 445: After BatchNorm = tensor([[[[ 5.2884,  8.6817],\n",
      "          [15.4681, 18.8613]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 445: After ReLU = tensor([[[[ 5.2884,  8.6817],\n",
      "          [15.4681, 18.8613]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 445: Loss = 0.6591410040855408\n",
      "Epoch 450: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 450: After BatchNorm = tensor([[[[ 5.2964,  8.6899],\n",
      "          [15.4770, 18.8706]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 450: After ReLU = tensor([[[[ 5.2964,  8.6899],\n",
      "          [15.4770, 18.8706]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 450: Loss = 0.6576699614524841\n",
      "Epoch 455: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 455: After BatchNorm = tensor([[[[ 5.3039,  8.6978],\n",
      "          [15.4855, 18.8794]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 455: After ReLU = tensor([[[[ 5.3039,  8.6978],\n",
      "          [15.4855, 18.8794]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 455: Loss = 0.6563389897346497\n",
      "Epoch 460: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 460: After BatchNorm = tensor([[[[ 5.3110,  8.7052],\n",
      "          [15.4936, 18.8877]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 460: After ReLU = tensor([[[[ 5.3110,  8.7052],\n",
      "          [15.4936, 18.8877]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 460: Loss = 0.6551342010498047\n",
      "Epoch 465: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 465: After BatchNorm = tensor([[[[ 5.3179,  8.7123],\n",
      "          [15.5012, 18.8957]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 465: After ReLU = tensor([[[[ 5.3179,  8.7123],\n",
      "          [15.5012, 18.8957]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 465: Loss = 0.6540459990501404\n",
      "Epoch 470: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 470: After BatchNorm = tensor([[[[ 5.3243,  8.7191],\n",
      "          [15.5085, 18.9032]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 470: After ReLU = tensor([[[[ 5.3243,  8.7191],\n",
      "          [15.5085, 18.9032]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 470: Loss = 0.6530618667602539\n",
      "Epoch 475: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 475: After BatchNorm = tensor([[[[ 5.3305,  8.7255],\n",
      "          [15.5154, 18.9104]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 475: After ReLU = tensor([[[[ 5.3305,  8.7255],\n",
      "          [15.5154, 18.9104]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 475: Loss = 0.6521716713905334\n",
      "Epoch 480: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 480: After BatchNorm = tensor([[[[ 5.3364,  8.7316],\n",
      "          [15.5220, 18.9173]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 480: After ReLU = tensor([[[[ 5.3364,  8.7316],\n",
      "          [15.5220, 18.9173]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 480: Loss = 0.6513668298721313\n",
      "Epoch 485: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 485: After BatchNorm = tensor([[[[ 5.3419,  8.7374],\n",
      "          [15.5283, 18.9238]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 485: After ReLU = tensor([[[[ 5.3419,  8.7374],\n",
      "          [15.5283, 18.9238]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 485: Loss = 0.6506383419036865\n",
      "Epoch 490: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 490: After BatchNorm = tensor([[[[ 5.3472,  8.7429],\n",
      "          [15.5343, 18.9300]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 490: After ReLU = tensor([[[[ 5.3472,  8.7429],\n",
      "          [15.5343, 18.9300]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 490: Loss = 0.6499794721603394\n",
      "Epoch 495: After Conv = tensor([[[[ 7.0001,  9.0001],\n",
      "          [13.0001, 15.0001]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 495: After BatchNorm = tensor([[[[ 5.3523,  8.7482],\n",
      "          [15.5400, 18.9358]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 495: After ReLU = tensor([[[[ 5.3523,  8.7482],\n",
      "          [15.5400, 18.9358]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 495: Loss = 0.6493848562240601\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tạo input (1x3x3)\n",
    "x = torch.tensor(\n",
    "    [[[[1.0, 2, 3], [4, 5, 6], [7, 8, 9]]]], requires_grad=True\n",
    ")  # Shape (1, 1, 3, 3)\n",
    "\n",
    "# Tạo kernel (2x2) và BatchNorm\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride=1, bias=False)\n",
    "batch_norm = nn.BatchNorm2d(1)  # Thêm BatchNorm\n",
    "relu = nn.ReLU(inplace=True)\n",
    "\n",
    "# Khởi tạo kernel với giá trị cụ thể\n",
    "with torch.no_grad():\n",
    "    conv.weight.copy_(torch.tensor([[[[1.0, -1], [2, 0]]]]))  # Shape (1, 1, 2, 2)\n",
    "\n",
    "# Giả sử nhãn thật\n",
    "y_true = torch.tensor([[[[5.0, 10], [14, 20]]]])\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(list(conv.parameters()) + list(batch_norm.parameters()), lr=0.01)\n",
    "\n",
    "# Lưu lịch sử loss\n",
    "loss_history = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Xóa gradient trước đó\n",
    "\n",
    "    # Forward pass với BatchNorm\n",
    "    output = conv(x)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: After Conv = {output}\")\n",
    "    output_bn = batch_norm(output)  # Áp dụng BatchNorm\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: After BatchNorm = {output_bn}\")\n",
    "    output_rl = relu(output_bn)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: After ReLU = {output_rl}\")\n",
    "\n",
    "    # Tính loss (MSE)\n",
    "    loss = torch.mean((output_rl - y_true) ** 2) / 2\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Cập nhật trọng số\n",
    "    optimizer.step()\n",
    "\n",
    "    # In loss mỗi 5 epoch\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
    "\n",
    "# Vẽ đồ thị loss\n",
    "plt.plot(loss_history, label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss giảm dần qua từng epoch với BatchNorm\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input 1x3x3(CxHxW) => Kernel 3x1x2x2 (NxCxHxW) => Conv 3x2x2 => BatchNorm 3x2x2 => ReLU 3x2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: After Conv = tensor([[[[ 7.,  9.],\n",
      "          [13., 15.]],\n",
      "\n",
      "         [[-4., -3.],\n",
      "          [-1.,  0.]],\n",
      "\n",
      "         [[ 8., 10.],\n",
      "          [14., 16.]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 0: After BatchNorm = tensor([[[[-1.2649, -0.6325],\n",
      "          [ 0.6325,  1.2649]],\n",
      "\n",
      "         [[-1.2649, -0.6325],\n",
      "          [ 0.6325,  1.2649]],\n",
      "\n",
      "         [[-1.2649, -0.6325],\n",
      "          [ 0.6325,  1.2649]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 0: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.6325, 1.2649]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.6325, 1.2649]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.6325, 1.2649]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 0: Loss = 81.83686065673828\n",
      "Epoch 100: After Conv = tensor([[[[ 7.0000e+00,  9.0000e+00],\n",
      "          [ 1.3000e+01,  1.5000e+01]],\n",
      "\n",
      "         [[-3.9998e+00, -2.9997e+00],\n",
      "          [-9.9955e-01,  5.2404e-04]],\n",
      "\n",
      "         [[ 8.0000e+00,  1.0000e+01],\n",
      "          [ 1.4000e+01,  1.6000e+01]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 100: After BatchNorm = tensor([[[[-1.4813,  0.5116],\n",
      "          [ 4.4974,  6.4903]],\n",
      "\n",
      "         [[-1.4812,  0.5117],\n",
      "          [ 4.4974,  6.4903]],\n",
      "\n",
      "         [[-1.4813,  0.5116],\n",
      "          [ 4.4974,  6.4903]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 100: After ReLU = tensor([[[[0.0000, 0.5116],\n",
      "          [4.4974, 6.4903]],\n",
      "\n",
      "         [[0.0000, 0.5117],\n",
      "          [4.4974, 6.4903]],\n",
      "\n",
      "         [[0.0000, 0.5116],\n",
      "          [4.4974, 6.4903]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 100: Loss = 48.479801177978516\n",
      "Epoch 200: After Conv = tensor([[[[ 7.0000e+00,  9.0000e+00],\n",
      "          [ 1.3000e+01,  1.5000e+01]],\n",
      "\n",
      "         [[-3.9996e+00, -2.9994e+00],\n",
      "          [-9.9913e-01,  1.0176e-03]],\n",
      "\n",
      "         [[ 8.0000e+00,  1.0000e+01],\n",
      "          [ 1.4000e+01,  1.6000e+01]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 200: After BatchNorm = tensor([[[[-0.6255,  2.1089],\n",
      "          [ 7.5778, 10.3122]],\n",
      "\n",
      "         [[-0.6255,  2.1089],\n",
      "          [ 7.5778, 10.3122]],\n",
      "\n",
      "         [[-0.6255,  2.1089],\n",
      "          [ 7.5778, 10.3122]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 200: After ReLU = tensor([[[[ 0.0000,  2.1089],\n",
      "          [ 7.5778, 10.3122]],\n",
      "\n",
      "         [[ 0.0000,  2.1089],\n",
      "          [ 7.5778, 10.3122]],\n",
      "\n",
      "         [[ 0.0000,  2.1089],\n",
      "          [ 7.5778, 10.3122]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 200: Loss = 27.795969009399414\n",
      "Epoch 300: After Conv = tensor([[[[ 7.0000e+00,  9.0000e+00],\n",
      "          [ 1.3000e+01,  1.5000e+01]],\n",
      "\n",
      "         [[-3.9994e+00, -2.9992e+00],\n",
      "          [-9.9884e-01,  1.3642e-03]],\n",
      "\n",
      "         [[ 8.0000e+00,  1.0000e+01],\n",
      "          [ 1.4000e+01,  1.6000e+01]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 300: After BatchNorm = tensor([[[[ 0.3230,  3.4852],\n",
      "          [ 9.8095, 12.9717]],\n",
      "\n",
      "         [[ 0.3230,  3.4852],\n",
      "          [ 9.8095, 12.9717]],\n",
      "\n",
      "         [[ 0.3230,  3.4852],\n",
      "          [ 9.8095, 12.9717]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 300: After ReLU = tensor([[[[ 0.3230,  3.4852],\n",
      "          [ 9.8095, 12.9717]],\n",
      "\n",
      "         [[ 0.3230,  3.4852],\n",
      "          [ 9.8095, 12.9717]],\n",
      "\n",
      "         [[ 0.3230,  3.4852],\n",
      "          [ 9.8095, 12.9717]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 300: Loss = 16.409400939941406\n",
      "Epoch 400: After Conv = tensor([[[[ 7.0000e+00,  9.0000e+00],\n",
      "          [ 1.3000e+01,  1.5000e+01]],\n",
      "\n",
      "         [[-3.9994e+00, -2.9992e+00],\n",
      "          [-9.9882e-01,  1.3795e-03]],\n",
      "\n",
      "         [[ 8.0000e+00,  1.0000e+01],\n",
      "          [ 1.4000e+01,  1.6000e+01]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 400: After BatchNorm = tensor([[[[ 1.7784,  5.0081],\n",
      "          [11.4674, 14.6971]],\n",
      "\n",
      "         [[ 1.7784,  5.0081],\n",
      "          [11.4674, 14.6971]],\n",
      "\n",
      "         [[ 1.7784,  5.0081],\n",
      "          [11.4674, 14.6971]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 400: After ReLU = tensor([[[[ 1.7784,  5.0081],\n",
      "          [11.4674, 14.6971]],\n",
      "\n",
      "         [[ 1.7784,  5.0081],\n",
      "          [11.4674, 14.6971]],\n",
      "\n",
      "         [[ 1.7784,  5.0081],\n",
      "          [11.4674, 14.6971]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 400: Loss = 8.729105949401855\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tạo input (1x3x3)\n",
    "x = torch.tensor(\n",
    "    [[[[1.0, 2, 3], [4, 5, 6], [7, 8, 9]]]], requires_grad=True\n",
    ")  # Shape: (1, 1, 3, 3)\n",
    "\n",
    "# Tạo kernel (2x2) và BatchNorm\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=2, stride=1, bias=False)\n",
    "batch_norm = nn.BatchNorm2d(3)  # Thêm BatchNorm\n",
    "relu = nn.ReLU(inplace=True)\n",
    "\n",
    "# Khởi tạo kernel với giá trị cụ thể (Shape: 3x1x2x2)\n",
    "with torch.no_grad():\n",
    "    conv.weight = nn.Parameter(\n",
    "        torch.tensor(\n",
    "            [\n",
    "                [[[1.0, -1], [2, 0]]],  # Kernel 1\n",
    "                [[[2.0, 1], [-2, 0]]],  # Kernel 2\n",
    "                [[[1.0, -1], [1, 1]]],  # Kernel 3\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Giả sử nhãn thật\n",
    "y_true = torch.tensor([[[[5.0, 10], [14, 20]]]])\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    list(conv.parameters()) + list(batch_norm.parameters()), lr=0.01\n",
    ")\n",
    "\n",
    "# Lưu lịch sử loss\n",
    "loss_history = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Xóa gradient trước đó\n",
    "\n",
    "    # Forward pass với BatchNorm\n",
    "    output = conv(x)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: After Conv = {output}\")\n",
    "\n",
    "    output_bn = batch_norm(output)  # Áp dụng BatchNorm\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: After BatchNorm = {output_bn}\")\n",
    "\n",
    "    output_rl = relu(output_bn)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: After ReLU = {output_rl}\")\n",
    "\n",
    "    # Tính loss (MSE)\n",
    "    loss = torch.mean((output_rl - y_true) ** 2) / 2\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Cập nhật trọng số\n",
    "    optimizer.step()\n",
    "\n",
    "    # In loss mỗi 100 epoch\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
    "\n",
    "# Vẽ đồ thị loss\n",
    "plt.plot(loss_history, label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss giảm dần qua từng epoch với CNN\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input 3x3x3(CxHxW) => Kernel 3x3x2x2 (NxCxHxW) => Conv 3x2x2 => BatchNorm 3x2x2 => ReLU 3x2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: After Conv = tensor([[[[ 8.5000, 10.5000],\n",
      "          [14.5000, 16.5000]],\n",
      "\n",
      "         [[ 1.5000,  2.0000],\n",
      "          [ 3.5000,  4.0000]],\n",
      "\n",
      "         [[ 6.5000,  8.5000],\n",
      "          [12.5000, 14.5000]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 0: After BatchNorm = tensor([[[[-1.2649, -0.6325],\n",
      "          [ 0.6325,  1.2649]],\n",
      "\n",
      "         [[-1.2127, -0.7276],\n",
      "          [ 0.7276,  1.2127]],\n",
      "\n",
      "         [[-1.2649, -0.6325],\n",
      "          [ 0.6325,  1.2649]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 0: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.6325, 1.2649]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.7276, 1.2127]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.6325, 1.2649]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 0: Loss = 81.81290435791016\n",
      "Epoch 5: After Conv = tensor([[[[ 8.5066, 10.5066],\n",
      "          [14.5110, 16.5110]],\n",
      "\n",
      "         [[ 1.5273,  2.0173],\n",
      "          [ 3.5024,  3.9924]],\n",
      "\n",
      "         [[ 6.5066,  8.5066],\n",
      "          [12.5110, 14.5110]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 5: After BatchNorm = tensor([[[[-1.3002, -0.5841],\n",
      "          [ 0.8498,  1.5660]],\n",
      "\n",
      "         [[-1.2414, -0.6950],\n",
      "          [ 0.9605,  1.5068]],\n",
      "\n",
      "         [[-1.3002, -0.5841],\n",
      "          [ 0.8498,  1.5660]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 5: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.8498, 1.5660]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.9605, 1.5068]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.8498, 1.5660]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 5: Loss = 79.6877670288086\n",
      "Epoch 10: After Conv = tensor([[[[ 8.5140, 10.5139],\n",
      "          [14.5231, 16.5231]],\n",
      "\n",
      "         [[ 1.5572,  2.0362],\n",
      "          [ 3.5046,  3.9836]],\n",
      "\n",
      "         [[ 6.5140,  8.5139],\n",
      "          [12.5231, 14.5231]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 10: After BatchNorm = tensor([[[[-1.3349, -0.5366],\n",
      "          [ 1.0638,  1.8622]],\n",
      "\n",
      "         [[-1.2691, -0.6640],\n",
      "          [ 1.1905,  1.7955]],\n",
      "\n",
      "         [[-1.3349, -0.5366],\n",
      "          [ 1.0638,  1.8622]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 10: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [1.0638, 1.8622]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.1905, 1.7955]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.0638, 1.8622]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 10: Loss = 77.63065338134766\n",
      "Epoch 15: After Conv = tensor([[[[ 8.5220, 10.5219],\n",
      "          [14.5363, 16.5362]],\n",
      "\n",
      "         [[ 1.5895,  2.0565],\n",
      "          [ 3.5065,  3.9736]],\n",
      "\n",
      "         [[ 6.5220,  8.5219],\n",
      "          [12.5363, 14.5362]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 15: After BatchNorm = tensor([[[[-1.3690, -0.4900],\n",
      "          [ 1.2745,  2.1536]],\n",
      "\n",
      "         [[-1.2957, -0.6346],\n",
      "          [ 1.4178,  2.0789]],\n",
      "\n",
      "         [[-1.3690, -0.4900],\n",
      "          [ 1.2745,  2.1536]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 15: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [1.2745, 2.1536]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.4178, 2.0789]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.2745, 2.1536]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 15: Loss = 75.6393814086914\n",
      "Epoch 20: After Conv = tensor([[[[ 8.5308, 10.5306],\n",
      "          [14.5504, 16.5502]],\n",
      "\n",
      "         [[ 1.6238,  2.0780],\n",
      "          [ 3.5080,  3.9622]],\n",
      "\n",
      "         [[ 6.5308,  8.5306],\n",
      "          [12.5504, 14.5502]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 20: After BatchNorm = tensor([[[[-1.4025, -0.4443],\n",
      "          [ 1.4819,  2.4402]],\n",
      "\n",
      "         [[-1.3213, -0.6068],\n",
      "          [ 1.6425,  2.3570]],\n",
      "\n",
      "         [[-1.4025, -0.4443],\n",
      "          [ 1.4819,  2.4402]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 20: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [1.4819, 2.4402]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.6425, 2.3570]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.4819, 2.4402]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 20: Loss = 73.71184539794922\n",
      "Epoch 25: After Conv = tensor([[[[ 8.5401, 10.5398],\n",
      "          [14.5654, 16.5651]],\n",
      "\n",
      "         [[ 1.6597,  2.1004],\n",
      "          [ 3.5090,  3.9496]],\n",
      "\n",
      "         [[ 6.5401,  8.5398],\n",
      "          [12.5654, 14.5651]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 25: After BatchNorm = tensor([[[[-1.4354, -0.3994],\n",
      "          [ 1.6861,  2.7221]],\n",
      "\n",
      "         [[-1.3457, -0.5807],\n",
      "          [ 1.8648,  2.6298]],\n",
      "\n",
      "         [[-1.4354, -0.3994],\n",
      "          [ 1.6861,  2.7221]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 25: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [1.6861, 2.7221]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.8648, 2.6298]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.6861, 2.7221]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 25: Loss = 71.84603118896484\n",
      "Epoch 30: After Conv = tensor([[[[ 8.5501, 10.5496],\n",
      "          [14.5813, 16.5807]],\n",
      "\n",
      "         [[ 1.6969,  2.1234],\n",
      "          [ 3.5094,  3.9358]],\n",
      "\n",
      "         [[ 6.5501,  8.5496],\n",
      "          [12.5813, 14.5807]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 30: After BatchNorm = tensor([[[[-1.4677, -0.3555],\n",
      "          [ 1.8872,  2.9994]],\n",
      "\n",
      "         [[-1.3691, -0.5565],\n",
      "          [ 2.0848,  2.8973]],\n",
      "\n",
      "         [[-1.4677, -0.3555],\n",
      "          [ 1.8872,  2.9994]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 30: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [1.8872, 2.9994]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.0848, 2.8973]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.8872, 2.9994]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 30: Loss = 70.03996276855469\n",
      "Epoch 35: After Conv = tensor([[[[ 8.5606, 10.5599],\n",
      "          [14.5978, 16.5971]],\n",
      "\n",
      "         [[ 1.7348,  2.1466],\n",
      "          [ 3.5092,  3.9210]],\n",
      "\n",
      "         [[ 6.5606,  8.5599],\n",
      "          [12.5978, 14.5971]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 35: After BatchNorm = tensor([[[[-1.4994, -0.3124],\n",
      "          [ 2.0851,  3.2721]],\n",
      "\n",
      "         [[-1.3912, -0.5339],\n",
      "          [ 2.3024,  3.1597]],\n",
      "\n",
      "         [[-1.4994, -0.3124],\n",
      "          [ 2.0851,  3.2721]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 35: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [2.0851, 3.2721]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.3024, 3.1597]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.0851, 3.2721]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 35: Loss = 68.29175567626953\n",
      "Epoch 40: After Conv = tensor([[[[ 8.5718, 10.5707],\n",
      "          [14.6151, 16.6140]],\n",
      "\n",
      "         [[ 1.7727,  2.1698],\n",
      "          [ 3.5083,  3.9053]],\n",
      "\n",
      "         [[ 6.5718,  8.5707],\n",
      "          [12.6151, 14.6140]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 40: After BatchNorm = tensor([[[[-1.5305, -0.2701],\n",
      "          [ 2.2800,  3.5404]],\n",
      "\n",
      "         [[-1.4122, -0.5132],\n",
      "          [ 2.5179,  3.4169]],\n",
      "\n",
      "         [[-1.5305, -0.2701],\n",
      "          [ 2.2800,  3.5404]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 40: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [2.2800, 3.5404]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.5179, 3.4169]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.2800, 3.5404]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 40: Loss = 66.59960174560547\n",
      "Epoch 45: After Conv = tensor([[[[ 8.5834, 10.5820],\n",
      "          [14.6329, 16.6315]],\n",
      "\n",
      "         [[ 1.8101,  2.1924],\n",
      "          [ 3.5068,  3.8891]],\n",
      "\n",
      "         [[ 6.5834,  8.5820],\n",
      "          [12.6329, 14.6315]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 45: After BatchNorm = tensor([[[[-1.5611, -0.2287],\n",
      "          [ 2.4719,  3.8043]],\n",
      "\n",
      "         [[-1.4321, -0.4940],\n",
      "          [ 2.7311,  3.6691]],\n",
      "\n",
      "         [[-1.5611, -0.2287],\n",
      "          [ 2.4719,  3.8043]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 45: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [2.4719, 3.8043]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.7311, 3.6691]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.4719, 3.8043]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 45: Loss = 64.96172332763672\n",
      "Epoch 50: After Conv = tensor([[[[ 8.5955, 10.5937],\n",
      "          [14.6513, 16.6495]],\n",
      "\n",
      "         [[ 1.8462,  2.2141],\n",
      "          [ 3.5048,  3.8727]],\n",
      "\n",
      "         [[ 6.5955,  8.5937],\n",
      "          [12.6513, 14.6495]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 50: After BatchNorm = tensor([[[[-1.5911, -0.1881],\n",
      "          [ 2.6609,  4.0639]],\n",
      "\n",
      "         [[-1.4509, -0.4764],\n",
      "          [ 2.9419,  3.9165]],\n",
      "\n",
      "         [[-1.5911, -0.1881],\n",
      "          [ 2.6609,  4.0639]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 50: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [2.6609, 4.0639]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.9419, 3.9165]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.6609, 4.0639]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 50: Loss = 63.376461029052734\n",
      "Epoch 55: After Conv = tensor([[[[ 8.6082, 10.6058],\n",
      "          [14.6702, 16.6678]],\n",
      "\n",
      "         [[ 1.8801,  2.2344],\n",
      "          [ 3.5023,  3.8567]],\n",
      "\n",
      "         [[ 6.6082,  8.6058],\n",
      "          [12.6702, 14.6678]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 55: After BatchNorm = tensor([[[[-1.6206, -0.1484],\n",
      "          [ 2.8469,  4.3192]],\n",
      "\n",
      "         [[-1.4689, -0.4600],\n",
      "          [ 3.1502,  4.1591]],\n",
      "\n",
      "         [[-1.6206, -0.1484],\n",
      "          [ 2.8469,  4.3192]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 55: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [2.8469, 4.3192]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [3.1502, 4.1591]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.8469, 4.3192]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 55: Loss = 61.84214401245117\n",
      "Epoch 60: After Conv = tensor([[[[ 8.6212, 10.6183],\n",
      "          [14.6895, 16.6866]],\n",
      "\n",
      "         [[ 1.9111,  2.2529],\n",
      "          [ 3.4996,  3.8414]],\n",
      "\n",
      "         [[ 6.6212,  8.6183],\n",
      "          [12.6895, 14.6866]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 60: After BatchNorm = tensor([[[[-1.6496, -0.1094],\n",
      "          [ 3.0301,  4.5703]],\n",
      "\n",
      "         [[-1.4863, -0.4445],\n",
      "          [ 3.3556,  4.3974]],\n",
      "\n",
      "         [[-1.6496, -0.1094],\n",
      "          [ 3.0301,  4.5703]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 60: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [3.0301, 4.5703]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [3.3556, 4.3974]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [3.0301, 4.5703]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 60: Loss = 60.3571662902832\n",
      "Epoch 65: After Conv = tensor([[[[ 8.6348, 10.6312],\n",
      "          [14.7092, 16.7056]],\n",
      "\n",
      "         [[ 1.9386,  2.2692],\n",
      "          [ 3.4968,  3.8274]],\n",
      "\n",
      "         [[ 6.6348,  8.6312],\n",
      "          [12.7092, 14.7056]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 65: After BatchNorm = tensor([[[[-1.6780, -0.0713],\n",
      "          [ 3.2106,  4.8172]],\n",
      "\n",
      "         [[-1.5032, -0.4295],\n",
      "          [ 3.5579,  4.6316]],\n",
      "\n",
      "         [[-1.6780, -0.0713],\n",
      "          [ 3.2106,  4.8172]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 65: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [3.2106, 4.8172]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [3.5579, 4.6316]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [3.2106, 4.8172]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 65: Loss = 58.91999816894531\n",
      "Epoch 70: After Conv = tensor([[[[ 8.6487, 10.6444],\n",
      "          [14.7292, 16.7249]],\n",
      "\n",
      "         [[ 1.9620,  2.2830],\n",
      "          [ 3.4942,  3.8152]],\n",
      "\n",
      "         [[ 6.6487,  8.6444],\n",
      "          [12.7292, 14.7249]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 70: After BatchNorm = tensor([[[[-1.7059, -0.0339],\n",
      "          [ 3.3882,  5.0602]],\n",
      "\n",
      "         [[-1.5201, -0.4147],\n",
      "          [ 3.7567,  4.8621]],\n",
      "\n",
      "         [[-1.7059, -0.0339],\n",
      "          [ 3.3882,  5.0602]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 70: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [3.3882, 5.0602]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [3.7567, 4.8621]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [3.3882, 5.0602]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 70: Loss = 57.52911376953125\n",
      "Epoch 75: After Conv = tensor([[[[ 8.6630, 10.6579],\n",
      "          [14.7495, 16.7444]],\n",
      "\n",
      "         [[ 1.9810,  2.2941],\n",
      "          [ 3.4920,  3.8051]],\n",
      "\n",
      "         [[ 6.6630,  8.6579],\n",
      "          [12.7495, 14.7444]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 75: After BatchNorm = tensor([[[[-1.7332e+00,  2.6789e-03],\n",
      "          [ 3.5632e+00,  5.2991e+00]],\n",
      "\n",
      "         [[-1.5370e+00, -3.9949e-01],\n",
      "          [ 3.9516e+00,  5.0892e+00]],\n",
      "\n",
      "         [[-1.7332e+00,  2.6793e-03],\n",
      "          [ 3.5632e+00,  5.2991e+00]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 75: After ReLU = tensor([[[[0.0000e+00, 2.6789e-03],\n",
      "          [3.5632e+00, 5.2991e+00]],\n",
      "\n",
      "         [[0.0000e+00, 0.0000e+00],\n",
      "          [3.9516e+00, 5.0892e+00]],\n",
      "\n",
      "         [[0.0000e+00, 2.6793e-03],\n",
      "          [3.5632e+00, 5.2991e+00]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 75: Loss = 56.178558349609375\n",
      "Epoch 80: After Conv = tensor([[[[ 8.6289, 10.6257],\n",
      "          [14.7010, 16.6978]],\n",
      "\n",
      "         [[ 1.9955,  2.3026],\n",
      "          [ 3.4901,  3.7973]],\n",
      "\n",
      "         [[ 6.6289,  8.6257],\n",
      "          [12.7010, 14.6978]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 80: After BatchNorm = tensor([[[[-1.6878,  0.1020],\n",
      "          [ 3.7548,  5.5446]],\n",
      "\n",
      "         [[-1.5543, -0.3837],\n",
      "          [ 4.1424,  5.3131]],\n",
      "\n",
      "         [[-1.6878,  0.1020],\n",
      "          [ 3.7548,  5.5446]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 80: After ReLU = tensor([[[[0.0000, 0.1020],\n",
      "          [3.7548, 5.5446]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [4.1424, 5.3131]],\n",
      "\n",
      "         [[0.0000, 0.1020],\n",
      "          [3.7548, 5.5446]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 80: Loss = 54.65257263183594\n",
      "Epoch 85: After Conv = tensor([[[[ 8.5949, 10.5932],\n",
      "          [14.6508, 16.6491]],\n",
      "\n",
      "         [[ 2.0057,  2.3086],\n",
      "          [ 3.4888,  3.7917]],\n",
      "\n",
      "         [[ 6.5949,  8.5932],\n",
      "          [12.6508, 14.6491]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 85: After BatchNorm = tensor([[[[-1.6428,  0.2004],\n",
      "          [ 3.9430,  5.7862]],\n",
      "\n",
      "         [[-1.5722, -0.3670],\n",
      "          [ 4.3289,  5.5341]],\n",
      "\n",
      "         [[-1.6428,  0.2004],\n",
      "          [ 3.9430,  5.7862]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 85: After ReLU = tensor([[[[0.0000, 0.2004],\n",
      "          [3.9430, 5.7862]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [4.3289, 5.5341]],\n",
      "\n",
      "         [[0.0000, 0.2004],\n",
      "          [3.9430, 5.7862]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 85: Loss = 53.17536926269531\n",
      "Epoch 90: After Conv = tensor([[[[ 8.5608, 10.5603],\n",
      "          [14.5989, 16.5983]],\n",
      "\n",
      "         [[ 2.0121,  2.3124],\n",
      "          [ 3.4880,  3.7883]],\n",
      "\n",
      "         [[ 6.5608,  8.5603],\n",
      "          [12.5989, 14.5983]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 90: After BatchNorm = tensor([[[[-1.5983,  0.2978],\n",
      "          [ 4.1278,  6.0239]],\n",
      "\n",
      "         [[-1.5906, -0.3492],\n",
      "          [ 4.5110,  5.7524]],\n",
      "\n",
      "         [[-1.5983,  0.2978],\n",
      "          [ 4.1278,  6.0239]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 90: After ReLU = tensor([[[[0.0000, 0.2978],\n",
      "          [4.1278, 6.0239]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [4.5110, 5.7524]],\n",
      "\n",
      "         [[0.0000, 0.2978],\n",
      "          [4.1278, 6.0239]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 90: Loss = 51.74531555175781\n",
      "Epoch 95: After Conv = tensor([[[[ 8.5269, 10.5270],\n",
      "          [14.5453, 16.5454]],\n",
      "\n",
      "         [[ 2.0150,  2.3141],\n",
      "          [ 3.4877,  3.7868]],\n",
      "\n",
      "         [[ 6.5269,  8.5270],\n",
      "          [12.5453, 14.5454]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 95: After BatchNorm = tensor([[[[-1.5543,  0.3943],\n",
      "          [ 4.3091,  6.2578]],\n",
      "\n",
      "         [[-1.6097, -0.3305],\n",
      "          [ 4.6888,  5.9679]],\n",
      "\n",
      "         [[-1.5543,  0.3943],\n",
      "          [ 4.3091,  6.2578]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 95: After ReLU = tensor([[[[0.0000, 0.3943],\n",
      "          [4.3091, 6.2578]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [4.6888, 5.9679]],\n",
      "\n",
      "         [[0.0000, 0.3943],\n",
      "          [4.3091, 6.2578]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 95: Loss = 50.36085891723633\n",
      "Epoch 100: After Conv = tensor([[[[ 8.4932, 10.4935],\n",
      "          [14.4899, 16.4902]],\n",
      "\n",
      "         [[ 2.0150,  2.3141],\n",
      "          [ 3.4878,  3.7869]],\n",
      "\n",
      "         [[ 6.4932,  8.4935],\n",
      "          [12.4899, 14.4902]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 100: After BatchNorm = tensor([[[[-1.5108,  0.4899],\n",
      "          [ 4.4872,  6.4879]],\n",
      "\n",
      "         [[-1.6293, -0.3109],\n",
      "          [ 4.8624,  6.1808]],\n",
      "\n",
      "         [[-1.5108,  0.4899],\n",
      "          [ 4.4872,  6.4879]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 100: After ReLU = tensor([[[[0.0000, 0.4899],\n",
      "          [4.4872, 6.4879]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [4.8624, 6.1808]],\n",
      "\n",
      "         [[0.0000, 0.4899],\n",
      "          [4.4872, 6.4879]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 100: Loss = 49.020477294921875\n",
      "Epoch 105: After Conv = tensor([[[[ 8.4597, 10.4597],\n",
      "          [14.4328, 16.4328]],\n",
      "\n",
      "         [[ 2.0127,  2.3128],\n",
      "          [ 3.4882,  3.7883]],\n",
      "\n",
      "         [[ 6.4597,  8.4597],\n",
      "          [12.4328, 14.4328]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 105: After BatchNorm = tensor([[[[-1.4678,  0.5846],\n",
      "          [ 4.6619,  6.7144]],\n",
      "\n",
      "         [[-1.6494, -0.2905],\n",
      "          [ 5.0320,  6.3909]],\n",
      "\n",
      "         [[-1.4678,  0.5846],\n",
      "          [ 4.6619,  6.7144]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 105: After ReLU = tensor([[[[0.0000, 0.5846],\n",
      "          [4.6619, 6.7144]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [5.0320, 6.3909]],\n",
      "\n",
      "         [[0.0000, 0.5846],\n",
      "          [4.6619, 6.7144]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 105: Loss = 47.72272872924805\n",
      "Epoch 110: After Conv = tensor([[[[ 8.4265, 10.4257],\n",
      "          [14.3740, 16.3731]],\n",
      "\n",
      "         [[ 2.0085,  2.3104],\n",
      "          [ 3.4889,  3.7908]],\n",
      "\n",
      "         [[ 6.4265,  8.4257],\n",
      "          [12.3740, 14.3731]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 110: After BatchNorm = tensor([[[[-1.4254,  0.6785],\n",
      "          [ 4.8334,  6.9373]],\n",
      "\n",
      "         [[-1.6698, -0.2695],\n",
      "          [ 5.1979,  6.5982]],\n",
      "\n",
      "         [[-1.4254,  0.6785],\n",
      "          [ 4.8334,  6.9373]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 110: After ReLU = tensor([[[[0.0000, 0.6785],\n",
      "          [4.8334, 6.9373]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [5.1979, 6.5982]],\n",
      "\n",
      "         [[0.0000, 0.6785],\n",
      "          [4.8334, 6.9373]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 110: Loss = 46.466190338134766\n",
      "Epoch 115: After Conv = tensor([[[[ 8.3938, 10.3915],\n",
      "          [14.3133, 16.3110]],\n",
      "\n",
      "         [[ 2.0028,  2.3071],\n",
      "          [ 3.4899,  3.7941]],\n",
      "\n",
      "         [[ 6.3938,  8.3916],\n",
      "          [12.3133, 14.3110]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 115: After BatchNorm = tensor([[[[-1.3834,  0.7715],\n",
      "          [ 5.0017,  7.1566]],\n",
      "\n",
      "         [[-1.6906, -0.2479],\n",
      "          [ 5.3601,  6.8027]],\n",
      "\n",
      "         [[-1.3834,  0.7715],\n",
      "          [ 5.0017,  7.1566]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 115: After ReLU = tensor([[[[0.0000, 0.7715],\n",
      "          [5.0017, 7.1566]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [5.3601, 6.8027]],\n",
      "\n",
      "         [[0.0000, 0.7715],\n",
      "          [5.0017, 7.1566]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 115: Loss = 45.24952697753906\n",
      "Epoch 120: After Conv = tensor([[[[ 8.3615, 10.3573],\n",
      "          [14.2507, 16.2464]],\n",
      "\n",
      "         [[ 1.9960,  2.3031],\n",
      "          [ 3.4909,  3.7981]],\n",
      "\n",
      "         [[ 6.3615,  8.3573],\n",
      "          [12.2507, 14.2464]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 120: After BatchNorm = tensor([[[[-1.3419,  0.8638],\n",
      "          [ 5.1668,  7.3725]],\n",
      "\n",
      "         [[-1.7115, -0.2259],\n",
      "          [ 5.5189,  7.0044]],\n",
      "\n",
      "         [[-1.3419,  0.8638],\n",
      "          [ 5.1668,  7.3725]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 120: After ReLU = tensor([[[[0.0000, 0.8638],\n",
      "          [5.1668, 7.3725]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [5.5189, 7.0044]],\n",
      "\n",
      "         [[0.0000, 0.8638],\n",
      "          [5.1668, 7.3725]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 120: Loss = 44.0713996887207\n",
      "Epoch 125: After Conv = tensor([[[[ 8.3298, 10.3229],\n",
      "          [14.1863, 16.1793]],\n",
      "\n",
      "         [[ 1.9882,  2.2986],\n",
      "          [ 3.4921,  3.8025]],\n",
      "\n",
      "         [[ 6.3298,  8.3229],\n",
      "          [12.1863, 14.1793]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 125: After BatchNorm = tensor([[[[-1.3010,  0.9552],\n",
      "          [ 5.3288,  7.5850]],\n",
      "\n",
      "         [[-1.7325, -0.2037],\n",
      "          [ 5.6745,  7.2033]],\n",
      "\n",
      "         [[-1.3010,  0.9552],\n",
      "          [ 5.3288,  7.5850]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 125: After ReLU = tensor([[[[0.0000, 0.9552],\n",
      "          [5.3288, 7.5850]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [5.6745, 7.2033]],\n",
      "\n",
      "         [[0.0000, 0.9552],\n",
      "          [5.3288, 7.5850]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 125: Loss = 42.930545806884766\n",
      "Epoch 130: After Conv = tensor([[[[ 8.2987, 10.2884],\n",
      "          [14.1199, 16.1096]],\n",
      "\n",
      "         [[ 1.9797,  2.2937],\n",
      "          [ 3.4933,  3.8073]],\n",
      "\n",
      "         [[ 6.2987,  8.2884],\n",
      "          [12.1199, 14.1096]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 130: After BatchNorm = tensor([[[[-1.2606,  1.0459],\n",
      "          [ 5.4876,  7.7941]],\n",
      "\n",
      "         [[-1.7536, -0.1813],\n",
      "          [ 5.8269,  7.3992]],\n",
      "\n",
      "         [[-1.2606,  1.0459],\n",
      "          [ 5.4876,  7.7941]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 130: After ReLU = tensor([[[[0.0000, 1.0459],\n",
      "          [5.4876, 7.7941]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [5.8269, 7.3992]],\n",
      "\n",
      "         [[0.0000, 1.0459],\n",
      "          [5.4876, 7.7941]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 130: Loss = 41.82573699951172\n",
      "Epoch 135: After Conv = tensor([[[[ 8.2683, 10.2540],\n",
      "          [14.0515, 16.0372]],\n",
      "\n",
      "         [[ 1.9707,  2.2884],\n",
      "          [ 3.4946,  3.8123]],\n",
      "\n",
      "         [[ 6.2683,  8.2540],\n",
      "          [12.0516, 14.0372]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 135: After BatchNorm = tensor([[[[-1.2207,  1.1360],\n",
      "          [ 5.6433,  8.0000]],\n",
      "\n",
      "         [[-1.7747, -0.1587],\n",
      "          [ 5.9763,  7.5923]],\n",
      "\n",
      "         [[-1.2207,  1.1360],\n",
      "          [ 5.6433,  8.0000]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 135: After ReLU = tensor([[[[0.0000, 1.1360],\n",
      "          [5.6433, 8.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [5.9763, 7.5923]],\n",
      "\n",
      "         [[0.0000, 1.1360],\n",
      "          [5.6433, 8.0000]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 135: Loss = 40.75578308105469\n",
      "Epoch 140: After Conv = tensor([[[[ 8.2388, 10.2195],\n",
      "          [13.9812, 15.9619]],\n",
      "\n",
      "         [[ 1.9612,  2.2829],\n",
      "          [ 3.4959,  3.8175]],\n",
      "\n",
      "         [[ 6.2388,  8.2195],\n",
      "          [11.9812, 13.9619]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 140: After BatchNorm = tensor([[[[-1.1814,  1.2253],\n",
      "          [ 5.7960,  8.2028]],\n",
      "\n",
      "         [[-1.7958, -0.1361],\n",
      "          [ 6.1228,  7.7825]],\n",
      "\n",
      "         [[-1.1814,  1.2253],\n",
      "          [ 5.7960,  8.2027]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 140: After ReLU = tensor([[[[0.0000, 1.2253],\n",
      "          [5.7960, 8.2028]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [6.1228, 7.7825]],\n",
      "\n",
      "         [[0.0000, 1.2253],\n",
      "          [5.7960, 8.2027]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 140: Loss = 39.719512939453125\n",
      "Epoch 145: After Conv = tensor([[[[ 8.2100, 10.1851],\n",
      "          [13.9087, 15.8837]],\n",
      "\n",
      "         [[ 1.9514,  2.2771],\n",
      "          [ 3.4971,  3.8229]],\n",
      "\n",
      "         [[ 6.2100,  8.1851],\n",
      "          [11.9087, 13.8837]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 145: After BatchNorm = tensor([[[[-1.1427,  1.3140],\n",
      "          [ 5.9456,  8.4023]],\n",
      "\n",
      "         [[-1.8168, -0.1135],\n",
      "          [ 6.2665,  7.9699]],\n",
      "\n",
      "         [[-1.1427,  1.3140],\n",
      "          [ 5.9456,  8.4023]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 145: After ReLU = tensor([[[[0.0000, 1.3140],\n",
      "          [5.9456, 8.4023]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [6.2665, 7.9699]],\n",
      "\n",
      "         [[0.0000, 1.3140],\n",
      "          [5.9456, 8.4023]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 145: Loss = 38.71582794189453\n",
      "Epoch 150: After Conv = tensor([[[[ 8.1823, 10.1508],\n",
      "          [13.8340, 15.8025]],\n",
      "\n",
      "         [[ 1.9413,  2.2712],\n",
      "          [ 3.4984,  3.8283]],\n",
      "\n",
      "         [[ 6.1823,  8.1508],\n",
      "          [11.8340, 13.8025]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 150: After BatchNorm = tensor([[[[-1.1045,  1.4021],\n",
      "          [ 6.0922,  8.5989]],\n",
      "\n",
      "         [[-1.8378, -0.0909],\n",
      "          [ 6.4075,  8.1544]],\n",
      "\n",
      "         [[-1.1045,  1.4021],\n",
      "          [ 6.0922,  8.5989]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 150: After ReLU = tensor([[[[0.0000, 1.4021],\n",
      "          [6.0922, 8.5989]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [6.4075, 8.1544]],\n",
      "\n",
      "         [[0.0000, 1.4021],\n",
      "          [6.0922, 8.5989]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 150: Loss = 37.74362564086914\n",
      "Epoch 155: After Conv = tensor([[[[ 8.1556, 10.1166],\n",
      "          [13.7571, 15.7181]],\n",
      "\n",
      "         [[ 1.9310,  2.2651],\n",
      "          [ 3.4997,  3.8338]],\n",
      "\n",
      "         [[ 6.1556,  8.1166],\n",
      "          [11.7571, 13.7181]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 155: After BatchNorm = tensor([[[[-1.0669,  1.4897],\n",
      "          [ 6.2358,  8.7924]],\n",
      "\n",
      "         [[-1.8586, -0.0683],\n",
      "          [ 6.5459,  8.3362]],\n",
      "\n",
      "         [[-1.0669,  1.4897],\n",
      "          [ 6.2358,  8.7924]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 155: After ReLU = tensor([[[[0.0000, 1.4897],\n",
      "          [6.2358, 8.7924]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [6.5459, 8.3362]],\n",
      "\n",
      "         [[0.0000, 1.4897],\n",
      "          [6.2358, 8.7924]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 155: Loss = 36.80186080932617\n",
      "Epoch 160: After Conv = tensor([[[[ 8.1301, 10.0826],\n",
      "          [13.6778, 15.6304]],\n",
      "\n",
      "         [[ 1.9204,  2.2589],\n",
      "          [ 3.5009,  3.8394]],\n",
      "\n",
      "         [[ 6.1301,  8.0826],\n",
      "          [11.6778, 13.6304]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 160: After BatchNorm = tensor([[[[-1.0299,  1.5767],\n",
      "          [ 6.3764,  8.9830]],\n",
      "\n",
      "         [[-1.8794, -0.0458],\n",
      "          [ 6.6816,  8.5152]],\n",
      "\n",
      "         [[-1.0299,  1.5767],\n",
      "          [ 6.3764,  8.9830]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 160: After ReLU = tensor([[[[0.0000, 1.5767],\n",
      "          [6.3764, 8.9830]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [6.6816, 8.5152]],\n",
      "\n",
      "         [[0.0000, 1.5767],\n",
      "          [6.3764, 8.9830]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 160: Loss = 35.889495849609375\n",
      "Epoch 165: After Conv = tensor([[[[ 8.1058, 10.0489],\n",
      "          [13.5962, 15.5392]],\n",
      "\n",
      "         [[ 1.9097,  2.2526],\n",
      "          [ 3.5020,  3.8449]],\n",
      "\n",
      "         [[ 6.1058,  8.0489],\n",
      "          [11.5962, 13.5392]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 165: After BatchNorm = tensor([[[[-0.9936,  1.6633],\n",
      "          [ 6.5139,  9.1708]],\n",
      "\n",
      "         [[-1.9000, -0.0233],\n",
      "          [ 6.8148,  8.6915]],\n",
      "\n",
      "         [[-0.9936,  1.6633],\n",
      "          [ 6.5139,  9.1708]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 165: After ReLU = tensor([[[[0.0000, 1.6633],\n",
      "          [6.5139, 9.1708]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [6.8148, 8.6915]],\n",
      "\n",
      "         [[0.0000, 1.6633],\n",
      "          [6.5139, 9.1708]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 165: Loss = 35.005550384521484\n",
      "Epoch 170: After Conv = tensor([[[[ 8.0830, 10.0154],\n",
      "          [13.5119, 15.4443]],\n",
      "\n",
      "         [[ 1.8988,  2.2461],\n",
      "          [ 3.5032,  3.8505]],\n",
      "\n",
      "         [[ 6.0830,  8.0154],\n",
      "          [11.5119, 13.4443]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 170: After BatchNorm = tensor([[[[-9.5785e-01,  1.7495e+00],\n",
      "          [ 6.6485e+00,  9.3559e+00]],\n",
      "\n",
      "         [[-1.9205e+00, -9.7370e-04],\n",
      "          [ 6.9456e+00,  8.8651e+00]],\n",
      "\n",
      "         [[-9.5785e-01,  1.7495e+00],\n",
      "          [ 6.6485e+00,  9.3559e+00]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 170: After ReLU = tensor([[[[0.0000, 1.7495],\n",
      "          [6.6485, 9.3559]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [6.9456, 8.8651]],\n",
      "\n",
      "         [[0.0000, 1.7495],\n",
      "          [6.6485, 9.3559]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 170: Loss = 34.149051666259766\n",
      "Epoch 175: After Conv = tensor([[[[ 8.0616,  9.9822],\n",
      "          [13.4251, 15.3456]],\n",
      "\n",
      "         [[ 1.3461,  1.9059],\n",
      "          [ 3.5028,  4.0626]],\n",
      "\n",
      "         [[ 6.0616,  7.9822],\n",
      "          [11.4251, 13.3456]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 175: After BatchNorm = tensor([[[[-0.9228,  1.8354],\n",
      "          [ 6.7800,  9.5382]],\n",
      "\n",
      "         [[-2.0136,  0.2922],\n",
      "          [ 6.8690,  9.1747]],\n",
      "\n",
      "         [[-0.9228,  1.8354],\n",
      "          [ 6.7800,  9.5382]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 175: After ReLU = tensor([[[[0.0000, 1.8354],\n",
      "          [6.7800, 9.5382]],\n",
      "\n",
      "         [[0.0000, 0.2922],\n",
      "          [6.8690, 9.1747]],\n",
      "\n",
      "         [[0.0000, 1.8354],\n",
      "          [6.7800, 9.5382]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 175: Loss = 33.073184967041016\n",
      "Epoch 180: After Conv = tensor([[[[ 8.0419,  9.9493],\n",
      "          [13.3354, 15.2428]],\n",
      "\n",
      "         [[ 0.9532,  1.6424],\n",
      "          [ 3.4083,  4.0974]],\n",
      "\n",
      "         [[ 6.0419,  7.9493],\n",
      "          [11.3354, 13.2428]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 180: After BatchNorm = tensor([[[[-0.8885,  1.9210],\n",
      "          [ 6.9084,  9.7179]],\n",
      "\n",
      "         [[-2.0212,  0.4846],\n",
      "          [ 6.9056,  9.4113]],\n",
      "\n",
      "         [[-0.8885,  1.9210],\n",
      "          [ 6.9084,  9.7179]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 180: After ReLU = tensor([[[[0.0000, 1.9210],\n",
      "          [6.9084, 9.7179]],\n",
      "\n",
      "         [[0.0000, 0.4846],\n",
      "          [6.9056, 9.4113]],\n",
      "\n",
      "         [[0.0000, 1.9210],\n",
      "          [6.9084, 9.7179]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 180: Loss = 32.106563568115234\n",
      "Epoch 185: After Conv = tensor([[[[ 8.0241,  9.9170],\n",
      "          [13.2427, 15.1356]],\n",
      "\n",
      "         [[ 0.6671,  1.4374],\n",
      "          [ 3.2831,  4.0534]],\n",
      "\n",
      "         [[ 6.0241,  7.9170],\n",
      "          [11.2427, 13.1356]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 185: After BatchNorm = tensor([[[[-0.8549,  2.0065],\n",
      "          [ 7.0338,  9.8951]],\n",
      "\n",
      "         [[-2.0077,  0.6380],\n",
      "          [ 6.9773,  9.6230]],\n",
      "\n",
      "         [[-0.8549,  2.0065],\n",
      "          [ 7.0338,  9.8951]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 185: After ReLU = tensor([[[[0.0000, 2.0065],\n",
      "          [7.0338, 9.8951]],\n",
      "\n",
      "         [[0.0000, 0.6380],\n",
      "          [6.9773, 9.6230]],\n",
      "\n",
      "         [[0.0000, 2.0065],\n",
      "          [7.0338, 9.8951]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 185: Loss = 31.19639015197754\n",
      "Epoch 190: After Conv = tensor([[[[ 8.0082,  9.8851],\n",
      "          [13.1469, 15.0238]],\n",
      "\n",
      "         [[ 0.4384,  1.2635],\n",
      "          [ 3.1395,  3.9645]],\n",
      "\n",
      "         [[ 6.0082,  7.8851],\n",
      "          [11.1469, 13.0238]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 190: After BatchNorm = tensor([[[[-0.8221,  2.0919],\n",
      "          [ 7.1560, 10.0699]],\n",
      "\n",
      "         [[-1.9872,  0.7766],\n",
      "          [ 7.0603,  9.8241]],\n",
      "\n",
      "         [[-0.8221,  2.0919],\n",
      "          [ 7.1560, 10.0699]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 190: After ReLU = tensor([[[[ 0.0000,  2.0919],\n",
      "          [ 7.1560, 10.0699]],\n",
      "\n",
      "         [[ 0.0000,  0.7766],\n",
      "          [ 7.0603,  9.8241]],\n",
      "\n",
      "         [[ 0.0000,  2.0919],\n",
      "          [ 7.1560, 10.0699]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 190: Loss = 30.32295036315918\n",
      "Epoch 195: After Conv = tensor([[[[ 7.9945,  9.8538],\n",
      "          [13.0477, 14.9070]],\n",
      "\n",
      "         [[ 0.2489,  1.1102],\n",
      "          [ 2.9804,  3.8417]],\n",
      "\n",
      "         [[ 5.9945,  7.8538],\n",
      "          [11.0477, 12.9070]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 195: After BatchNorm = tensor([[[[-0.7901,  2.1774],\n",
      "          [ 7.2749, 10.2424]],\n",
      "\n",
      "         [[-1.9643,  0.9085],\n",
      "          [ 7.1465, 10.0193]],\n",
      "\n",
      "         [[-0.7901,  2.1774],\n",
      "          [ 7.2749, 10.2424]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 195: After ReLU = tensor([[[[ 0.0000,  2.1774],\n",
      "          [ 7.2749, 10.2424]],\n",
      "\n",
      "         [[ 0.0000,  0.9085],\n",
      "          [ 7.1465, 10.0193]],\n",
      "\n",
      "         [[ 0.0000,  2.1774],\n",
      "          [ 7.2749, 10.2424]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 195: Loss = 29.479307174682617\n",
      "Epoch 200: After Conv = tensor([[[[ 7.9832,  9.8231],\n",
      "          [12.9449, 14.7848]],\n",
      "\n",
      "         [[ 0.0898,  0.9720],\n",
      "          [ 2.8060,  3.6882]],\n",
      "\n",
      "         [[ 5.9832,  7.8231],\n",
      "          [10.9449, 12.7848]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 200: After BatchNorm = tensor([[[[-0.7590,  2.2630],\n",
      "          [ 7.3906, 10.4126]],\n",
      "\n",
      "         [[-1.9414,  1.0381],\n",
      "          [ 7.2316, 10.2110]],\n",
      "\n",
      "         [[-0.7590,  2.2630],\n",
      "          [ 7.3906, 10.4126]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 200: After ReLU = tensor([[[[ 0.0000,  2.2630],\n",
      "          [ 7.3906, 10.4126]],\n",
      "\n",
      "         [[ 0.0000,  1.0381],\n",
      "          [ 7.2316, 10.2110]],\n",
      "\n",
      "         [[ 0.0000,  2.2630],\n",
      "          [ 7.3906, 10.4126]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 200: Loss = 28.661598205566406\n",
      "Epoch 205: After Conv = tensor([[[[ 7.9746,  9.7932],\n",
      "          [12.8383, 14.6569]],\n",
      "\n",
      "         [[-0.0428,  0.8462],\n",
      "          [ 2.6144,  3.5034]],\n",
      "\n",
      "         [[ 5.9746,  7.7932],\n",
      "          [10.8383, 12.6569]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 205: After BatchNorm = tensor([[[[-0.7289,  2.3490],\n",
      "          [ 7.5028, 10.5807]],\n",
      "\n",
      "         [[-1.9201,  1.1688],\n",
      "          [ 7.3122, 10.4011]],\n",
      "\n",
      "         [[-0.7289,  2.3491],\n",
      "          [ 7.5028, 10.5807]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 205: After ReLU = tensor([[[[ 0.0000,  2.3490],\n",
      "          [ 7.5028, 10.5807]],\n",
      "\n",
      "         [[ 0.0000,  1.1688],\n",
      "          [ 7.3122, 10.4011]],\n",
      "\n",
      "         [[ 0.0000,  2.3491],\n",
      "          [ 7.5028, 10.5807]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 205: Loss = 27.866865158081055\n",
      "Epoch 210: After Conv = tensor([[[[ 7.9690,  9.7642],\n",
      "          [12.7275, 14.5227]],\n",
      "\n",
      "         [[-0.1497,  0.7315],\n",
      "          [ 2.4022,  3.2834]],\n",
      "\n",
      "         [[ 5.9690,  7.7642],\n",
      "          [10.7275, 12.5227]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 210: After BatchNorm = tensor([[[[-0.6998,  2.4357],\n",
      "          [ 7.6114, 10.7468]],\n",
      "\n",
      "         [[-1.9023,  1.3044],\n",
      "          [ 7.3846, 10.5913]],\n",
      "\n",
      "         [[-0.6998,  2.4357],\n",
      "          [ 7.6114, 10.7468]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 210: After ReLU = tensor([[[[ 0.0000,  2.4357],\n",
      "          [ 7.6114, 10.7468]],\n",
      "\n",
      "         [[ 0.0000,  1.3044],\n",
      "          [ 7.3846, 10.5913]],\n",
      "\n",
      "         [[ 0.0000,  2.4357],\n",
      "          [ 7.6114, 10.7468]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 210: Loss = 27.09214973449707\n",
      "Epoch 215: After Conv = tensor([[[[ 7.9668,  9.7362],\n",
      "          [12.6121, 14.3815]],\n",
      "\n",
      "         [[-0.2288,  0.6277],\n",
      "          [ 2.1633,  3.0197]],\n",
      "\n",
      "         [[ 5.9668,  7.7362],\n",
      "          [10.6121, 12.3815]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 215: After BatchNorm = tensor([[[[-0.6719,  2.5231],\n",
      "          [ 7.7161, 10.9111]],\n",
      "\n",
      "         [[-1.8907,  1.4510],\n",
      "          [ 7.4427, 10.7843]],\n",
      "\n",
      "         [[-0.6719,  2.5231],\n",
      "          [ 7.7161, 10.9111]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 215: After ReLU = tensor([[[[ 0.0000,  2.5231],\n",
      "          [ 7.7161, 10.9111]],\n",
      "\n",
      "         [[ 0.0000,  1.4510],\n",
      "          [ 7.4427, 10.7843]],\n",
      "\n",
      "         [[ 0.0000,  2.5231],\n",
      "          [ 7.7161, 10.9111]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 215: Loss = 26.333765029907227\n",
      "Epoch 220: After Conv = tensor([[[[ 7.9683,  9.7094],\n",
      "          [12.4917, 14.2328]],\n",
      "\n",
      "         [[-0.2727,  0.5367],\n",
      "          [ 1.8865,  2.6959]],\n",
      "\n",
      "         [[ 5.9683,  7.7094],\n",
      "          [10.4917, 12.2328]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 220: After BatchNorm = tensor([[[[-0.6454,  2.6117],\n",
      "          [ 7.8167, 11.0738]],\n",
      "\n",
      "         [[-1.8904,  1.6205],\n",
      "          [ 7.4746, 10.9854]],\n",
      "\n",
      "         [[-0.6454,  2.6117],\n",
      "          [ 7.8167, 11.0738]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 220: After ReLU = tensor([[[[ 0.0000,  2.6117],\n",
      "          [ 7.8167, 11.0738]],\n",
      "\n",
      "         [[ 0.0000,  1.6205],\n",
      "          [ 7.4746, 10.9854]],\n",
      "\n",
      "         [[ 0.0000,  2.6117],\n",
      "          [ 7.8167, 11.0738]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 220: Loss = 25.585617065429688\n",
      "Epoch 225: After Conv = tensor([[[[ 7.9742,  9.6839],\n",
      "          [12.3658, 14.0755]],\n",
      "\n",
      "         [[-0.2629,  0.4649],\n",
      "          [ 1.5490,  2.2768]],\n",
      "\n",
      "         [[ 5.9742,  7.6839],\n",
      "          [10.3658, 12.0755]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 225: After BatchNorm = tensor([[[[-0.6202,  2.7019],\n",
      "          [ 7.9129, 11.2350]],\n",
      "\n",
      "         [[-1.9148,  1.8457],\n",
      "          [ 7.4472, 11.2078]],\n",
      "\n",
      "         [[-0.6202,  2.7019],\n",
      "          [ 7.9129, 11.2350]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 225: After ReLU = tensor([[[[ 0.0000,  2.7019],\n",
      "          [ 7.9129, 11.2350]],\n",
      "\n",
      "         [[ 0.0000,  1.8457],\n",
      "          [ 7.4472, 11.2078]],\n",
      "\n",
      "         [[ 0.0000,  2.7019],\n",
      "          [ 7.9129, 11.2350]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 225: Loss = 24.8339786529541\n",
      "Epoch 230: After Conv = tensor([[[[ 7.9849,  9.6600],\n",
      "          [12.2337, 13.9088]],\n",
      "\n",
      "         [[-0.1388,  0.4356],\n",
      "          [ 1.0911,  1.6655]],\n",
      "\n",
      "         [[ 5.9849,  7.6600],\n",
      "          [10.2337, 11.9088]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 230: After BatchNorm = tensor([[[[-0.5968,  2.7941],\n",
      "          [ 8.0041, 11.3950]],\n",
      "\n",
      "         [[-2.0205,  2.2863],\n",
      "          [ 7.2010, 11.5077]],\n",
      "\n",
      "         [[-0.5968,  2.7941],\n",
      "          [ 8.0041, 11.3950]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 230: After ReLU = tensor([[[[ 0.0000,  2.7941],\n",
      "          [ 8.0041, 11.3950]],\n",
      "\n",
      "         [[ 0.0000,  2.2863],\n",
      "          [ 7.2010, 11.5077]],\n",
      "\n",
      "         [[ 0.0000,  2.7941],\n",
      "          [ 8.0041, 11.3950]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 230: Loss = 24.028730392456055\n",
      "Epoch 235: After Conv = tensor([[[[ 8.0014,  9.6381],\n",
      "          [12.0947, 13.7314]],\n",
      "\n",
      "         [[ 0.0570,  0.4705],\n",
      "          [ 0.7202,  1.1338]],\n",
      "\n",
      "         [[ 6.0014,  7.6381],\n",
      "          [10.0947, 11.7314]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 235: After BatchNorm = tensor([[[[-0.5754,  2.8892],\n",
      "          [ 8.0895, 11.5542]],\n",
      "\n",
      "         [[-2.2367,  3.1973],\n",
      "          [ 6.4791, 11.9131]],\n",
      "\n",
      "         [[-0.5754,  2.8892],\n",
      "          [ 8.0895, 11.5542]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 235: After ReLU = tensor([[[[ 0.0000,  2.8892],\n",
      "          [ 8.0895, 11.5542]],\n",
      "\n",
      "         [[ 0.0000,  3.1973],\n",
      "          [ 6.4791, 11.9131]],\n",
      "\n",
      "         [[ 0.0000,  2.8892],\n",
      "          [ 8.0895, 11.5542]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 235: Loss = 23.203889846801758\n",
      "Epoch 240: After Conv = tensor([[[[ 8.0247,  9.6185],\n",
      "          [11.9478, 13.5417]],\n",
      "\n",
      "         [[ 0.1205,  0.4852],\n",
      "          [ 0.6145,  0.9792]],\n",
      "\n",
      "         [[ 6.0247,  7.6185],\n",
      "          [ 9.9478, 11.5417]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 240: After BatchNorm = tensor([[[[-0.5564,  2.9882],\n",
      "          [ 8.1684, 11.7130]],\n",
      "\n",
      "         [[-2.3169,  3.8392],\n",
      "          [ 6.0225, 12.1785]],\n",
      "\n",
      "         [[-0.5564,  2.9882],\n",
      "          [ 8.1684, 11.7130]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 240: After ReLU = tensor([[[[ 0.0000,  2.9882],\n",
      "          [ 8.1684, 11.7130]],\n",
      "\n",
      "         [[ 0.0000,  3.8392],\n",
      "          [ 6.0225, 12.1785]],\n",
      "\n",
      "         [[ 0.0000,  2.9882],\n",
      "          [ 8.1684, 11.7130]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 240: Loss = 22.561187744140625\n",
      "Epoch 245: After Conv = tensor([[[[ 8.0559,  9.6018],\n",
      "          [11.7919, 13.3377]],\n",
      "\n",
      "         [[ 0.0341,  0.4500],\n",
      "          [ 0.6924,  1.1083]],\n",
      "\n",
      "         [[ 6.0559,  7.6018],\n",
      "          [ 9.7919, 11.3377]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 245: After BatchNorm = tensor([[[[-0.5404,  3.0924],\n",
      "          [ 8.2391, 11.8719]],\n",
      "\n",
      "         [[-2.1921,  3.3940],\n",
      "          [ 6.6499, 12.2359]],\n",
      "\n",
      "         [[-0.5404,  3.0924],\n",
      "          [ 8.2391, 11.8719]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 245: After ReLU = tensor([[[[ 0.0000,  3.0924],\n",
      "          [ 8.2391, 11.8719]],\n",
      "\n",
      "         [[ 0.0000,  3.3940],\n",
      "          [ 6.6499, 12.2359]],\n",
      "\n",
      "         [[ 0.0000,  3.0924],\n",
      "          [ 8.2391, 11.8719]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 245: Loss = 21.953445434570312\n",
      "Epoch 250: After Conv = tensor([[[[ 8.0970,  9.5886],\n",
      "          [11.6254, 13.1170]],\n",
      "\n",
      "         [[ 0.0923,  0.4638],\n",
      "          [ 0.5973,  0.9689]],\n",
      "\n",
      "         [[ 6.0970,  7.5886],\n",
      "          [ 9.6254, 11.1170]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 250: After BatchNorm = tensor([[[[-0.5280,  3.2039],\n",
      "          [ 8.2998, 12.0317]],\n",
      "\n",
      "         [[-2.2629,  3.9888],\n",
      "          [ 6.2346, 12.4863]],\n",
      "\n",
      "         [[-0.5280,  3.2039],\n",
      "          [ 8.2998, 12.0317]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 250: After ReLU = tensor([[[[ 0.0000,  3.2039],\n",
      "          [ 8.2998, 12.0317]],\n",
      "\n",
      "         [[ 0.0000,  3.9888],\n",
      "          [ 6.2346, 12.4863]],\n",
      "\n",
      "         [[ 0.0000,  3.2039],\n",
      "          [ 8.2998, 12.0317]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 250: Loss = 21.343164443969727\n",
      "Epoch 255: After Conv = tensor([[[[8.1500e+00, 9.5800e+00],\n",
      "          [1.1447e+01, 1.2876e+01]],\n",
      "\n",
      "         [[1.1572e-02, 4.3130e-01],\n",
      "          [6.7145e-01, 1.0912e+00]],\n",
      "\n",
      "         [[6.1500e+00, 7.5800e+00],\n",
      "          [9.4465e+00, 1.0876e+01]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 255: After BatchNorm = tensor([[[[-0.5205,  3.3261],\n",
      "          [ 8.3471, 12.1937]],\n",
      "\n",
      "         [[-2.1434,  3.5665],\n",
      "          [ 6.8335, 12.5435]],\n",
      "\n",
      "         [[-0.5205,  3.3261],\n",
      "          [ 8.3471, 12.1937]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 255: After ReLU = tensor([[[[ 0.0000,  3.3261],\n",
      "          [ 8.3471, 12.1937]],\n",
      "\n",
      "         [[ 0.0000,  3.5665],\n",
      "          [ 6.8335, 12.5435]],\n",
      "\n",
      "         [[ 0.0000,  3.3261],\n",
      "          [ 8.3471, 12.1937]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 255: Loss = 20.759075164794922\n",
      "Epoch 260: After Conv = tensor([[[[ 8.2183,  9.5773],\n",
      "          [11.2529, 12.6120]],\n",
      "\n",
      "         [[ 0.0673,  0.4451],\n",
      "          [ 0.5827,  0.9605]],\n",
      "\n",
      "         [[ 6.2182,  7.5773],\n",
      "          [ 9.2529, 10.6120]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 260: After BatchNorm = tensor([[[[-0.5197,  3.4640],\n",
      "          [ 8.3758, 12.3595]],\n",
      "\n",
      "         [[-2.2092,  4.1322],\n",
      "          [ 6.4419, 12.7833]],\n",
      "\n",
      "         [[-0.5197,  3.4640],\n",
      "          [ 8.3758, 12.3595]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 260: After ReLU = tensor([[[[ 0.0000,  3.4640],\n",
      "          [ 8.3758, 12.3595]],\n",
      "\n",
      "         [[ 0.0000,  4.1322],\n",
      "          [ 6.4419, 12.7833]],\n",
      "\n",
      "         [[ 0.0000,  3.4640],\n",
      "          [ 8.3758, 12.3595]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 260: Loss = 20.170490264892578\n",
      "Epoch 265: After Conv = tensor([[[[ 8.3060e+00,  9.5826e+00],\n",
      "          [ 1.1042e+01,  1.2319e+01]],\n",
      "\n",
      "         [[-7.9904e-03,  4.1501e-01],\n",
      "          [ 6.5304e-01,  1.0760e+00]],\n",
      "\n",
      "         [[ 6.3060e+00,  7.5826e+00],\n",
      "          [ 9.0419e+00,  1.0319e+01]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 265: After BatchNorm = tensor([[[[-0.5283,  3.6269],\n",
      "          [ 8.3768, 12.5320]],\n",
      "\n",
      "         [[-2.0949,  3.7330],\n",
      "          [ 7.0124, 12.8403]],\n",
      "\n",
      "         [[-0.5283,  3.6269],\n",
      "          [ 8.3768, 12.5320]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 265: After ReLU = tensor([[[[ 0.0000,  3.6269],\n",
      "          [ 8.3768, 12.5320]],\n",
      "\n",
      "         [[ 0.0000,  3.7330],\n",
      "          [ 7.0124, 12.8403]],\n",
      "\n",
      "         [[ 0.0000,  3.6269],\n",
      "          [ 8.3768, 12.5320]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 265: Loss = 19.599105834960938\n",
      "Epoch 270: After Conv = tensor([[[[ 8.4188,  9.5988],\n",
      "          [10.8113, 11.9913]],\n",
      "\n",
      "         [[ 0.0452,  0.4286],\n",
      "          [ 0.5701,  0.9535]],\n",
      "\n",
      "         [[ 6.4188,  7.5988],\n",
      "          [ 8.8113,  9.9912]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 270: After BatchNorm = tensor([[[[-0.5508,  3.8311],\n",
      "          [ 8.3337, 12.7156]],\n",
      "\n",
      "         [[-2.1559,  4.2711],\n",
      "          [ 6.6431, 13.0701]],\n",
      "\n",
      "         [[-0.5508,  3.8311],\n",
      "          [ 8.3337, 12.7156]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 270: After ReLU = tensor([[[[ 0.0000,  3.8311],\n",
      "          [ 8.3337, 12.7156]],\n",
      "\n",
      "         [[ 0.0000,  4.2711],\n",
      "          [ 6.6431, 13.0701]],\n",
      "\n",
      "         [[ 0.0000,  3.8311],\n",
      "          [ 8.3337, 12.7156]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 270: Loss = 19.017438888549805\n",
      "Epoch 275: After Conv = tensor([[[[ 8.5612,  9.6292],\n",
      "          [10.5634, 11.6314]],\n",
      "\n",
      "         [[-0.0251,  0.4007],\n",
      "          [ 0.6369,  1.0627]],\n",
      "\n",
      "         [[ 6.5612,  7.6292],\n",
      "          [ 8.5634,  9.6314]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 275: After BatchNorm = tensor([[[[-0.5937,  4.1061],\n",
      "          [ 8.2169, 12.9167]],\n",
      "\n",
      "         [[-2.0465,  3.8934],\n",
      "          [ 7.1869, 13.1268]],\n",
      "\n",
      "         [[-0.5937,  4.1061],\n",
      "          [ 8.2169, 12.9167]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 275: After ReLU = tensor([[[[ 0.0000,  4.1061],\n",
      "          [ 8.2169, 12.9167]],\n",
      "\n",
      "         [[ 0.0000,  3.8934],\n",
      "          [ 7.1869, 13.1268]],\n",
      "\n",
      "         [[ 0.0000,  4.1061],\n",
      "          [ 8.2169, 12.9167]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 275: Loss = 18.44437599182129\n",
      "Epoch 280: After Conv = tensor([[[[ 8.7225,  9.6730],\n",
      "          [10.3232, 11.2738]],\n",
      "\n",
      "         [[ 0.0256,  0.4140],\n",
      "          [ 0.5592,  0.9476]],\n",
      "\n",
      "         [[ 6.7225,  7.6730],\n",
      "          [ 8.3232,  9.2738]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 280: After BatchNorm = tensor([[[[-0.6578,  4.4814],\n",
      "          [ 7.9966, 13.1358]],\n",
      "\n",
      "         [[-2.1031,  4.4056],\n",
      "          [ 6.8383, 13.3470]],\n",
      "\n",
      "         [[-0.6578,  4.4814],\n",
      "          [ 7.9966, 13.1358]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 280: After ReLU = tensor([[[[ 0.0000,  4.4814],\n",
      "          [ 7.9966, 13.1358]],\n",
      "\n",
      "         [[ 0.0000,  4.4056],\n",
      "          [ 6.8383, 13.3470]],\n",
      "\n",
      "         [[ 0.0000,  4.4814],\n",
      "          [ 7.9966, 13.1358]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 280: Loss = 17.87810707092285\n",
      "Epoch 285: After Conv = tensor([[[[ 8.8394,  9.7095],\n",
      "          [10.1696, 11.0398]],\n",
      "\n",
      "         [[-0.0402,  0.3882],\n",
      "          [ 0.6226,  1.0509]],\n",
      "\n",
      "         [[ 6.8394,  7.7095],\n",
      "          [ 8.1696,  9.0398]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 285: After BatchNorm = tensor([[[[-0.7068,  4.8468],\n",
      "          [ 7.7832, 13.3368]],\n",
      "\n",
      "         [[-1.9983,  4.0482],\n",
      "          [ 7.3567, 13.4032]],\n",
      "\n",
      "         [[-0.7068,  4.8468],\n",
      "          [ 7.7832, 13.3368]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 285: After ReLU = tensor([[[[ 0.0000,  4.8468],\n",
      "          [ 7.7832, 13.3368]],\n",
      "\n",
      "         [[ 0.0000,  4.0482],\n",
      "          [ 7.3567, 13.4032]],\n",
      "\n",
      "         [[ 0.0000,  4.8468],\n",
      "          [ 7.7832, 13.3368]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 285: Loss = 17.386659622192383\n",
      "Epoch 290: After Conv = tensor([[[[8.8706e+00, 9.7199e+00],\n",
      "          [1.0131e+01, 1.0980e+01]],\n",
      "\n",
      "         [[8.1867e-03, 4.0115e-01],\n",
      "          [5.4981e-01, 9.4278e-01]],\n",
      "\n",
      "         [[6.8707e+00, 7.7199e+00],\n",
      "          [8.1311e+00, 8.9803e+00]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 290: After BatchNorm = tensor([[[[-0.7063,  5.0065],\n",
      "          [ 7.7727, 13.4856]],\n",
      "\n",
      "         [[-2.0508,  4.5359],\n",
      "          [ 7.0277, 13.6144]],\n",
      "\n",
      "         [[-0.7063,  5.0065],\n",
      "          [ 7.7727, 13.4856]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 290: After ReLU = tensor([[[[ 0.0000,  5.0065],\n",
      "          [ 7.7727, 13.4856]],\n",
      "\n",
      "         [[ 0.0000,  4.5359],\n",
      "          [ 7.0277, 13.6144]],\n",
      "\n",
      "         [[ 0.0000,  5.0065],\n",
      "          [ 7.7727, 13.4856]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 290: Loss = 16.939496994018555\n",
      "Epoch 295: After Conv = tensor([[[[ 8.8738,  9.7209],\n",
      "          [10.1272, 10.9744]],\n",
      "\n",
      "         [[-0.0534,  0.3771],\n",
      "          [ 0.6099,  1.0404]],\n",
      "\n",
      "         [[ 6.8738,  7.7209],\n",
      "          [ 8.1272,  8.9744]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 295: After BatchNorm = tensor([[[[-0.6887,  5.0797],\n",
      "          [ 7.8464, 13.6148]],\n",
      "\n",
      "         [[-1.9502,  4.1977],\n",
      "          [ 7.5221, 13.6700]],\n",
      "\n",
      "         [[-0.6887,  5.0797],\n",
      "          [ 7.8464, 13.6148]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 295: After ReLU = tensor([[[[ 0.0000,  5.0797],\n",
      "          [ 7.8464, 13.6148]],\n",
      "\n",
      "         [[ 0.0000,  4.1977],\n",
      "          [ 7.5221, 13.6700]],\n",
      "\n",
      "         [[ 0.0000,  5.0797],\n",
      "          [ 7.8464, 13.6148]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 295: Loss = 16.51638412475586\n",
      "Epoch 300: After Conv = tensor([[[[ 8.8737e+00,  9.7209e+00],\n",
      "          [ 1.0127e+01,  1.0974e+01]],\n",
      "\n",
      "         [[-7.3675e-03,  3.8970e-01],\n",
      "          [ 5.4169e-01,  9.3876e-01]],\n",
      "\n",
      "         [[ 6.8737e+00,  7.7209e+00],\n",
      "          [ 8.1273e+00,  8.9745e+00]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 300: After BatchNorm = tensor([[[[-0.6691,  5.1416],\n",
      "          [ 7.9291, 13.7398]],\n",
      "\n",
      "         [[-1.9989,  4.6620],\n",
      "          [ 7.2116, 13.8726]],\n",
      "\n",
      "         [[-0.6691,  5.1416],\n",
      "          [ 7.9292, 13.7398]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 300: After ReLU = tensor([[[[ 0.0000,  5.1416],\n",
      "          [ 7.9291, 13.7398]],\n",
      "\n",
      "         [[ 0.0000,  4.6620],\n",
      "          [ 7.2116, 13.8726]],\n",
      "\n",
      "         [[ 0.0000,  5.1416],\n",
      "          [ 7.9292, 13.7398]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 300: Loss = 16.1008243560791\n",
      "Epoch 305: After Conv = tensor([[[[ 8.8733,  9.7207],\n",
      "          [10.1277, 10.9752]],\n",
      "\n",
      "         [[-0.0651,  0.3673],\n",
      "          [ 0.5987,  1.0311]],\n",
      "\n",
      "         [[ 6.8733,  7.7207],\n",
      "          [ 8.1277,  8.9752]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 305: After BatchNorm = tensor([[[[-0.6495,  5.2015],\n",
      "          [ 8.0116, 13.8625]],\n",
      "\n",
      "         [[-1.9024,  4.3422],\n",
      "          [ 7.6829, 13.9275]],\n",
      "\n",
      "         [[-0.6495,  5.2015],\n",
      "          [ 8.0116, 13.8625]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 305: After ReLU = tensor([[[[ 0.0000,  5.2015],\n",
      "          [ 8.0116, 13.8625]],\n",
      "\n",
      "         [[ 0.0000,  4.3422],\n",
      "          [ 7.6829, 13.9275]],\n",
      "\n",
      "         [[ 0.0000,  5.2015],\n",
      "          [ 8.0116, 13.8625]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 305: Loss = 15.704297065734863\n",
      "Epoch 310: After Conv = tensor([[[[ 8.8729,  9.7206],\n",
      "          [10.1282, 10.9759]],\n",
      "\n",
      "         [[-0.0213,  0.3795],\n",
      "          [ 0.5347,  0.9355]],\n",
      "\n",
      "         [[ 6.8729,  7.7206],\n",
      "          [ 8.1282,  8.9759]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 310: After BatchNorm = tensor([[[[-0.6299,  5.2605],\n",
      "          [ 8.0928, 13.9831]],\n",
      "\n",
      "         [[-1.9475,  4.7842],\n",
      "          [ 7.3901, 14.1218]],\n",
      "\n",
      "         [[-0.6299,  5.2605],\n",
      "          [ 8.0927, 13.9831]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 310: After ReLU = tensor([[[[ 0.0000,  5.2605],\n",
      "          [ 8.0928, 13.9831]],\n",
      "\n",
      "         [[ 0.0000,  4.7842],\n",
      "          [ 7.3901, 14.1218]],\n",
      "\n",
      "         [[ 0.0000,  5.2605],\n",
      "          [ 8.0927, 13.9831]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 310: Loss = 15.315449714660645\n",
      "Epoch 315: After Conv = tensor([[[[ 8.8724,  9.7204],\n",
      "          [10.1286, 10.9766]],\n",
      "\n",
      "         [[-0.0753,  0.3587],\n",
      "          [ 0.5887,  1.0227]],\n",
      "\n",
      "         [[ 6.8725,  7.7204],\n",
      "          [ 8.1286,  8.9766]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 315: After BatchNorm = tensor([[[[-0.6104,  5.3186],\n",
      "          [ 8.1727, 14.1016]],\n",
      "\n",
      "         [[-1.8548,  4.4820],\n",
      "          [ 7.8392, 14.1760]],\n",
      "\n",
      "         [[-0.6104,  5.3186],\n",
      "          [ 8.1727, 14.1016]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 315: After ReLU = tensor([[[[ 0.0000,  5.3186],\n",
      "          [ 8.1727, 14.1016]],\n",
      "\n",
      "         [[ 0.0000,  4.4820],\n",
      "          [ 7.8392, 14.1760]],\n",
      "\n",
      "         [[ 0.0000,  5.3186],\n",
      "          [ 8.1727, 14.1016]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 315: Loss = 14.943839073181152\n",
      "Epoch 320: After Conv = tensor([[[[ 8.8720,  9.7202],\n",
      "          [10.1291, 10.9773]],\n",
      "\n",
      "         [[-0.0337,  0.3705],\n",
      "          [ 0.5286,  0.9328]],\n",
      "\n",
      "         [[ 6.8721,  7.7203],\n",
      "          [ 8.1291,  8.9773]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 320: After BatchNorm = tensor([[[[-0.5909,  5.3757],\n",
      "          [ 8.2514, 14.2181]],\n",
      "\n",
      "         [[-1.8965,  4.9026],\n",
      "          [ 7.5633, 14.3625]],\n",
      "\n",
      "         [[-0.5909,  5.3757],\n",
      "          [ 8.2514, 14.2181]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 320: After ReLU = tensor([[[[ 0.0000,  5.3757],\n",
      "          [ 8.2514, 14.2181]],\n",
      "\n",
      "         [[ 0.0000,  4.9026],\n",
      "          [ 7.5633, 14.3625]],\n",
      "\n",
      "         [[ 0.0000,  5.3757],\n",
      "          [ 8.2514, 14.2181]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 320: Loss = 14.57992935180664\n",
      "Epoch 325: After Conv = tensor([[[[ 8.8716,  9.7201],\n",
      "          [10.1295, 10.9780]],\n",
      "\n",
      "         [[-0.0844,  0.3511],\n",
      "          [ 0.5798,  1.0152]],\n",
      "\n",
      "         [[ 6.8716,  7.7201],\n",
      "          [ 8.1296,  8.9781]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 325: After BatchNorm = tensor([[[[-0.5716,  5.4320],\n",
      "          [ 8.3289, 14.3325]],\n",
      "\n",
      "         [[-1.8075,  4.6173],\n",
      "          [ 7.9911, 14.4159]],\n",
      "\n",
      "         [[-0.5716,  5.4320],\n",
      "          [ 8.3289, 14.3325]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 325: After ReLU = tensor([[[[ 0.0000,  5.4320],\n",
      "          [ 8.3289, 14.3325]],\n",
      "\n",
      "         [[ 0.0000,  4.6173],\n",
      "          [ 7.9911, 14.4159]],\n",
      "\n",
      "         [[ 0.0000,  5.4320],\n",
      "          [ 8.3289, 14.3325]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 325: Loss = 14.231644630432129\n",
      "Epoch 330: After Conv = tensor([[[[ 8.8712,  9.7200],\n",
      "          [10.1300, 10.9788]],\n",
      "\n",
      "         [[-0.0449,  0.3624],\n",
      "          [ 0.5235,  0.9308]],\n",
      "\n",
      "         [[ 6.8712,  7.7200],\n",
      "          [ 8.1300,  8.9788]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 330: After BatchNorm = tensor([[[[-0.5523,  5.4875],\n",
      "          [ 8.4052, 14.4450]],\n",
      "\n",
      "         [[-1.8460,  5.0173],\n",
      "          [ 7.7315, 14.5947]],\n",
      "\n",
      "         [[-0.5523,  5.4875],\n",
      "          [ 8.4052, 14.4450]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 330: After ReLU = tensor([[[[ 0.0000,  5.4875],\n",
      "          [ 8.4052, 14.4450]],\n",
      "\n",
      "         [[ 0.0000,  5.0173],\n",
      "          [ 7.7315, 14.5947]],\n",
      "\n",
      "         [[ 0.0000,  5.4875],\n",
      "          [ 8.4052, 14.4450]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 330: Loss = 13.891036987304688\n",
      "Epoch 335: After Conv = tensor([[[[ 8.8708,  9.7198],\n",
      "          [10.1304, 10.9795]],\n",
      "\n",
      "         [[-0.0923,  0.3444],\n",
      "          [ 0.5718,  1.0085]],\n",
      "\n",
      "         [[ 6.8708,  7.7198],\n",
      "          [ 8.1305,  8.9795]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 335: After BatchNorm = tensor([[[[-0.5331,  5.5420],\n",
      "          [ 8.4803, 14.5555]],\n",
      "\n",
      "         [[-1.7605,  4.7484],\n",
      "          [ 8.1386, 14.6475]],\n",
      "\n",
      "         [[-0.5331,  5.5421],\n",
      "          [ 8.4803, 14.5555]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 335: After ReLU = tensor([[[[ 0.0000,  5.5420],\n",
      "          [ 8.4803, 14.5555]],\n",
      "\n",
      "         [[ 0.0000,  4.7484],\n",
      "          [ 8.1386, 14.6475]],\n",
      "\n",
      "         [[ 0.0000,  5.5421],\n",
      "          [ 8.4803, 14.5555]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 335: Loss = 13.564643859863281\n",
      "Epoch 340: After Conv = tensor([[[[ 8.8704,  9.7197],\n",
      "          [10.1309, 10.9802]],\n",
      "\n",
      "         [[-0.0549,  0.3552],\n",
      "          [ 0.5191,  0.9292]],\n",
      "\n",
      "         [[ 6.8704,  7.7197],\n",
      "          [ 8.1309,  8.9802]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 340: After BatchNorm = tensor([[[[-0.5140,  5.5958],\n",
      "          [ 8.5543, 14.6641]],\n",
      "\n",
      "         [[-1.7958,  5.1284],\n",
      "          [ 7.8947, 14.8190]],\n",
      "\n",
      "         [[-0.5140,  5.5958],\n",
      "          [ 8.5543, 14.6641]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 340: After ReLU = tensor([[[[ 0.0000,  5.5958],\n",
      "          [ 8.5543, 14.6641]],\n",
      "\n",
      "         [[ 0.0000,  5.1284],\n",
      "          [ 7.8947, 14.8190]],\n",
      "\n",
      "         [[ 0.0000,  5.5958],\n",
      "          [ 8.5543, 14.6641]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 340: Loss = 13.245784759521484\n",
      "Epoch 345: After Conv = tensor([[[[ 8.8700,  9.7195],\n",
      "          [10.1314, 10.9809]],\n",
      "\n",
      "         [[-0.0993,  0.3385],\n",
      "          [ 0.5648,  1.0025]],\n",
      "\n",
      "         [[ 6.8700,  7.7196],\n",
      "          [ 8.1314,  8.9809]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 345: After BatchNorm = tensor([[[[-0.4950,  5.6488],\n",
      "          [ 8.6271, 14.7708]],\n",
      "\n",
      "         [[-1.7139,  4.8754],\n",
      "          [ 8.2818, 14.8710]],\n",
      "\n",
      "         [[-0.4950,  5.6488],\n",
      "          [ 8.6271, 14.7709]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 345: After ReLU = tensor([[[[ 0.0000,  5.6488],\n",
      "          [ 8.6271, 14.7708]],\n",
      "\n",
      "         [[ 0.0000,  4.8754],\n",
      "          [ 8.2818, 14.8710]],\n",
      "\n",
      "         [[ 0.0000,  5.6488],\n",
      "          [ 8.6271, 14.7709]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 345: Loss = 12.939888000488281\n",
      "Epoch 350: After Conv = tensor([[[[ 8.8696,  9.7194],\n",
      "          [10.1318, 10.9816]],\n",
      "\n",
      "         [[-0.0639,  0.3488],\n",
      "          [ 0.5154,  0.9281]],\n",
      "\n",
      "         [[ 6.8696,  7.7194],\n",
      "          [ 8.1319,  8.9817]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 350: After BatchNorm = tensor([[[[-0.4760,  5.7009],\n",
      "          [ 8.6988, 14.8757]],\n",
      "\n",
      "         [[-1.7461,  5.2360],\n",
      "          [ 8.0532, 15.0354]],\n",
      "\n",
      "         [[-0.4760,  5.7010],\n",
      "          [ 8.6988, 14.8757]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 350: After ReLU = tensor([[[[ 0.0000,  5.7009],\n",
      "          [ 8.6988, 14.8757]],\n",
      "\n",
      "         [[ 0.0000,  5.2360],\n",
      "          [ 8.0532, 15.0354]],\n",
      "\n",
      "         [[ 0.0000,  5.7010],\n",
      "          [ 8.6988, 14.8757]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 350: Loss = 12.641375541687012\n",
      "Epoch 355: After Conv = tensor([[[[ 8.8691,  9.7192],\n",
      "          [10.1322, 10.9823]],\n",
      "\n",
      "         [[-0.1054,  0.3333],\n",
      "          [ 0.5584,  0.9971]],\n",
      "\n",
      "         [[ 6.8692,  7.7193],\n",
      "          [ 8.1323,  8.9824]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 355: After BatchNorm = tensor([[[[-0.4571,  5.7523],\n",
      "          [ 8.7694, 14.9788]],\n",
      "\n",
      "         [[-1.6675,  4.9986],\n",
      "          [ 8.4207, 15.0868]],\n",
      "\n",
      "         [[-0.4571,  5.7523],\n",
      "          [ 8.7694, 14.9788]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 355: After ReLU = tensor([[[[ 0.0000,  5.7523],\n",
      "          [ 8.7694, 14.9788]],\n",
      "\n",
      "         [[ 0.0000,  4.9986],\n",
      "          [ 8.4207, 15.0868]],\n",
      "\n",
      "         [[ 0.0000,  5.7523],\n",
      "          [ 8.7694, 14.9788]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 355: Loss = 12.354670524597168\n",
      "Epoch 360: After Conv = tensor([[[[ 8.8687,  9.7191],\n",
      "          [10.1327, 10.9830]],\n",
      "\n",
      "         [[-0.0720,  0.3431],\n",
      "          [ 0.5123,  0.9274]],\n",
      "\n",
      "         [[ 6.8688,  7.7191],\n",
      "          [ 8.1328,  8.9831]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 360: After BatchNorm = tensor([[[[-0.4383,  5.8029],\n",
      "          [ 8.8389, 15.0801]],\n",
      "\n",
      "         [[-1.6968,  5.3402],\n",
      "          [ 8.2072, 15.2442]],\n",
      "\n",
      "         [[-0.4383,  5.8029],\n",
      "          [ 8.8389, 15.0801]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 360: After ReLU = tensor([[[[ 0.0000,  5.8029],\n",
      "          [ 8.8389, 15.0801]],\n",
      "\n",
      "         [[ 0.0000,  5.3402],\n",
      "          [ 8.2072, 15.2442]],\n",
      "\n",
      "         [[ 0.0000,  5.8029],\n",
      "          [ 8.8389, 15.0801]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 360: Loss = 12.075173377990723\n",
      "Epoch 365: After Conv = tensor([[[[ 8.8683,  9.7189],\n",
      "          [10.1332, 10.9838]],\n",
      "\n",
      "         [[-0.1106,  0.3287],\n",
      "          [ 0.5528,  0.9921]],\n",
      "\n",
      "         [[ 6.8684,  7.7190],\n",
      "          [ 8.1332,  8.9838]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 365: After BatchNorm = tensor([[[[-0.4196,  5.8527],\n",
      "          [ 8.9073, 15.1796]],\n",
      "\n",
      "         [[-1.6215,  5.1181],\n",
      "          [ 8.5555, 15.2951]],\n",
      "\n",
      "         [[-0.4196,  5.8527],\n",
      "          [ 8.9073, 15.1796]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 365: After ReLU = tensor([[[[ 0.0000,  5.8527],\n",
      "          [ 8.9073, 15.1796]],\n",
      "\n",
      "         [[ 0.0000,  5.1181],\n",
      "          [ 8.5555, 15.2951]],\n",
      "\n",
      "         [[ 0.0000,  5.8527],\n",
      "          [ 8.9073, 15.1796]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 365: Loss = 11.80645751953125\n",
      "Epoch 370: After Conv = tensor([[[[ 8.8679,  9.7188],\n",
      "          [10.1336, 10.9845]],\n",
      "\n",
      "         [[-0.0792,  0.3381],\n",
      "          [ 0.5098,  0.9271]],\n",
      "\n",
      "         [[ 6.8680,  7.7188],\n",
      "          [ 8.1337,  8.9845]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 370: After BatchNorm = tensor([[[[-0.4010,  5.9018],\n",
      "          [ 8.9747, 15.2774]],\n",
      "\n",
      "         [[-1.6479,  5.4411],\n",
      "          [ 8.3568, 15.4458]],\n",
      "\n",
      "         [[-0.4010,  5.9018],\n",
      "          [ 8.9747, 15.2774]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 370: After ReLU = tensor([[[[ 0.0000,  5.9018],\n",
      "          [ 8.9747, 15.2774]],\n",
      "\n",
      "         [[ 0.0000,  5.4411],\n",
      "          [ 8.3568, 15.4458]],\n",
      "\n",
      "         [[ 0.0000,  5.9018],\n",
      "          [ 8.9747, 15.2774]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 370: Loss = 11.544737815856934\n",
      "Epoch 375: After Conv = tensor([[[[ 8.8675,  9.7186],\n",
      "          [10.1341, 10.9852]],\n",
      "\n",
      "         [[-0.1152,  0.3248],\n",
      "          [ 0.5478,  0.9877]],\n",
      "\n",
      "         [[ 6.8676,  7.7187],\n",
      "          [ 8.1341,  8.9852]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 375: After BatchNorm = tensor([[[[-0.3824,  5.9501],\n",
      "          [ 9.0410, 15.3736]],\n",
      "\n",
      "         [[-1.5758,  5.2342],\n",
      "          [ 8.6861, 15.4962]],\n",
      "\n",
      "         [[-0.3824,  5.9501],\n",
      "          [ 9.0410, 15.3736]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 375: After ReLU = tensor([[[[ 0.0000,  5.9501],\n",
      "          [ 9.0410, 15.3736]],\n",
      "\n",
      "         [[ 0.0000,  5.2342],\n",
      "          [ 8.6861, 15.4962]],\n",
      "\n",
      "         [[ 0.0000,  5.9501],\n",
      "          [ 9.0410, 15.3736]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 375: Loss = 11.292869567871094\n",
      "Epoch 380: After Conv = tensor([[[[ 8.8671,  9.7185],\n",
      "          [10.1345, 10.9859]],\n",
      "\n",
      "         [[-0.0857,  0.3336],\n",
      "          [ 0.5077,  0.9271]],\n",
      "\n",
      "         [[ 6.8672,  7.7185],\n",
      "          [ 8.1346,  8.9859]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 380: After BatchNorm = tensor([[[[-0.3639,  5.9977],\n",
      "          [ 9.1063, 15.4680]],\n",
      "\n",
      "         [[-1.5993,  5.5387],\n",
      "          [ 8.5022, 15.6403]],\n",
      "\n",
      "         [[-0.3639,  5.9977],\n",
      "          [ 9.1064, 15.4680]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 380: After ReLU = tensor([[[[ 0.0000,  5.9977],\n",
      "          [ 9.1063, 15.4680]],\n",
      "\n",
      "         [[ 0.0000,  5.5387],\n",
      "          [ 8.5022, 15.6403]],\n",
      "\n",
      "         [[ 0.0000,  5.9977],\n",
      "          [ 9.1064, 15.4680]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 380: Loss = 11.047751426696777\n",
      "Epoch 385: After Conv = tensor([[[[ 8.8667,  9.7184],\n",
      "          [10.1350, 10.9866]],\n",
      "\n",
      "         [[-0.1191,  0.3213],\n",
      "          [ 0.5433,  0.9837]],\n",
      "\n",
      "         [[ 6.8668,  7.7184],\n",
      "          [ 8.1350,  8.9867]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 385: After BatchNorm = tensor([[[[-0.3455,  6.0446],\n",
      "          [ 9.1707, 15.5608]],\n",
      "\n",
      "         [[-1.5305,  5.3469],\n",
      "          [ 8.8128, 15.6903]],\n",
      "\n",
      "         [[-0.3455,  6.0446],\n",
      "          [ 9.1707, 15.5608]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 385: After ReLU = tensor([[[[ 0.0000,  6.0446],\n",
      "          [ 9.1707, 15.5608]],\n",
      "\n",
      "         [[ 0.0000,  5.3469],\n",
      "          [ 8.8128, 15.6903]],\n",
      "\n",
      "         [[ 0.0000,  6.0446],\n",
      "          [ 9.1707, 15.5608]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 385: Loss = 10.81166934967041\n",
      "Epoch 390: After Conv = tensor([[[[ 8.8663,  9.7182],\n",
      "          [10.1354, 10.9873]],\n",
      "\n",
      "         [[-0.0916,  0.3297],\n",
      "          [ 0.5062,  0.9275]],\n",
      "\n",
      "         [[ 6.8664,  7.7183],\n",
      "          [ 8.1355,  8.9874]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 390: After BatchNorm = tensor([[[[-0.3271,  6.0908],\n",
      "          [ 9.2340, 15.6520]],\n",
      "\n",
      "         [[-1.5512,  5.6333],\n",
      "          [ 8.6435, 15.8279]],\n",
      "\n",
      "         [[-0.3271,  6.0908],\n",
      "          [ 9.2340, 15.6520]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 390: After ReLU = tensor([[[[ 0.0000,  6.0908],\n",
      "          [ 9.2340, 15.6520]],\n",
      "\n",
      "         [[ 0.0000,  5.6333],\n",
      "          [ 8.6435, 15.8279]],\n",
      "\n",
      "         [[ 0.0000,  6.0908],\n",
      "          [ 9.2340, 15.6520]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 390: Loss = 10.582079887390137\n",
      "Epoch 395: After Conv = tensor([[[[ 8.8659,  9.7181],\n",
      "          [10.1359, 10.9881]],\n",
      "\n",
      "         [[-0.1224,  0.3184],\n",
      "          [ 0.5393,  0.9800]],\n",
      "\n",
      "         [[ 6.8660,  7.7181],\n",
      "          [ 8.1359,  8.9881]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 395: After BatchNorm = tensor([[[[-0.3089,  6.1363],\n",
      "          [ 9.2964, 15.7416]],\n",
      "\n",
      "         [[-1.4856,  5.4564],\n",
      "          [ 8.9356, 15.8776]],\n",
      "\n",
      "         [[-0.3089,  6.1363],\n",
      "          [ 9.2964, 15.7416]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 395: After ReLU = tensor([[[[ 0.0000,  6.1363],\n",
      "          [ 9.2964, 15.7416]],\n",
      "\n",
      "         [[ 0.0000,  5.4564],\n",
      "          [ 8.9356, 15.8776]],\n",
      "\n",
      "         [[ 0.0000,  6.1363],\n",
      "          [ 9.2964, 15.7416]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 395: Loss = 10.360785484313965\n",
      "Epoch 400: After Conv = tensor([[[[ 8.8655,  9.7179],\n",
      "          [10.1364, 10.9888]],\n",
      "\n",
      "         [[-0.0968,  0.3262],\n",
      "          [ 0.5051,  0.9281]],\n",
      "\n",
      "         [[ 6.8656,  7.7180],\n",
      "          [ 8.1364,  8.9888]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 400: After BatchNorm = tensor([[[[-0.2907,  6.1811],\n",
      "          [ 9.3578, 15.8296]],\n",
      "\n",
      "         [[-1.5034,  5.7247],\n",
      "          [ 8.7809, 16.0090]],\n",
      "\n",
      "         [[-0.2907,  6.1811],\n",
      "          [ 9.3578, 15.8296]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 400: After ReLU = tensor([[[[ 0.0000,  6.1811],\n",
      "          [ 9.3578, 15.8296]],\n",
      "\n",
      "         [[ 0.0000,  5.7247],\n",
      "          [ 8.7809, 16.0090]],\n",
      "\n",
      "         [[ 0.0000,  6.1811],\n",
      "          [ 9.3578, 15.8296]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 400: Loss = 10.145709991455078\n",
      "Epoch 405: After Conv = tensor([[[[ 8.8651,  9.7178],\n",
      "          [10.1368, 10.9895]],\n",
      "\n",
      "         [[-0.1251,  0.3159],\n",
      "          [ 0.5357,  0.9767]],\n",
      "\n",
      "         [[ 6.8652,  7.7178],\n",
      "          [ 8.1368,  8.9895]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 405: After BatchNorm = tensor([[[[-0.2726,  6.2253],\n",
      "          [ 9.4183, 15.9161]],\n",
      "\n",
      "         [[-1.4410,  5.5629],\n",
      "          [ 9.0546, 16.0585]],\n",
      "\n",
      "         [[-0.2726,  6.2253],\n",
      "          [ 9.4183, 15.9161]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 405: After ReLU = tensor([[[[ 0.0000,  6.2253],\n",
      "          [ 9.4183, 15.9161]],\n",
      "\n",
      "         [[ 0.0000,  5.5629],\n",
      "          [ 9.0546, 16.0585]],\n",
      "\n",
      "         [[ 0.0000,  6.2253],\n",
      "          [ 9.4183, 15.9161]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 405: Loss = 9.938261985778809\n",
      "Epoch 410: After Conv = tensor([[[[ 8.8648,  9.7177],\n",
      "          [10.1373, 10.9902]],\n",
      "\n",
      "         [[-0.1014,  0.3232],\n",
      "          [ 0.5044,  0.9290]],\n",
      "\n",
      "         [[ 6.8648,  7.7177],\n",
      "          [ 8.1373,  8.9902]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 410: After BatchNorm = tensor([[[[-0.2545,  6.2687],\n",
      "          [ 9.4779, 16.0011]],\n",
      "\n",
      "         [[-1.4559,  5.8132],\n",
      "          [ 8.9145, 16.1836]],\n",
      "\n",
      "         [[-0.2545,  6.2687],\n",
      "          [ 9.4778, 16.0011]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 410: After ReLU = tensor([[[[ 0.0000,  6.2687],\n",
      "          [ 9.4779, 16.0011]],\n",
      "\n",
      "         [[ 0.0000,  5.8132],\n",
      "          [ 8.9145, 16.1836]],\n",
      "\n",
      "         [[ 0.0000,  6.2687],\n",
      "          [ 9.4778, 16.0011]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 410: Loss = 9.736771583557129\n",
      "Epoch 415: After Conv = tensor([[[[ 8.8644,  9.7175],\n",
      "          [10.1377, 10.9909]],\n",
      "\n",
      "         [[-0.1273,  0.3138],\n",
      "          [ 0.5326,  0.9737]],\n",
      "\n",
      "         [[ 6.8644,  7.7176],\n",
      "          [ 8.1377,  8.9909]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 415: After BatchNorm = tensor([[[[-0.2366,  6.3116],\n",
      "          [ 9.5365, 16.0846]],\n",
      "\n",
      "         [[-1.3968,  5.6664],\n",
      "          [ 9.1698, 16.2331]],\n",
      "\n",
      "         [[-0.2366,  6.3116],\n",
      "          [ 9.5365, 16.0846]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 415: After ReLU = tensor([[[[ 0.0000,  6.3116],\n",
      "          [ 9.5365, 16.0846]],\n",
      "\n",
      "         [[ 0.0000,  5.6664],\n",
      "          [ 9.1698, 16.2331]],\n",
      "\n",
      "         [[ 0.0000,  6.3116],\n",
      "          [ 9.5365, 16.0846]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 415: Loss = 9.542285919189453\n",
      "Epoch 420: After Conv = tensor([[[[ 8.8640,  9.7174],\n",
      "          [10.1382, 10.9916]],\n",
      "\n",
      "         [[-0.1056,  0.3206],\n",
      "          [ 0.5040,  0.9302]],\n",
      "\n",
      "         [[ 6.8640,  7.7174],\n",
      "          [ 8.1382,  8.9916]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 420: After BatchNorm = tensor([[[[-0.2187,  6.3538],\n",
      "          [ 9.5942, 16.1667]],\n",
      "\n",
      "         [[-1.4089,  5.8988],\n",
      "          [ 9.0445, 16.3521]],\n",
      "\n",
      "         [[-0.2187,  6.3538],\n",
      "          [ 9.5942, 16.1667]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 420: After ReLU = tensor([[[[ 0.0000,  6.3538],\n",
      "          [ 9.5942, 16.1667]],\n",
      "\n",
      "         [[ 0.0000,  5.8988],\n",
      "          [ 9.0445, 16.3521]],\n",
      "\n",
      "         [[ 0.0000,  6.3538],\n",
      "          [ 9.5942, 16.1667]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 420: Loss = 9.353507995605469\n",
      "Epoch 425: After Conv = tensor([[[[ 8.8636,  9.7173],\n",
      "          [10.1386, 10.9923]],\n",
      "\n",
      "         [[-0.1290,  0.3121],\n",
      "          [ 0.5298,  0.9709]],\n",
      "\n",
      "         [[ 6.8636,  7.7173],\n",
      "          [ 8.1386,  8.9923]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 425: After BatchNorm = tensor([[[[-0.2008,  6.3954],\n",
      "          [ 9.6511, 16.2473]],\n",
      "\n",
      "         [[-1.3530,  5.7673],\n",
      "          [ 9.2813, 16.4016]],\n",
      "\n",
      "         [[-0.2008,  6.3954],\n",
      "          [ 9.6511, 16.2473]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 425: After ReLU = tensor([[[[ 0.0000,  6.3954],\n",
      "          [ 9.6511, 16.2473]],\n",
      "\n",
      "         [[ 0.0000,  5.7673],\n",
      "          [ 9.2813, 16.4016]],\n",
      "\n",
      "         [[ 0.0000,  6.3954],\n",
      "          [ 9.6511, 16.2473]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 425: Loss = 9.171165466308594\n",
      "Epoch 430: After Conv = tensor([[[[ 8.8632,  9.7171],\n",
      "          [10.1391, 10.9930]],\n",
      "\n",
      "         [[-0.1093,  0.3183],\n",
      "          [ 0.5040,  0.9315]],\n",
      "\n",
      "         [[ 6.8632,  7.7171],\n",
      "          [ 8.1391,  8.9930]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 430: After BatchNorm = tensor([[[[-0.1831,  6.4363],\n",
      "          [ 9.7071, 16.3265]],\n",
      "\n",
      "         [[-1.3621,  5.9814],\n",
      "          [ 9.1710, 16.5146]],\n",
      "\n",
      "         [[-0.1831,  6.4363],\n",
      "          [ 9.7071, 16.3265]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 430: After ReLU = tensor([[[[ 0.0000,  6.4363],\n",
      "          [ 9.7071, 16.3265]],\n",
      "\n",
      "         [[ 0.0000,  5.9814],\n",
      "          [ 9.1710, 16.5146]],\n",
      "\n",
      "         [[ 0.0000,  6.4363],\n",
      "          [ 9.7071, 16.3265]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 430: Loss = 8.99425983428955\n",
      "Epoch 435: After Conv = tensor([[[[ 8.8628,  9.7170],\n",
      "          [10.1395, 10.9937]],\n",
      "\n",
      "         [[-0.1304,  0.3107],\n",
      "          [ 0.5273,  0.9683]],\n",
      "\n",
      "         [[ 6.8628,  7.7170],\n",
      "          [ 8.1395,  8.9937]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 435: After BatchNorm = tensor([[[[-0.1654,  6.4767],\n",
      "          [ 9.7622, 16.4044]],\n",
      "\n",
      "         [[-1.3096,  5.8655],\n",
      "          [ 9.3893, 16.5643]],\n",
      "\n",
      "         [[-0.1654,  6.4767],\n",
      "          [ 9.7623, 16.4044]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 435: After ReLU = tensor([[[[ 0.0000,  6.4767],\n",
      "          [ 9.7622, 16.4044]],\n",
      "\n",
      "         [[ 0.0000,  5.8655],\n",
      "          [ 9.3893, 16.5643]],\n",
      "\n",
      "         [[ 0.0000,  6.4767],\n",
      "          [ 9.7623, 16.4044]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 435: Loss = 8.823298454284668\n",
      "Epoch 440: After Conv = tensor([[[[ 8.8624,  9.7169],\n",
      "          [10.1400, 10.9944]],\n",
      "\n",
      "         [[-0.1126,  0.3163],\n",
      "          [ 0.5043,  0.9332]],\n",
      "\n",
      "         [[ 6.8624,  7.7168],\n",
      "          [ 8.1400,  8.9944]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 440: After BatchNorm = tensor([[[[-0.1478,  6.5164],\n",
      "          [ 9.8166, 16.4808]],\n",
      "\n",
      "         [[-1.3157,  6.0613],\n",
      "          [ 9.2942, 16.6713]],\n",
      "\n",
      "         [[-0.1478,  6.5164],\n",
      "          [ 9.8166, 16.4808]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 440: After ReLU = tensor([[[[ 0.0000,  6.5164],\n",
      "          [ 9.8166, 16.4808]],\n",
      "\n",
      "         [[ 0.0000,  6.0613],\n",
      "          [ 9.2942, 16.6713]],\n",
      "\n",
      "         [[ 0.0000,  6.5164],\n",
      "          [ 9.8166, 16.4808]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 440: Loss = 8.657492637634277\n",
      "Epoch 445: After Conv = tensor([[[[ 8.8620,  9.7167],\n",
      "          [10.1404, 10.9951]],\n",
      "\n",
      "         [[-0.1313,  0.3096],\n",
      "          [ 0.5251,  0.9660]],\n",
      "\n",
      "         [[ 6.8620,  7.7167],\n",
      "          [ 8.1404,  8.9951]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 445: After BatchNorm = tensor([[[[-0.1303,  6.5556],\n",
      "          [ 9.8701, 16.5560]],\n",
      "\n",
      "         [[-1.2665,  5.9610],\n",
      "          [ 9.4938, 16.7213]],\n",
      "\n",
      "         [[-0.1303,  6.5556],\n",
      "          [ 9.8701, 16.5560]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 445: After ReLU = tensor([[[[ 0.0000,  6.5556],\n",
      "          [ 9.8701, 16.5560]],\n",
      "\n",
      "         [[ 0.0000,  5.9610],\n",
      "          [ 9.4938, 16.7213]],\n",
      "\n",
      "         [[ 0.0000,  6.5556],\n",
      "          [ 9.8701, 16.5560]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 445: Loss = 8.497174263000488\n",
      "Epoch 450: After Conv = tensor([[[[ 8.8616,  9.7166],\n",
      "          [10.1409, 10.9958]],\n",
      "\n",
      "         [[-0.1156,  0.3146],\n",
      "          [ 0.5048,  0.9350]],\n",
      "\n",
      "         [[ 6.8616,  7.7166],\n",
      "          [ 8.1409,  8.9958]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 450: After BatchNorm = tensor([[[[-0.1129,  6.5942],\n",
      "          [ 9.9227, 16.6298]],\n",
      "\n",
      "         [[-1.2697,  6.1386],\n",
      "          [ 9.4141, 16.8224]],\n",
      "\n",
      "         [[-0.1128,  6.5942],\n",
      "          [ 9.9227, 16.6298]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 450: After ReLU = tensor([[[[ 0.0000,  6.5942],\n",
      "          [ 9.9227, 16.6298]],\n",
      "\n",
      "         [[ 0.0000,  6.1386],\n",
      "          [ 9.4141, 16.8224]],\n",
      "\n",
      "         [[ 0.0000,  6.5942],\n",
      "          [ 9.9227, 16.6298]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 450: Loss = 8.341769218444824\n",
      "Epoch 455: After Conv = tensor([[[[ 8.8612,  9.7164],\n",
      "          [10.1413, 10.9966]],\n",
      "\n",
      "         [[-0.1319,  0.3088],\n",
      "          [ 0.5232,  0.9639]],\n",
      "\n",
      "         [[ 6.8612,  7.7164],\n",
      "          [ 8.1413,  8.9965]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 455: After BatchNorm = tensor([[[[-0.0955,  6.6322],\n",
      "          [ 9.9746, 16.7023]],\n",
      "\n",
      "         [[-1.2238,  6.0539],\n",
      "          [ 9.5951, 16.8728]],\n",
      "\n",
      "         [[-0.0955,  6.6322],\n",
      "          [ 9.9746, 16.7023]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 455: After ReLU = tensor([[[[ 0.0000,  6.6322],\n",
      "          [ 9.9746, 16.7023]],\n",
      "\n",
      "         [[ 0.0000,  6.0539],\n",
      "          [ 9.5951, 16.8728]],\n",
      "\n",
      "         [[ 0.0000,  6.6322],\n",
      "          [ 9.9746, 16.7023]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 455: Loss = 8.191439628601074\n",
      "Epoch 460: After Conv = tensor([[[[ 8.8608,  9.7163],\n",
      "          [10.1418, 10.9973]],\n",
      "\n",
      "         [[-0.1182,  0.3132],\n",
      "          [ 0.5056,  0.9369]],\n",
      "\n",
      "         [[ 6.8608,  7.7163],\n",
      "          [ 8.1418,  8.9972]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 460: After BatchNorm = tensor([[[[-0.0782,  6.6697],\n",
      "          [10.0257, 16.7735]],\n",
      "\n",
      "         [[-1.2240,  6.2133],\n",
      "          [ 9.5308, 16.9680]],\n",
      "\n",
      "         [[-0.0782,  6.6697],\n",
      "          [10.0257, 16.7735]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 460: After ReLU = tensor([[[[ 0.0000,  6.6697],\n",
      "          [10.0257, 16.7735]],\n",
      "\n",
      "         [[ 0.0000,  6.2133],\n",
      "          [ 9.5308, 16.9680]],\n",
      "\n",
      "         [[ 0.0000,  6.6697],\n",
      "          [10.0257, 16.7735]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 460: Loss = 8.045741081237793\n",
      "Epoch 465: After Conv = tensor([[[[ 8.8604,  9.7162],\n",
      "          [10.1422, 10.9979]],\n",
      "\n",
      "         [[-0.1323,  0.3082],\n",
      "          [ 0.5215,  0.9620]],\n",
      "\n",
      "         [[ 6.8604,  7.7162],\n",
      "          [ 8.1422,  8.9979]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 465: After BatchNorm = tensor([[[[-0.0609,  6.7066],\n",
      "          [10.0760, 16.8435]],\n",
      "\n",
      "         [[-1.1814,  6.1444],\n",
      "          [ 9.6933, 17.0190]],\n",
      "\n",
      "         [[-0.0609,  6.7066],\n",
      "          [10.0760, 16.8435]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 465: After ReLU = tensor([[[[ 0.0000,  6.7066],\n",
      "          [10.0760, 16.8435]],\n",
      "\n",
      "         [[ 0.0000,  6.1444],\n",
      "          [ 9.6933, 17.0190]],\n",
      "\n",
      "         [[ 0.0000,  6.7066],\n",
      "          [10.0760, 16.8435]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 465: Loss = 7.904754161834717\n",
      "Epoch 470: After Conv = tensor([[[[ 8.8601,  9.7160],\n",
      "          [10.1427, 10.9986]],\n",
      "\n",
      "         [[-0.1205,  0.3119],\n",
      "          [ 0.5065,  0.9390]],\n",
      "\n",
      "         [[ 6.8600,  7.7160],\n",
      "          [ 8.1426,  8.9986]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 470: After BatchNorm = tensor([[[[-0.0437,  6.7430],\n",
      "          [10.1256, 16.9123]],\n",
      "\n",
      "         [[-1.1786,  6.2857],\n",
      "          [ 9.6442, 17.1085]],\n",
      "\n",
      "         [[-0.0438,  6.7430],\n",
      "          [10.1256, 16.9123]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 470: After ReLU = tensor([[[[ 0.0000,  6.7430],\n",
      "          [10.1256, 16.9123]],\n",
      "\n",
      "         [[ 0.0000,  6.2857],\n",
      "          [ 9.6442, 17.1085]],\n",
      "\n",
      "         [[ 0.0000,  6.7430],\n",
      "          [10.1256, 16.9123]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 470: Loss = 7.768161773681641\n",
      "Epoch 475: After Conv = tensor([[[[ 8.8597,  9.7159],\n",
      "          [10.1431, 10.9993]],\n",
      "\n",
      "         [[-0.1324,  0.3077],\n",
      "          [ 0.5201,  0.9603]],\n",
      "\n",
      "         [[ 6.8596,  7.7159],\n",
      "          [ 8.1431,  8.9993]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 475: After BatchNorm = tensor([[[[-0.0266,  6.7788],\n",
      "          [10.1744, 16.9799]],\n",
      "\n",
      "         [[-1.1393,  6.2323],\n",
      "          [ 9.7885, 17.1601]],\n",
      "\n",
      "         [[-0.0267,  6.7788],\n",
      "          [10.1744, 16.9799]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 475: After ReLU = tensor([[[[ 0.0000,  6.7788],\n",
      "          [10.1744, 16.9799]],\n",
      "\n",
      "         [[ 0.0000,  6.2323],\n",
      "          [ 9.7885, 17.1601]],\n",
      "\n",
      "         [[ 0.0000,  6.7788],\n",
      "          [10.1744, 16.9799]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 475: Loss = 7.63593864440918\n",
      "Epoch 480: After Conv = tensor([[[[ 8.8593,  9.7158],\n",
      "          [10.1436, 11.0000]],\n",
      "\n",
      "         [[-0.1226,  0.3109],\n",
      "          [ 0.5076,  0.9412]],\n",
      "\n",
      "         [[ 6.8593,  7.7157],\n",
      "          [ 8.1435,  9.0000]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 480: After BatchNorm = tensor([[[[-9.6207e-03,  6.8141e+00],\n",
      "          [ 1.0222e+01,  1.7046e+01]],\n",
      "\n",
      "         [[-1.1336e+00,  6.3558e+00],\n",
      "          [ 9.7545e+00,  1.7244e+01]],\n",
      "\n",
      "         [[-9.6283e-03,  6.8141e+00],\n",
      "          [ 1.0222e+01,  1.7046e+01]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 480: After ReLU = tensor([[[[ 0.0000,  6.8141],\n",
      "          [10.2224, 17.0462]],\n",
      "\n",
      "         [[ 0.0000,  6.3558],\n",
      "          [ 9.7545, 17.2440]],\n",
      "\n",
      "         [[ 0.0000,  6.8141],\n",
      "          [10.2224, 17.0462]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 480: Loss = 7.507850646972656\n",
      "Epoch 485: After Conv = tensor([[[[ 8.7853,  9.6914],\n",
      "          [10.2352, 11.1413]],\n",
      "\n",
      "         [[-0.1324,  0.3075],\n",
      "          [ 0.5189,  0.9588]],\n",
      "\n",
      "         [[ 6.7852,  7.6913],\n",
      "          [ 8.2352,  9.1413]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 485: After BatchNorm = tensor([[[[ 0.0856,  6.6097],\n",
      "          [10.5256, 17.0498]],\n",
      "\n",
      "         [[-1.0976,  6.3175],\n",
      "          [ 9.8811, 17.2962]],\n",
      "\n",
      "         [[ 0.0856,  6.6097],\n",
      "          [10.5257, 17.0498]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 485: After ReLU = tensor([[[[ 0.0856,  6.6097],\n",
      "          [10.5256, 17.0498]],\n",
      "\n",
      "         [[ 0.0000,  6.3175],\n",
      "          [ 9.8811, 17.2962]],\n",
      "\n",
      "         [[ 0.0856,  6.6097],\n",
      "          [10.5257, 17.0498]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 485: Loss = 7.319879055023193\n",
      "Epoch 490: After Conv = tensor([[[[ 8.7244,  9.6724],\n",
      "          [10.3155, 11.2635]],\n",
      "\n",
      "         [[-0.1245,  0.3100],\n",
      "          [ 0.5088,  0.9434]],\n",
      "\n",
      "         [[ 6.7243,  7.6724],\n",
      "          [ 8.3154,  9.2635]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 490: After BatchNorm = tensor([[[[ 0.2051,  6.4954],\n",
      "          [10.7619, 17.0522]],\n",
      "\n",
      "         [[-1.0891,  6.4240],\n",
      "          [ 9.8616, 17.3746]],\n",
      "\n",
      "         [[ 0.2051,  6.4954],\n",
      "          [10.7620, 17.0522]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 490: After ReLU = tensor([[[[ 0.2051,  6.4954],\n",
      "          [10.7619, 17.0522]],\n",
      "\n",
      "         [[ 0.0000,  6.4240],\n",
      "          [ 9.8616, 17.3746]],\n",
      "\n",
      "         [[ 0.2051,  6.4954],\n",
      "          [10.7620, 17.0522]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 490: Loss = 7.112621307373047\n",
      "Epoch 495: After Conv = tensor([[[[ 8.7030,  9.6660],\n",
      "          [10.3447, 11.3077]],\n",
      "\n",
      "         [[-0.1324,  0.3073],\n",
      "          [ 0.5178,  0.9575]],\n",
      "\n",
      "         [[ 6.7030,  7.6660],\n",
      "          [ 8.3446,  9.3076]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 495: After BatchNorm = tensor([[[[ 0.2947,  6.5015],\n",
      "          [10.8757, 17.0825]],\n",
      "\n",
      "         [[-1.0560,  6.4000],\n",
      "          [ 9.9713, 17.4273]],\n",
      "\n",
      "         [[ 0.2947,  6.5016],\n",
      "          [10.8757, 17.0825]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 495: After ReLU = tensor([[[[ 0.2947,  6.5015],\n",
      "          [10.8757, 17.0825]],\n",
      "\n",
      "         [[ 0.0000,  6.4000],\n",
      "          [ 9.9713, 17.4273]],\n",
      "\n",
      "         [[ 0.2947,  6.5016],\n",
      "          [10.8757, 17.0825]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 495: Loss = 6.921390533447266\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tạo input (3x3x3)\n",
    "x = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [[1.0, 2, 3], [4, 5, 6], [7, 8, 9]],  # Channel 1\n",
    "            [[9.0, 8, 7], [6, 5, 4], [3, 2, 1]],  # Channel 2\n",
    "            [[1.0, 1, 1], [2, 2, 2], [3, 3, 3]],  # Channel 3\n",
    "        ]\n",
    "    ],\n",
    "    requires_grad=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Tạo kernel (2x2) và BatchNorm\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=2, stride=1, bias=False)\n",
    "batch_norm = nn.BatchNorm2d(3)  # Thêm BatchNorm\n",
    "relu = nn.ReLU(inplace=True)\n",
    "\n",
    "# Khởi tạo kernel với giá trị cụ thể Shape (3, 3, 2, 2)\n",
    "with torch.no_grad():\n",
    "    conv.weight = nn.Parameter(\n",
    "        torch.tensor(\n",
    "            [\n",
    "                [\n",
    "                    [[1.0, -1], [2, 0]],\n",
    "                    [[0.5, -0.5], [1, -1]],\n",
    "                    [[-1, 1], [0.5, -0.5]],\n",
    "                ],  # Kernel 1\n",
    "                [\n",
    "                    [[2.0, 1], [-2, 0]],\n",
    "                    [[1.5, -1], [-0.5, 0.5]],\n",
    "                    [[0, 0.5], [-1, 1]],\n",
    "                ],  # Kernel 2\n",
    "                [\n",
    "                    [[1.0, -1], [2, 0]],\n",
    "                    [[-1, 1], [0.5, -0.5]],\n",
    "                    [[2, -2], [1, -1]],\n",
    "                ],  # Kernel 3\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Giả sử nhãn thật\n",
    "y_true = torch.tensor([[[[5.0, 10], [14, 20]]]])\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(list(conv.parameters()) + list(batch_norm.parameters()), lr=0.01)\n",
    "\n",
    "# Lưu lịch sử loss\n",
    "loss_history = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Xóa gradient trước đó\n",
    "\n",
    "    # Forward pass với BatchNorm\n",
    "    output = conv(x)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: After Conv = {output}\")\n",
    "    output_bn = batch_norm(output)  # Áp dụng BatchNorm\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: After BatchNorm = {output_bn}\")\n",
    "    output_rl = relu(output_bn)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: After ReLU = {output_rl}\")\n",
    "\n",
    "    # Tính loss (MSE)\n",
    "    loss = torch.mean((output_rl - y_true) ** 2) / 2\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Cập nhật trọng số\n",
    "    optimizer.step()\n",
    "\n",
    "    # In loss mỗi 5 epoch\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
    "\n",
    "# Vẽ đồ thị loss\n",
    "plt.plot(loss_history, label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss giảm dần qua từng epoch với CNN\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Input(3x3x3) với CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: After Conv = tensor([[[[ 8.5000, 10.5000],\n",
      "          [14.5000, 16.5000]],\n",
      "\n",
      "         [[ 1.5000,  2.0000],\n",
      "          [ 3.5000,  4.0000]],\n",
      "\n",
      "         [[ 6.5000,  8.5000],\n",
      "          [12.5000, 14.5000]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 0: After BatchNorm = tensor([[[[-1.2649, -0.6325],\n",
      "          [ 0.6325,  1.2649]],\n",
      "\n",
      "         [[-1.2127, -0.7276],\n",
      "          [ 0.7276,  1.2127]],\n",
      "\n",
      "         [[-1.2649, -0.6325],\n",
      "          [ 0.6325,  1.2649]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 0: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.6325, 1.2649]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.7276, 1.2127]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.6325, 1.2649]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 0: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [1.2649, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.4552, 2.4253]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 0: After MaxPool = tensor([[[[1.2649]],\n",
      "\n",
      "         [[2.4253]],\n",
      "\n",
      "         [[0.0000]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 0: After Flatten = tensor([[1.2649, 2.4253, 0.0000]], grad_fn=<ViewBackward0>)\n",
      "Epoch 0: After Fully Connected = tensor([[-1.7175]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 0: Loss = 137.30059814453125\n",
      "Epoch 1: After Conv = tensor([[[[ 8.4163, 10.4163],\n",
      "          [14.3604, 16.3604]],\n",
      "\n",
      "         [[ 1.8939,  2.2506],\n",
      "          [ 3.5358,  3.8925]],\n",
      "\n",
      "         [[ 6.5000,  8.5000],\n",
      "          [12.5000, 14.5000]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 1: After BatchNorm = tensor([[[[-1.3106, -0.7618],\n",
      "          [ 0.3204,  0.8692]],\n",
      "\n",
      "         [[-1.0970, -0.7800],\n",
      "          [ 0.3618,  0.6787]],\n",
      "\n",
      "         [[-1.2649, -0.6325],\n",
      "          [ 0.6325,  1.2649]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 1: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.3204, 0.8692]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.3618, 0.6787]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.6325, 1.2649]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 1: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.6409, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 1: After MaxPool = tensor([[[[0.6409]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 1: After Flatten = tensor([[0.6409, 0.0000, 0.0000]], grad_fn=<ViewBackward0>)\n",
      "Epoch 1: After Fully Connected = tensor([[0.0827]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 2: After Conv = tensor([[[[ 8.3950, 10.3941],\n",
      "          [14.3212, 16.3204]],\n",
      "\n",
      "         [[ 1.8939,  2.2506],\n",
      "          [ 3.5358,  3.8925]],\n",
      "\n",
      "         [[ 6.5000,  8.5000],\n",
      "          [12.5000, 14.5000]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 2: After BatchNorm = tensor([[[[-1.3251, -0.8028],\n",
      "          [ 0.2231,  0.7453]],\n",
      "\n",
      "         [[-1.0970, -0.7800],\n",
      "          [ 0.3618,  0.6787]],\n",
      "\n",
      "         [[-1.2649, -0.6325],\n",
      "          [ 0.6325,  1.2649]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 2: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.2231, 0.7453]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.3618, 0.6787]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.6325, 1.2649]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 2: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 1.3575]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.2649, 2.5298]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 2: After MaxPool = tensor([[[[0.0000]],\n",
      "\n",
      "         [[1.3575]],\n",
      "\n",
      "         [[2.5298]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 2: After Flatten = tensor([[0.0000, 1.3575, 2.5298]], grad_fn=<ViewBackward0>)\n",
      "Epoch 2: After Fully Connected = tensor([[1.4049]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 3: After Conv = tensor([[[[ 8.3950, 10.3941],\n",
      "          [14.3212, 16.3204]],\n",
      "\n",
      "         [[ 1.7980,  2.1936],\n",
      "          [ 3.5443,  3.9399]],\n",
      "\n",
      "         [[ 6.4782,  8.4782],\n",
      "          [12.4636, 14.4636]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 3: After BatchNorm = tensor([[[[-1.3251, -0.8028],\n",
      "          [ 0.2231,  0.7453]],\n",
      "\n",
      "         [[-1.1198, -0.7678],\n",
      "          [ 0.4336,  0.7856]],\n",
      "\n",
      "         [[-1.3344, -0.6084],\n",
      "          [ 0.8384,  1.5644]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 3: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.2231, 0.7453]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.4336, 0.7856]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.8384, 1.5644]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 3: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.4461, 1.4906]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.8673, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 3.1288]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 3: After MaxPool = tensor([[[[1.4906]],\n",
      "\n",
      "         [[0.8673]],\n",
      "\n",
      "         [[3.1288]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 3: After Flatten = tensor([[1.4906, 0.8673, 3.1288]], grad_fn=<ViewBackward0>)\n",
      "Epoch 3: After Fully Connected = tensor([[3.2098]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 4: After Conv = tensor([[[[ 8.3968, 10.3960],\n",
      "          [14.3247, 16.3239]],\n",
      "\n",
      "         [[ 2.1229,  2.3901],\n",
      "          [ 3.5294,  3.7966]],\n",
      "\n",
      "         [[ 6.4335,  8.4331],\n",
      "          [12.3873, 14.3868]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 4: After BatchNorm = tensor([[[[-1.3172, -0.8055],\n",
      "          [ 0.2001,  0.7118]],\n",
      "\n",
      "         [[-1.0867, -0.7622],\n",
      "          [ 0.6212,  0.9457]],\n",
      "\n",
      "         [[-1.4614, -0.5637],\n",
      "          [ 1.2116,  2.1093]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 4: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.2001, 0.7118]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.6212, 0.9457]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.2116, 2.1093]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 4: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.4001, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.4232, 4.2186]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 4: After MaxPool = tensor([[[[0.4001]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[4.2186]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 4: After Flatten = tensor([[0.4001, 0.0000, 4.2186]], grad_fn=<ViewBackward0>)\n",
      "Epoch 4: After Fully Connected = tensor([[5.8008]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 5: After Conv = tensor([[[[ 8.4041, 10.4038],\n",
      "          [14.3386, 16.3382]],\n",
      "\n",
      "         [[ 2.1229,  2.3901],\n",
      "          [ 3.5294,  3.7966]],\n",
      "\n",
      "         [[ 6.3826,  8.3805],\n",
      "          [12.2954, 14.2934]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 5: After BatchNorm = tensor([[[[-1.3118, -0.7900],\n",
      "          [ 0.2367,  0.7584]],\n",
      "\n",
      "         [[-1.0867, -0.7622],\n",
      "          [ 0.6212,  0.9457]],\n",
      "\n",
      "         [[-1.5844, -0.5191],\n",
      "          [ 1.5682,  2.6335]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 5: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.2367, 0.7584]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.6212, 0.9457]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.5682, 2.6335]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 5: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.0000, 1.5168]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [3.1365, 0.0000]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 5: After MaxPool = tensor([[[[1.5168]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[3.1365]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 5: After Flatten = tensor([[1.5168, 0.0000, 3.1365]], grad_fn=<ViewBackward0>)\n",
      "Epoch 5: After Fully Connected = tensor([[5.9278]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 5: Loss = 16.58258628845215\n",
      "Epoch 6: After Conv = tensor([[[[ 8.3997, 10.3991],\n",
      "          [14.3303, 16.3297]],\n",
      "\n",
      "         [[ 2.1229,  2.3901],\n",
      "          [ 3.5294,  3.7966]],\n",
      "\n",
      "         [[ 6.5286,  8.5354],\n",
      "          [12.5771, 14.5839]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 6: After BatchNorm = tensor([[[[-1.3305, -0.7836],\n",
      "          [ 0.2917,  0.8386]],\n",
      "\n",
      "         [[-1.0867, -0.7622],\n",
      "          [ 0.6212,  0.9457]],\n",
      "\n",
      "         [[-1.5264, -0.3788],\n",
      "          [ 1.9324,  3.0800]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 6: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.2917, 0.8386]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.6212, 0.9457]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.9324, 3.0800]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 6: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.0000, 1.6772]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.2424, 1.8913]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [3.8648, 6.1600]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 6: After MaxPool = tensor([[[[1.6772]],\n",
      "\n",
      "         [[1.8913]],\n",
      "\n",
      "         [[6.1600]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 6: After Flatten = tensor([[1.6772, 1.8913, 6.1600]], grad_fn=<ViewBackward0>)\n",
      "Epoch 6: After Fully Connected = tensor([[13.3976]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 7: After Conv = tensor([[[[ 8.4061, 10.4058],\n",
      "          [14.3423, 16.3420]],\n",
      "\n",
      "         [[ 2.3679,  2.5300],\n",
      "          [ 3.4823,  3.6445]],\n",
      "\n",
      "         [[ 6.6158,  8.6210],\n",
      "          [12.7155, 14.7208]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 7: After BatchNorm = tensor([[[[-1.3046, -0.7925],\n",
      "          [ 0.2157,  0.7279]],\n",
      "\n",
      "         [[-1.0349, -0.8063],\n",
      "          [ 0.5366,  0.7652]],\n",
      "\n",
      "         [[-1.3772, -0.4327],\n",
      "          [ 1.4959,  2.4405]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 7: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.2157, 0.7279]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.5366, 0.7652]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.4959, 2.4405]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 7: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.0000, 1.4558]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 1.5304]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 7: After MaxPool = tensor([[[[1.4558]],\n",
      "\n",
      "         [[1.5304]],\n",
      "\n",
      "         [[0.0000]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 7: After Flatten = tensor([[1.4558, 1.5304, 0.0000]], grad_fn=<ViewBackward0>)\n",
      "Epoch 7: After Fully Connected = tensor([[1.6145]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 8: After Conv = tensor([[[[ 8.3967, 10.3960],\n",
      "          [14.3248, 16.3241]],\n",
      "\n",
      "         [[ 1.6928,  2.1627],\n",
      "          [ 3.6908,  4.1607]],\n",
      "\n",
      "         [[ 6.6158,  8.6210],\n",
      "          [12.7155, 14.7208]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 8: After BatchNorm = tensor([[[[-1.3451, -0.7785],\n",
      "          [ 0.3348,  0.9014]],\n",
      "\n",
      "         [[-1.1315, -0.7080],\n",
      "          [ 0.6696,  1.0932]],\n",
      "\n",
      "         [[-1.3772, -0.4327],\n",
      "          [ 1.4959,  2.4405]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 8: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.3348, 0.9014]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.6696, 1.0932]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.4959, 2.4405]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 8: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.0000, 1.8027]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 4.8809]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 8: After MaxPool = tensor([[[[1.8027]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[4.8809]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 8: After Flatten = tensor([[1.8027, 0.0000, 4.8809]], grad_fn=<ViewBackward0>)\n",
      "Epoch 8: After Fully Connected = tensor([[8.5255]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 9: After Conv = tensor([[[[ 8.3926, 10.3917],\n",
      "          [14.3171, 16.3162]],\n",
      "\n",
      "         [[ 1.6928,  2.1627],\n",
      "          [ 3.6908,  4.1607]],\n",
      "\n",
      "         [[ 6.5902,  8.5967],\n",
      "          [12.6785, 14.6850]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 9: After BatchNorm = tensor([[[[-1.3610, -0.7730],\n",
      "          [ 0.3815,  0.9695]],\n",
      "\n",
      "         [[-1.1315, -0.7080],\n",
      "          [ 0.6696,  1.0932]],\n",
      "\n",
      "         [[-1.4265, -0.4152],\n",
      "          [ 1.6419,  2.6531]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 9: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.3815, 0.9695]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.6696, 1.0932]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.6419, 2.6531]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 9: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.7630, 1.9389]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.3392, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 9: After MaxPool = tensor([[[[1.9389]],\n",
      "\n",
      "         [[1.3392]],\n",
      "\n",
      "         [[0.0000]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 9: After Flatten = tensor([[1.9389, 1.3392, 0.0000]], grad_fn=<ViewBackward0>)\n",
      "Epoch 9: After Fully Connected = tensor([[2.7617]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 10: After Conv = tensor([[[[ 8.3697, 10.3676],\n",
      "          [14.2735, 16.2714]],\n",
      "\n",
      "         [[ 2.2554,  2.5088],\n",
      "          [ 3.6905,  3.9439]],\n",
      "\n",
      "         [[ 6.5902,  8.5967],\n",
      "          [12.6785, 14.6850]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 10: After BatchNorm = tensor([[[[-1.4485, -0.7425],\n",
      "          [ 0.6379,  1.3440]],\n",
      "\n",
      "         [[-1.0672, -0.7003],\n",
      "          [ 1.0102,  1.3771]],\n",
      "\n",
      "         [[-1.4265, -0.4152],\n",
      "          [ 1.6419,  2.6531]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 10: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.6379, 1.3440]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.0102, 1.3771]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.6419, 2.6531]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 10: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.0000, 2.6880]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 5.3063]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 10: After MaxPool = tensor([[[[2.6880]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[5.3063]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 10: After Flatten = tensor([[2.6880, 0.0000, 5.3063]], grad_fn=<ViewBackward0>)\n",
      "Epoch 10: After Fully Connected = tensor([[11.3419]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 10: Loss = 1.800672173500061\n",
      "Epoch 11: After Conv = tensor([[[[ 8.3775, 10.3759],\n",
      "          [14.2889, 16.2872]],\n",
      "\n",
      "         [[ 2.2554,  2.5088],\n",
      "          [ 3.6905,  3.9439]],\n",
      "\n",
      "         [[ 6.6172,  8.6226],\n",
      "          [12.7186, 14.7240]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 11: After BatchNorm = tensor([[[[-1.4230, -0.7514],\n",
      "          [ 0.5636,  1.2351]],\n",
      "\n",
      "         [[-1.0672, -0.7003],\n",
      "          [ 1.0102,  1.3771]],\n",
      "\n",
      "         [[-1.3769, -0.4329],\n",
      "          [ 1.4953,  2.4394]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 11: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.5636, 1.2351]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.0102, 1.3771]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.4953, 2.4394]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 11: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.0000, 2.4703]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.0205, 2.7542]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.9907, 0.0000]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 11: After MaxPool = tensor([[[[2.4703]],\n",
      "\n",
      "         [[2.7542]],\n",
      "\n",
      "         [[2.9907]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 11: After Flatten = tensor([[2.4703, 2.7542, 2.9907]], grad_fn=<ViewBackward0>)\n",
      "Epoch 11: After Fully Connected = tensor([[9.1912]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 12: After Conv = tensor([[[[ 8.3734, 10.3715],\n",
      "          [14.2809, 16.2790]],\n",
      "\n",
      "         [[ 2.1277,  2.4371],\n",
      "          [ 3.7202,  4.0295]],\n",
      "\n",
      "         [[ 6.6450,  8.6490],\n",
      "          [12.7588, 14.7628]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 12: After BatchNorm = tensor([[[[-1.4369, -0.7465],\n",
      "          [ 0.6042,  1.2946]],\n",
      "\n",
      "         [[-1.0909, -0.6772],\n",
      "          [ 1.0385,  1.4522]],\n",
      "\n",
      "         [[-1.3673, -0.4078],\n",
      "          [ 1.5600,  2.5196]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 12: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.6042, 1.2946]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.0385, 1.4522]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.5600, 2.5196]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 12: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 2.9045]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [3.1201, 0.0000]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 12: After MaxPool = tensor([[[[0.0000]],\n",
      "\n",
      "         [[2.9045]],\n",
      "\n",
      "         [[3.1201]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 12: After Flatten = tensor([[0.0000, 2.9045, 3.1201]], grad_fn=<ViewBackward0>)\n",
      "Epoch 12: After Fully Connected = tensor([[8.0472]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 13: After Conv = tensor([[[[ 8.3734, 10.3715],\n",
      "          [14.2809, 16.2790]],\n",
      "\n",
      "         [[ 1.8606,  2.2837],\n",
      "          [ 3.7677,  4.1907]],\n",
      "\n",
      "         [[ 6.7169,  8.7165],\n",
      "          [12.8600, 14.8597]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 13: After BatchNorm = tensor([[[[-1.4369, -0.7465],\n",
      "          [ 0.6042,  1.2946]],\n",
      "\n",
      "         [[-1.1392, -0.6361],\n",
      "          [ 1.1287,  1.6318]],\n",
      "\n",
      "         [[-1.3435, -0.3456],\n",
      "          [ 1.7221,  2.7199]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 13: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.6042, 1.2946]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.1287, 1.6318]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.7221, 2.7199]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 13: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.0000, 2.5893]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.2574, 3.2636]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [3.4441, 5.4399]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 13: After MaxPool = tensor([[[[2.5893]],\n",
      "\n",
      "         [[3.2636]],\n",
      "\n",
      "         [[5.4399]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 13: After Flatten = tensor([[2.5893, 3.2636, 5.4399]], grad_fn=<ViewBackward0>)\n",
      "Epoch 13: After Fully Connected = tensor([[14.6774]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 14: After Conv = tensor([[[[ 8.3990, 10.3989],\n",
      "          [14.3309, 16.3307]],\n",
      "\n",
      "         [[ 2.3864,  2.5989],\n",
      "          [ 3.7317,  3.9442]],\n",
      "\n",
      "         [[ 6.8201,  8.8114],\n",
      "          [12.9960, 14.9873]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 14: After BatchNorm = tensor([[[[-1.3519, -0.7763],\n",
      "          [ 0.3555,  0.9311]],\n",
      "\n",
      "         [[-1.0173, -0.7211],\n",
      "          [ 0.8570,  1.1531]],\n",
      "\n",
      "         [[-1.1697, -0.4058],\n",
      "          [ 1.1996,  1.9635]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 14: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.3555, 0.9311]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.8570, 1.1531]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.1996, 1.9635]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 14: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.7140, 2.3062]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 14: After MaxPool = tensor([[[[0.0000]],\n",
      "\n",
      "         [[2.3062]],\n",
      "\n",
      "         [[0.0000]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 14: After Flatten = tensor([[0.0000, 2.3062, 0.0000]], grad_fn=<ViewBackward0>)\n",
      "Epoch 14: After Fully Connected = tensor([[2.5694]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 15: After Conv = tensor([[[[ 8.3990, 10.3989],\n",
      "          [14.3309, 16.3307]],\n",
      "\n",
      "         [[ 1.3540,  2.0305],\n",
      "          [ 4.0215,  4.6979]],\n",
      "\n",
      "         [[ 6.8201,  8.8114],\n",
      "          [12.9960, 14.9873]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 15: After BatchNorm = tensor([[[[-1.3519, -0.7763],\n",
      "          [ 0.3555,  0.9311]],\n",
      "\n",
      "         [[-1.1600, -0.5853],\n",
      "          [ 1.1064,  1.6811]],\n",
      "\n",
      "         [[-1.1697, -0.4058],\n",
      "          [ 1.1996,  1.9635]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 15: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.3555, 0.9311]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.1064, 1.6811]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.1996, 1.9635]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 15: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.7110, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 3.3623]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.3991, 3.9270]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 15: After MaxPool = tensor([[[[0.7110]],\n",
      "\n",
      "         [[3.3623]],\n",
      "\n",
      "         [[3.9270]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 15: After Flatten = tensor([[0.7110, 3.3623, 3.9270]], grad_fn=<ViewBackward0>)\n",
      "Epoch 15: After Fully Connected = tensor([[9.0283]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 15: Loss = 0.9442225694656372\n",
      "Epoch 16: After Conv = tensor([[[[ 8.4053, 10.4054],\n",
      "          [14.3426, 16.3427]],\n",
      "\n",
      "         [[ 1.2927,  1.9911],\n",
      "          [ 4.0143,  4.7128]],\n",
      "\n",
      "         [[ 6.8083,  8.8009],\n",
      "          [12.9818, 14.9744]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 16: After BatchNorm = tensor([[[[-1.3478, -0.7647],\n",
      "          [ 0.3830,  0.9660]],\n",
      "\n",
      "         [[-1.1809, -0.5764],\n",
      "          [ 1.1745,  1.7790]],\n",
      "\n",
      "         [[-1.1938, -0.3976],\n",
      "          [ 1.2729,  2.0691]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 16: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.3830, 0.9660]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.1745, 1.7790]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.2729, 2.0691]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 16: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.7660, 1.9320]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 3.5580]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 4.1381]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 16: After MaxPool = tensor([[[[1.9320]],\n",
      "\n",
      "         [[3.5580]],\n",
      "\n",
      "         [[4.1381]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 16: After Flatten = tensor([[1.9320, 3.5580, 4.1381]], grad_fn=<ViewBackward0>)\n",
      "Epoch 16: After Fully Connected = tensor([[10.6507]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 17: After Conv = tensor([[[[ 8.4074, 10.4076],\n",
      "          [14.3466, 16.3468]],\n",
      "\n",
      "         [[ 1.3360,  2.0192],\n",
      "          [ 4.0203,  4.7035]],\n",
      "\n",
      "         [[ 6.8171,  8.8088],\n",
      "          [12.9924, 14.9841]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 17: After BatchNorm = tensor([[[[-1.3396, -0.7675],\n",
      "          [ 0.3590,  0.9310]],\n",
      "\n",
      "         [[-1.1659, -0.5827],\n",
      "          [ 1.1258,  1.7091]],\n",
      "\n",
      "         [[-1.1765, -0.4035],\n",
      "          [ 1.2202,  1.9932]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 17: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.3590, 0.9310]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.1258, 1.7091]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.2202, 1.9932]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 17: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.7179, 1.8620]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.2516, 3.4181]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.4404, 0.0000]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 17: After MaxPool = tensor([[[[1.8620]],\n",
      "\n",
      "         [[3.4181]],\n",
      "\n",
      "         [[2.4404]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 17: After Flatten = tensor([[1.8620, 3.4181, 2.4404]], grad_fn=<ViewBackward0>)\n",
      "Epoch 17: After Fully Connected = tensor([[8.2082]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 18: After Conv = tensor([[[[ 8.4019, 10.4018],\n",
      "          [14.3363, 16.3362]],\n",
      "\n",
      "         [[ 1.2211,  1.9452],\n",
      "          [ 4.0062,  4.7303]],\n",
      "\n",
      "         [[ 6.8609,  8.8479],\n",
      "          [13.0453, 15.0324]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 18: After BatchNorm = tensor([[[[-1.3610, -0.7601],\n",
      "          [ 0.4219,  1.0227]],\n",
      "\n",
      "         [[-1.2052, -0.5659],\n",
      "          [ 1.2538,  1.8931]],\n",
      "\n",
      "         [[-1.1611, -0.3605],\n",
      "          [ 1.3307,  2.1313]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 18: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.4219, 1.0227]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.2538, 1.8931]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.3307, 2.1313]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 18: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.8438, 2.0454]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 3.7862]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.6614, 4.2626]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 18: After MaxPool = tensor([[[[2.0454]],\n",
      "\n",
      "         [[3.7862]],\n",
      "\n",
      "         [[4.2626]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 18: After Flatten = tensor([[2.0454, 3.7862, 4.2626]], grad_fn=<ViewBackward0>)\n",
      "Epoch 18: After Fully Connected = tensor([[11.6299]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 19: After Conv = tensor([[[[ 8.4079, 10.4081],\n",
      "          [14.3475, 16.3478]],\n",
      "\n",
      "         [[ 1.3359,  2.0202],\n",
      "          [ 4.0252,  4.7095]],\n",
      "\n",
      "         [[ 6.8843,  8.8687],\n",
      "          [13.0727, 15.0571]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 19: After BatchNorm = tensor([[[[-1.3389, -0.7678],\n",
      "          [ 0.3569,  0.9279]],\n",
      "\n",
      "         [[-1.1649, -0.5827],\n",
      "          [ 1.1230,  1.7051]],\n",
      "\n",
      "         [[-1.1167, -0.3755],\n",
      "          [ 1.1947,  1.9359]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "Epoch 19: After ReLU = tensor([[[[0.0000, 0.0000],\n",
      "          [0.3569, 0.9279]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.1230, 1.7051]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [1.1947, 1.9359]]]], grad_fn=<ReluBackward0>)\n",
      "Epoch 19: After Dropout = tensor([[[[0.0000, 0.0000],\n",
      "          [0.7138, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [2.2459, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 3.8717]]]], grad_fn=<MulBackward0>)\n",
      "Epoch 19: After MaxPool = tensor([[[[0.7138]],\n",
      "\n",
      "         [[2.2459]],\n",
      "\n",
      "         [[3.8717]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "Epoch 19: After Flatten = tensor([[0.7138, 2.2459, 3.8717]], grad_fn=<ViewBackward0>)\n",
      "Epoch 19: After Fully Connected = tensor([[7.7955]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Định nghĩa mô hình CNN với Dropout\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=2, stride=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout với xác suất tắt 50%\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(3 * 1 * 1, 1)  # Fully connected layer\n",
    "\n",
    "    def forward(self, x, num_epoch):\n",
    "        self.num_epoch = num_epoch\n",
    "        x = self.conv(x)\n",
    "        print(f\"Epoch {self.num_epoch}: After Conv = {x}\")\n",
    "        x = self.bn(x)\n",
    "        print(f\"Epoch {self.num_epoch}: After BatchNorm = {x}\")\n",
    "        x = self.relu(x)\n",
    "        print(f\"Epoch {self.num_epoch}: After ReLU = {x}\")\n",
    "        x = self.dropout(x)  # Thêm Dropout sau ReLU\n",
    "        print(f\"Epoch {self.num_epoch}: After Dropout = {x}\")\n",
    "        x = self.pool(x)\n",
    "        print(f\"Epoch {self.num_epoch}: After MaxPool = {x}\")\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        print(f\"Epoch {self.num_epoch}: After Flatten = {x}\")\n",
    "        x = self.fc(x)\n",
    "        print(f\"Epoch {self.num_epoch}: After Fully Connected = {x}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "# Khởi tạo mô hình\n",
    "model = CNN()\n",
    "\n",
    "# Tạo dữ liệu đầu vào (3x3x3)\n",
    "x = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [[1.0, 2, 3], [4, 5, 6], [7, 8, 9]],  # Channel 1\n",
    "            [[9.0, 8, 7], [6, 5, 4], [3, 2, 1]],  # Channel 2\n",
    "            [[1.0, 1, 1], [2, 2, 2], [3, 3, 3]],  # Channel 3\n",
    "        ]\n",
    "    ],\n",
    "    requires_grad=True,\n",
    ")\n",
    "\n",
    "# Khởi tạo kernel với giá trị cụ thể Shape (3, 3, 2, 2)\n",
    "with torch.no_grad():\n",
    "    model.conv.weight = nn.Parameter(\n",
    "        torch.tensor(\n",
    "            [\n",
    "                [\n",
    "                    [[1.0, -1], [2, 0]],\n",
    "                    [[0.5, -0.5], [1, -1]],\n",
    "                    [[-1, 1], [0.5, -0.5]],\n",
    "                ],  # Kernel 1\n",
    "                [\n",
    "                    [[2.0, 1], [-2, 0]],\n",
    "                    [[1.5, -1], [-0.5, 0.5]],\n",
    "                    [[0, 0.5], [-1, 1]],\n",
    "                ],  # Kernel 2\n",
    "                [\n",
    "                    [[1.0, -1], [2, 0]],\n",
    "                    [[-1, 1], [0.5, -0.5]],\n",
    "                    [[2, -2], [1, -1]],\n",
    "                ],  # Kernel 3\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Giả sử nhãn thật\n",
    "y_true = torch.tensor([[10.0]])  # Dự đoán giá trị số thực (1 giá trị)\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Bật Dropout trong Training mode\n",
    "    optimizer.zero_grad()  # Xóa gradient trước đó\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(x, epoch)\n",
    "    loss = criterion(output, y_true)\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # In loss mỗi 5 epoch\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
    "\n",
    "# Vẽ đồ thị loss\n",
    "plt.plot(loss_history, label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss giảm dần qua từng epoch với CNN + Dropout\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Dropout:\n",
      " tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "After Dropout (Training Mode):\n",
      " tensor([[0., 0., 0., 0., 0.],\n",
      "        [5., 0., 0., 0., 0.],\n",
      "        [5., 0., 0., 0., 5.],\n",
      "        [0., 0., 5., 5., 0.],\n",
      "        [0., 0., 0., 5., 0.]])\n",
      "After Dropout (Inference Mode):\n",
      " tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Khởi tạo một tensor đầu vào (giả sử đầu ra của một lớp FC)\n",
    "x = torch.ones(5, 5)  # Ma trận 5x5 toàn số 1\n",
    "\n",
    "# Khởi tạo lớp Dropout với p=0.5 (tắt 50% neurons)\n",
    "dropout = nn.Dropout(p=0.8)\n",
    "\n",
    "# Kết quả khi chưa qua Dropout\n",
    "print(\"Before Dropout:\\n\", x)\n",
    "\n",
    "# Áp dụng Dropout trong chế độ huấn luyện\n",
    "x_dropout = dropout(x)\n",
    "print(\"After Dropout (Training Mode):\\n\", x_dropout)\n",
    "\n",
    "# Chuyển sang chế độ kiểm tra (inference)\n",
    "dropout.eval()  # Tắt Dropout (không loại bỏ neurons)\n",
    "x_eval = dropout(x)\n",
    "print(\"After Dropout (Inference Mode):\\n\", x_eval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
